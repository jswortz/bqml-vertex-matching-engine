{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d673345a",
   "metadata": {},
   "source": [
    "# Prep for running pipelines\n",
    "### Mostly around:\n",
    "- Component `.yaml` for dataflow embedding export\n",
    "- One-time setup of network resources (e.g.: VPC)\n",
    "- Configuration of BigQuery flex slots (for the bqml modeling compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43068fb8",
   "metadata": {},
   "source": [
    "### Dataflow yaml creation\n",
    "\n",
    "Many examples available from [here](https://github.com/kubeflow/pipelines/tree/master/components/gcp/dataflow/launch_template) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba2890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataflow-launch_python-component.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataflow-launch_python-component.yaml\n",
    "\n",
    "# Copyright 2018 The Kubeflow Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "name: Launch Python\n",
    "description: |\n",
    "  Launch a self-executing beam python file.\n",
    "metadata:\n",
    "  labels:\n",
    "    add-pod-env: 'true'\n",
    "inputs:\n",
    "  - name: project\n",
    "    description: 'The ID of the GCP project to run the Dataflow job.'\n",
    "    type: String\n",
    "  - name: location\n",
    "    description: 'The GCP region to run the Dataflow job.'\n",
    "    type: String\n",
    "  - name: python_file_path\n",
    "    description: 'The gcs or local path to the python file to run.'\n",
    "    type: String\n",
    "  - name: staging_dir\n",
    "    description: >-\n",
    "      Optional. The GCS directory for keeping staging files.\n",
    "      A random subdirectory will be created under the directory to keep job info\n",
    "      for resuming the job in case of failure and it will be passed as\n",
    "      `staging_location` and `temp_location` command line args of the beam code.\n",
    "    default: ''\n",
    "    type: String\n",
    "  - name: requirements_file_path\n",
    "    description: 'Optional, the gcs or local path to the pip requirements file'\n",
    "    default: ''\n",
    "    type: String\n",
    "  - name: args\n",
    "    description: 'The list of args to pass to the python file.'\n",
    "    default: '[]'\n",
    "    type: typing.List\n",
    "  - name: wait_interval\n",
    "    default: '30'\n",
    "    description: 'Optional wait interval between calls to get job status. Defaults to 30.'\n",
    "    type: Integer\n",
    "outputs:\n",
    "  - name: job_id\n",
    "    description: 'The id of the created dataflow job.'\n",
    "    type: String\n",
    "  - name: MLPipeline UI metadata\n",
    "    type: UI metadata\n",
    "implementation:\n",
    "  container:\n",
    "    image: gcr.io/ml-pipeline/ml-pipeline-gcp:1.7.0-rc.3\n",
    "    command: ['python', '-u', '-m', 'kfp_component.launcher']\n",
    "    args: [\n",
    "      --ui_metadata_path, {outputPath: MLPipeline UI metadata},\n",
    "      kfp_component.google.dataflow, launch_python,\n",
    "      --python_file_path, {inputValue: python_file_path},\n",
    "      --project_id, {inputValue: project},\n",
    "      --region, {inputValue: location},\n",
    "      --staging_dir, {inputValue: staging_dir},\n",
    "      --requirements_file_path, {inputValue: requirements_file_path},\n",
    "      --args, {inputValue: args},\n",
    "      --wait_interval, {inputValue: wait_interval},\n",
    "      --job_id_output_path, {outputPath: job_id},\n",
    "    ]\n",
    "    env:\n",
    "      KFP_POD_NAME: \"{{pod.name}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e86b504",
   "metadata": {},
   "source": [
    "### Creation of vpc and other resources here - *ONLY RUN ONCE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f61c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN ONLY ONCE - NOTE THESE ARE CREATED IN NOTEBOOK 05\n",
    "\n",
    "# PROJECT_ID = 'rec-ai-demo-326116'  # @param {type:\"string\"}\n",
    "# NETWORK_NAME = \"default\"  # @param {type:\"string\"}\n",
    "# PEERING_RANGE_NAME = 'google-reserved-range'\n",
    "# BUCKET = 'rec_bq_jsw' # Change to the bucket you created.\n",
    "\n",
    "\n",
    "# # Run this only once - this sets up your sub nets to provide high-speed predictions\n",
    "\n",
    "# # Create a VPC network\n",
    "# ! gcloud compute networks create {NETWORK_NAME} --bgp-routing-mode=regional --subnet-mode=auto --project={PROJECT_ID}\n",
    "\n",
    "# # Add necessary firewall rules\n",
    "# ! gcloud compute firewall-rules create {NETWORK_NAME}-allow-icmp --network {NETWORK_NAME} --priority 65534 --project {PROJECT_ID} --allow icmp\n",
    "\n",
    "# ! gcloud compute firewall-rules create {NETWORK_NAME}-allow-internal --network {NETWORK_NAME} --priority 65534 --project {PROJECT_ID} --allow all --source-ranges 10.128.0.0/9\n",
    "\n",
    "# ! gcloud compute firewall-rules create {NETWORK_NAME}-allow-rdp --network {NETWORK_NAME} --priority 65534 --project {PROJECT_ID} --allow tcp:3389\n",
    "\n",
    "# ! gcloud compute firewall-rules create {NETWORK_NAME}-allow-ssh --network {NETWORK_NAME} --priority 65534 --project {PROJECT_ID} --allow tcp:22\n",
    "\n",
    "# # Reserve IP range\n",
    "# ! gcloud compute addresses create {PEERING_RANGE_NAME} --global --prefix-length=16 --network={NETWORK_NAME} --purpose=VPC_PEERING --project={PROJECT_ID} --description=\"peering range for uCAIP Haystack.\"\n",
    "\n",
    "# # Set up peering with service networking\n",
    "# ! gcloud services vpc-peerings connect --service=servicenetworking.googleapis.com --network={NETWORK_NAME} --ranges={PEERING_RANGE_NAME} --project={PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd564edb",
   "metadata": {},
   "source": [
    "### Optional - set up flex slots on BQ for model computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c41d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BQ_REGION = 'US' # Change to your BigQuery region.\n",
    "# RESERVATION = 'default'\n",
    "# SLOTS=10\n",
    "# !bq mk --reservation --project_id=$PROJECT_ID --slots=$SLOTS --location=$BQ_REGION $RESERVATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b93893",
   "metadata": {},
   "source": [
    "### Upload the embeddings and stored procs for use in vertex pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acdfde00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://embeddings_exporter/setup.py [Content-Type=text/x-python]...\n",
      "Copying file://embeddings_exporter/pipeline_kfp.py [Content-Type=text/x-python]...\n",
      "Copying file://embeddings_exporter/__init__.py [Content-Type=text/x-python]...  \n",
      "Copying file://embeddings_exporter/pipeline.py [Content-Type=text/x-python]...  \n",
      "/ [4 files][  5.5 KiB/  5.5 KiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file://embeddings_exporter/beam_kfp2.py [Content-Type=text/x-python]...\n",
      "Copying file://embeddings_exporter/runner.py [Content-Type=text/x-python]...    \n",
      "Copying file://embeddings_exporter/embedding_exporter.egg-info/SOURCES.txt [Content-Type=text/plain]...\n",
      "Copying file://embeddings_exporter/embedding_exporter.egg-info/top_level.txt [Content-Type=text/plain]...\n",
      "Copying file://embeddings_exporter/embedding_exporter.egg-info/PKG-INFO [Content-Type=application/octet-stream]...\n",
      "Copying file://embeddings_exporter/embedding_exporter.egg-info/dependency_links.txt [Content-Type=text/plain]...\n",
      "Copying file://embeddings_exporter/__pycache__/pipeline.cpython-37.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://embeddings_exporter/.ipynb_checkpoints/setup-checkpoint.py [Content-Type=text/x-python]...\n",
      "Copying file://embeddings_exporter/.ipynb_checkpoints/pipeline-checkpoint.py [Content-Type=text/x-python]...\n",
      "Copying file://embeddings_exporter/.ipynb_checkpoints/runner-checkpoint.py [Content-Type=text/x-python]...\n",
      "Copying file://embeddings_exporter/.ipynb_checkpoints/pipeline_kfp-checkpoint.py [Content-Type=text/x-python]...\n",
      "Copying file://embeddings_exporter/.ipynb_checkpoints/beam_kfp2-checkpoint.py [Content-Type=text/x-python]...\n",
      "\\ [16 files][ 24.8 KiB/ 24.8 KiB]                                               \n",
      "Operation completed over 16 objects/24.8 KiB.                                    \n",
      "Copying file://sql_scripts/sp_ExractEmbeddings.sql [Content-Type=application/x-sql]...\n",
      "Copying file://sql_scripts/sp_ComputePMI.sql [Content-Type=application/x-sql]...\n",
      "Copying file://sql_scripts/sp_TrainItemMatchingModel.sql [Content-Type=application/x-sql]...\n",
      "Copying file://sql_scripts/.ipynb_checkpoints/sp_ExractEmbeddings-checkpoint.sql [Content-Type=application/x-sql]...\n",
      "/ [4 files][  4.6 KiB/  4.6 KiB]                                                \n",
      "Operation completed over 4 objects/4.6 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Note we will be leveraging code copied up from the embeddings_exporter folder\n",
    "!gsutil cp -r embeddings_exporter/ gs://rec_bq_jsw\n",
    "    \n",
    "#do the same with the sproc files\n",
    "!gsutil cp -r sql_scripts/ gs://rec_bq_jsw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b42248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
