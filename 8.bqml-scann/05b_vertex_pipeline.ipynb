{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3652b2",
   "metadata": {},
   "source": [
    "# Vertex pipeline\n",
    "\n",
    "Requirements: See [notebook 5a](05a_setup_pipeline_resources.ipynb)\n",
    "\n",
    "This notebook will take the work of defining the BQML PMI matrix factorization and ANN deployment found in the 01 and 02 notebooks, then will create a queryable ScaNN index as seen in notebook 05. If there is a need to convert the keras model, the steps below should explain how to productionize, many of these steps can be explained in this [repo](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/pipelines/pipelines_intro_kfp.ipynb)\n",
    "\n",
    "The goal of this notebook is:\n",
    "1. Define the tasks that will be encapsulated in pipeline components\n",
    "2. Define the pipeline\n",
    "3. Run and monitor the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9f516",
   "metadata": {},
   "source": [
    "# Output pipeline\n",
    "\n",
    "1) Create the 2-column data schema for the model\n",
    "2) Upload model stored procedures\n",
    "3) Run the BQML model\n",
    "4) Export the embeddings using dataflow from BQ to GCS\n",
    "5) Deploy matching engine ScaNN index from the embeddings\n",
    "\n",
    "<img src=\"figures/pipeline_output2.png\" style=\"width:700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586c9031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Install packages - restart kernel if installed\n",
    "\n",
    "# %%capture --no-stderr\n",
    "\n",
    "!pip3 install -q kfp google.cloud.aiplatform --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8115e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "PROJECT_ID = \"smoke-test-jsw\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET = 'jsw-bucket-matching' # Change to the bucket you created.\n",
    "BUCKET_NAME = f'gs://{BUCKET}'\n",
    "DATASET_NAME=\"css_retail\"\n",
    "embeddings_table_name = 'item_embeddings'\n",
    "output_dir = f'gs://{BUCKET}/bqml/item_embeddings'\n",
    "temp_location = f'gs://{BUCKET}/dataflow_tmp'\n",
    "dataflow_job_code = f'gs://{BUCKET}/embeddings_exporter/beam_kfp2.py'\n",
    "PROJECT_NUMBER = '442799176336'\n",
    "DEPLOYED_INDEX_ID = \"ann_prod50_deployed11\"\n",
    "SA_NAME = \"sa-pipeline@rec-ai-demo-326116.iam.gserviceaccount.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5e8e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 442799176336-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# Get your GCP project id from gcloud\n",
    "shell_output = !gcloud auth list 2>/dev/null\n",
    "SERVICE_ACCOUNT = shell_output[2].strip()\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1364e",
   "metadata": {},
   "source": [
    "### Set service account access for Vertex Pipelines\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ba3171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://jsw-bucket-matching/\n",
      "No changes made to gs://jsw-bucket-matching/\n"
     ]
    }
   ],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_NAME\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07dc5fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "# API service endpoint\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/intro\".format(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c12a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0056999b",
   "metadata": {},
   "source": [
    "## Initialize Vertex SDK for Python\n",
    "Initialize the Vertex SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33a90e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcdc7aa",
   "metadata": {},
   "source": [
    "### Set up Biq Query DDLs for the pipelines\n",
    "These functions will be leveraged to make bq calls and train the BQML models. Guide found [here](https://medium.com/google-cloud/using-bigquery-and-bigquery-ml-from-kubeflow-pipelines-991a2fa4bea8) - this will be a reusable component to interface with BQ via query strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4fde11",
   "metadata": {},
   "source": [
    "### First component: compute the `sp_ComputePMI` stored proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d1a9422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b748e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file=\"bqml_scann_pipeline.yaml\", #just an example of how you can create a component artifact\n",
    "           base_image=\"python:3.9\", \n",
    "           packages_to_install=['google-cloud-bigquery==2.18.0']\n",
    "          )\n",
    "def run_a_bq_call(\n",
    "  project: str, query: str) -> NamedTuple('Outputs', [('RESULT', str)]):\n",
    "    from google.cloud import bigquery\n",
    "    bq_client = bigquery.Client(project=project)\n",
    "    j = bq_client.query(query).result()\n",
    "\n",
    "    return (\n",
    "    str(j),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269d5a7",
   "metadata": {},
   "source": [
    "#### Send the stored procedures for the models to local storage and load in BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cd1cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this loads the sprocs found in {BUCKET}/sql_scripts - loaded up in 00_prep_bq...\n",
    "\n",
    "@component(base_image=\"python:3.9\", packages_to_install=['google-cloud-bigquery', \n",
    "                                'google-cloud-storage']\n",
    "          )\n",
    "def load_sprocs(\n",
    "  bucket: str, dataset_name: str, project: str, bucket_name: str) -> NamedTuple('Outputs', [('COMPLETE_CODE', str)]):\n",
    "    import os\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    client = bigquery.Client(project=project)\n",
    "    \n",
    "    sql_scripts = dict()\n",
    "    SQL_SCRIPTS_DIR = f'{bucket_name}/sql_scripts'\n",
    "    BQ_DATASET_NAME = dataset_name\n",
    "\n",
    "    os.mkdir(\"downloads_\")\n",
    "    files = ['sp_ExractEmbeddings.sql', 'sp_ComputePMI.sql', 'sp_TrainItemMatchingModel.sql']\n",
    "    \n",
    "    storage_client = storage.Client(project)\n",
    "\n",
    "    bucket = storage_client.bucket(bucket)\n",
    "    \n",
    "    for file in files:\n",
    "        blob = bucket.blob(f\"sql_scripts/{file}\")\n",
    "\n",
    "        blob.download_to_filename(f\"downloads_/{file}\")\n",
    "\n",
    "\n",
    "    SQL_SCRIPTS_DIR = \"downloads_/\"\n",
    "\n",
    "    for script_file in [file for file in os.listdir(SQL_SCRIPTS_DIR) if '.sql' in file]:\n",
    "        script_file_path = os.path.join(SQL_SCRIPTS_DIR, script_file)\n",
    "        sql_script = open(script_file_path, 'r').read()\n",
    "        sql_script = sql_script.replace('@DATASET_NAME', BQ_DATASET_NAME)\n",
    "        sql_scripts[script_file] = sql_script\n",
    "    for script_file in sql_scripts:\n",
    "        print(f'Executing {script_file} script...')\n",
    "        query = sql_scripts[script_file]\n",
    "        query_job = client.query(query)\n",
    "        result = query_job.result()\n",
    "\n",
    "    return (\n",
    "    str(result),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392d395",
   "metadata": {},
   "source": [
    "### Import and use the Dataflow component - used to extract embeddings from BQ -> GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78101d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.components as comp\n",
    "\n",
    "dataflow_python_op = comp.load_component_from_file('dataflow-launch_python-component.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f6f11",
   "metadata": {},
   "source": [
    "#### Below are the BQ query strings to load the data and produce the embeddings for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61b81953",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "create_item_view = f\"\"\"\n",
    "CREATE or REPLACE VIEW `{PROJECT_ID}.{DATASET_NAME}.vw_item_groups`\n",
    "AS\n",
    "SELECT \n",
    "  userInfo.userID as group_id, \n",
    "  pd.id as item_id \n",
    "FROM \n",
    "  `{PROJECT_ID}.{DATASET_NAME}.purchase_complete`,\n",
    "  UNNEST(productEventDetail.productDetails) as pd\n",
    "\"\"\"\n",
    "\n",
    "create_cooc_matrix_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS css_retail.item_cooc\n",
    "AS SELECT 0 AS item1_Id, 0 AS item2_Id, 0 AS cooc, 0 AS pmi;\n",
    "\"\"\"\n",
    "\n",
    "create_bqml_model_query = f\"\"\"\n",
    "CREATE MODEL IF NOT EXISTS {DATASET_NAME}.item_matching_model\n",
    "OPTIONS(\n",
    "    MODEL_TYPE='matrix_factorization', \n",
    "    USER_COL='item1_Id', \n",
    "    ITEM_COL='item2_Id',\n",
    "    RATING_COL='score'\n",
    ")\n",
    "AS\n",
    "SELECT 0 AS item1_Id, 0 AS item2_Id, 0 AS score;\n",
    "\"\"\"\n",
    "\n",
    "compute_PMI_query = f\"\"\"\n",
    "DECLARE min_item_frequency INT64;\n",
    "DECLARE max_group_size INT64;\n",
    "\n",
    "SET min_item_frequency = 15;\n",
    "SET max_group_size = 100;\n",
    "\n",
    "CALL {DATASET_NAME}.sp_ComputePMI(min_item_frequency, max_group_size);\n",
    "\"\"\"\n",
    "\n",
    "train_item_matching_query = f\"\"\"\n",
    "DECLARE dimensions INT64 DEFAULT 50;\n",
    "CALL {DATASET_NAME}.sp_TrainItemMatchingModel(dimensions)\n",
    "\"\"\"\n",
    "\n",
    "extract_embeddings_query = f\"\"\"\n",
    "CALL {DATASET_NAME}.sp_ExractEmbeddings() \n",
    "\"\"\"\n",
    "\n",
    "export_embeddings_query = f\"\"\"\n",
    "CREATE TEMP FUNCTION array_int_to_string(int_array ARRAY<FLOAT64>) \n",
    "  RETURNS ARRAY<STRING> LANGUAGE js as \"return int_array.map(x => x+'')\";\n",
    "  \n",
    "EXPORT DATA\n",
    "OPTIONS (uri='{BUCKET_NAME}/bqml/item_embeddings/*.csv',\n",
    "  format='CSV',\n",
    "  overwrite=true) AS\n",
    "select item_id, array_to_string(array_int_to_string(embedding), ',')  as embedding_string \n",
    "from `{DATASET_NAME}.item_embeddings`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6987805e",
   "metadata": {},
   "source": [
    "#### Create and deploy the index (ScaNN only)\n",
    "\n",
    "##### Another very likely use case should be the index update for ANN ScaNN index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "841bb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", \n",
    "           packages_to_install=['grpcio-tools', 'google-cloud-storage', 'protobuf', 'google-cloud-aiplatform']\n",
    "          )\n",
    "def create_index(\n",
    "  project_id: str, bucket: str, region: str,\n",
    "project_number: str, deployed_index_id: str,\n",
    "sa_name: str) -> NamedTuple('Outputs', [('COMPLETE_CODE', str)]):\n",
    "    import os\n",
    "    import time\n",
    "    import subprocess\n",
    "    from google.cloud import storage\n",
    "\n",
    "    import grpc\n",
    "    from google.cloud import aiplatform_v1beta1\n",
    "    from google.protobuf import struct_pb2\n",
    "    \n",
    "    NETWORK_NAME = 'default'\n",
    "    PEERING_RANGE_NAME = 'google-reserved-range'\n",
    "    PROJECT_ID = project_id\n",
    "    BUCKET = bucket\n",
    "    \n",
    "    BUCKET_NAME = f\"gs://{BUCKET}/bqml/item_embeddings\"  # @param {type:\"string\"}\n",
    "    REGION = region  # @param {type:\"string\"}\n",
    "    PARENT = \"projects/{}/locations/{}\".format(PROJECT_ID, REGION)\n",
    "    ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "    DEPLOYED_INDEX_ID = deployed_index_id\n",
    "    #AUTH_TOKEN = subprocess.check_output(\"gcloud auth print-access-token\", shell=True)\n",
    "    #AUTH_TOKEN = [str(AUTH_TOKEN2,'utf-8').replace(\"\\n\", \"\")]\n",
    "    PROJECT_NUMBER = project_number\n",
    "    \n",
    "    index_client = aiplatform_v1beta1.IndexServiceClient(\n",
    "    client_options=dict(api_endpoint=ENDPOINT)\n",
    "    )\n",
    "    DIMENSIONS = 50\n",
    "    DISPLAY_NAME = \"retail_demo_matching_engine\"\n",
    "\n",
    "    treeAhConfig = struct_pb2.Struct(\n",
    "        fields={\n",
    "            \"leafNodeEmbeddingCount\": struct_pb2.Value(number_value=500),\n",
    "            \"leafNodesToSearchPercent\": struct_pb2.Value(number_value=7),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    algorithmConfig = struct_pb2.Struct(\n",
    "        fields={\"treeAhConfig\": struct_pb2.Value(struct_value=treeAhConfig)}\n",
    "    )\n",
    "\n",
    "    config = struct_pb2.Struct(\n",
    "        fields={\n",
    "            \"dimensions\": struct_pb2.Value(number_value=DIMENSIONS),\n",
    "            \"approximateNeighborsCount\": struct_pb2.Value(number_value=150),\n",
    "            \"distanceMeasureType\": struct_pb2.Value(string_value=\"DOT_PRODUCT_DISTANCE\"),\n",
    "            \"algorithmConfig\": struct_pb2.Value(struct_value=algorithmConfig),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    metadata = struct_pb2.Struct(\n",
    "        fields={\n",
    "            \"config\": struct_pb2.Value(struct_value=config),\n",
    "            \"contentsDeltaUri\": struct_pb2.Value(string_value=BUCKET_NAME), # 'tmp/' | BUCKET_NAME\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ann_index = {\n",
    "        \"display_name\": DISPLAY_NAME,\n",
    "        \"description\": \"Retail 50 Index\",\n",
    "        \"metadata\": struct_pb2.Value(struct_value=metadata),\n",
    "    }\n",
    "    \n",
    "    ann_index = index_client.create_index(parent=PARENT, index=ann_index)\n",
    "\n",
    "    # Poll the operation until it's done successfullly.\n",
    "    # This will take ~50 min.\n",
    "    \n",
    "    print(\"starting index creation\")\n",
    "    \n",
    "    while not ann_index.done():\n",
    "        print(\"Poll the operation to create index...\")\n",
    "        time.sleep(60)\n",
    "        \n",
    "    print(\"index created\")    \n",
    "    #create an indexEndpoint\n",
    "    index_endpoint_client = aiplatform_v1beta1.IndexEndpointServiceClient(client_options=dict(api_endpoint=ENDPOINT))\n",
    "\n",
    "    VPC_NETWORK_NAME = \"projects/{}/global/networks/{}\".format(PROJECT_NUMBER, NETWORK_NAME)\n",
    "    VPC_NETWORK_NAME\n",
    "    \n",
    "    index_endpoint = {\n",
    "    \"display_name\": \"index_endpoint_for_demo\",\n",
    "    \"network\": VPC_NETWORK_NAME,}\n",
    "    \n",
    "    r = index_endpoint_client.create_index_endpoint(parent=PARENT, index_endpoint=index_endpoint)\n",
    "    \n",
    "    INDEX_ENDPOINT_NAME = r.result().name\n",
    "    INDEX_RESOURCE_NAME = ann_index.result().name\n",
    "    \n",
    "    deploy_ann_index = {\n",
    "    \"id\": DEPLOYED_INDEX_ID,\n",
    "    \"display_name\": DEPLOYED_INDEX_ID,\n",
    "    \"index\": INDEX_RESOURCE_NAME,\n",
    "    }\n",
    "    \n",
    "    res = index_endpoint_client.deploy_index(index_endpoint=INDEX_ENDPOINT_NAME, deployed_index=deploy_ann_index)\n",
    "    print(f\"Starting endpoint deployment for{INDEX_ENDPOINT_NAME}\")\n",
    "    \n",
    "    while not res.done():\n",
    "        print(\"Poll the operation to deploy index...\")\n",
    "        time.sleep(60)\n",
    "    print(\"success\")\n",
    "    \n",
    "    return (\n",
    "    str(200),\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba04f9",
   "metadata": {},
   "source": [
    "## Declare the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acf7abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl.types import GCPProjectID, GCSPath\n",
    "import json\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=\"bqml-scann-demo\",\n",
    "             description=\"a bqml matching engine demo\",\n",
    "             pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(project:str = PROJECT_ID,\n",
    "             temp_location:str = temp_location,\n",
    "             dataflow_job_code:str  = dataflow_job_code,\n",
    "             create_item_view:str = create_item_view,\n",
    "             compute_PMI_query: str = compute_PMI_query,\n",
    "             train_item_matching_query: str=train_item_matching_query,\n",
    "             extract_embeddings_query: str=extract_embeddings_query,\n",
    "             dataset_name: str = DATASET_NAME, \n",
    "             embeddings_table_name: str = embeddings_table_name,\n",
    "             output_dir: str = output_dir, \n",
    "             region: str = REGION,\n",
    "             create_cooc_matrix_query: str = create_cooc_matrix_query,\n",
    "             create_bqml_model_query: str = create_bqml_model_query,\n",
    "             bucket_name: str = BUCKET_NAME,\n",
    "             export_embeddings_query: str = export_embeddings_query,\n",
    "             bucket: str = BUCKET, project_number: str=PROJECT_NUMBER,\n",
    "             deployed_index_id: str = DEPLOYED_INDEX_ID,\n",
    "             sa_name: str = SA_NAME,\n",
    "             args: List = json.dumps([\n",
    "                 '--bq_dataset_name', DATASET_NAME, \n",
    "                 '--embeddings_table_name',  embeddings_table_name, \n",
    "                 '--output_dir', output_dir,\n",
    "                 '--project_id', PROJECT_ID,\n",
    "                 '--output', output_dir\n",
    "             ]),\n",
    "            \n",
    "            ):\n",
    "    \n",
    "   \n",
    "    ###### NOTEBOOK 0 TASKS\n",
    "    item_view_created = run_a_bq_call(project, create_item_view).set_display_name('BigQuery - Item user view creation')\n",
    "    \n",
    "    \n",
    "    create_cooc_matrix = run_a_bq_call(project, create_cooc_matrix_query).after(item_view_created).set_display_name('BigQuery - Create co-occurance matrix')\n",
    "\n",
    "    \n",
    "    create_bqml_model = run_a_bq_call(project, create_bqml_model_query).after(create_cooc_matrix).set_display_name('BigQuery - Create recAI model')\n",
    "    \n",
    "    loaded_sprocs = load_sprocs(bucket_name=bucket_name, bucket = bucket, \n",
    "                                dataset_name=dataset_name, project=project).after(create_bqml_model).set_display_name('Python - Load BQ stored procedures')\n",
    "    \n",
    "    ###### NOTEBOOK 1 TASKS\n",
    "    compute_bq_pmi_task = run_a_bq_call(project, compute_PMI_query).after(loaded_sprocs).set_display_name('BigQuery - Compute PMI Query')\n",
    "    \n",
    "    train_bq_item_match_task = run_a_bq_call(project, train_item_matching_query).after(loaded_sprocs).set_display_name('BigQuery - Train the recAI model')\n",
    "\n",
    "    ###### NOTEBOOK 2 TASKS\n",
    "    create_bq_embeddings_task = run_a_bq_call(project, extract_embeddings_query).after(train_bq_item_match_task).set_display_name('BigQuery - Extract trained embeddings to table')\n",
    "    \n",
    "    export_emb_gcs_df = dataflow_python_op(project=project,\n",
    "                                          python_file_path=dataflow_job_code,\n",
    "                                          location = region,\n",
    "                                          staging_dir = temp_location, args=args).after(create_bq_embeddings_task).set_display_name('Dataflow - Extract embeddings from BQ to GCS')\n",
    "    \n",
    "    ##### NOTEBOOK 5 TASKS - SET UP THE INDEX\n",
    "    created_index = create_index(project_id=project, bucket=bucket, region=region, \n",
    "                                 project_number=project_number, deployed_index_id=deployed_index_id,\n",
    "                                sa_name=sa_name).after(export_emb_gcs_df).set_display_name('Python - Create and deploy ScaNN matching engine deployment')\n",
    "    \n",
    "    return (\n",
    "    str(200),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b15a8",
   "metadata": {},
   "source": [
    "### Now that the reusable components are set in a pipeline, declare the parameters, queries, logic, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87e68a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  # noqa: F811\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"intro_pipeline.json\".replace(\" \", \"_\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dddcd7d",
   "metadata": {},
   "source": [
    "#### Execute the pipline on Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc38cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/442799176336/locations/us-central1/pipelineJobs/matching-engine-deployment20210928185518\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/442799176336/locations/us-central1/pipelineJobs/matching-engine-deployment20210928185518')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/matching-engine-deployment20210928185518?project=442799176336\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/442799176336/locations/us-central1/pipelineJobs/matching-engine-deployment20210928185518 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/442799176336/locations/us-central1/pipelineJobs/matching-engine-deployment20210928185518 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/442799176336/locations/us-central1/pipelineJobs/matching-engine-deployment20210928185518 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/442799176336/locations/us-central1/pipelineJobs/matching-engine-deployment20210928185518 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/442799176336/locations/us-central1/pipelineJobs/matching-engine-deployment20210928185518 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/442799176336/locations/us-central1/pipelineJobs/matching-engine-deployment20210928185518 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/442799176336/locations/us-central1/pipelineJobs/matching-engine-deployment20210928185518 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "DISPLAY_NAME = \"matching-engine-deployment\" + TIMESTAMP\n",
    "\n",
    "pipeline_job = pipeline_jobs.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    job_id=DISPLAY_NAME,\n",
    "    template_path=\"intro_pipeline.json\".replace(\" \", \"_\"),\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # parameter_values=pipeline_params,\n",
    "    enable_caching=True\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e047184",
   "metadata": {},
   "source": [
    "# Results - a deployed, queriable index\n",
    "\n",
    "<img src=\"figures/Screen Shot 2021-09-25 at 11.09.49 PM.png\" style=\"width:600px;\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8ac91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
