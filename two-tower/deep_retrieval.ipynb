{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6ed4d9-f864-4944-91f1-1f382ea71584",
   "metadata": {},
   "source": [
    "## Setup - install packages and restart kernel to begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff8a74a-ac9e-4a77-954a-1e93c5784b5f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (0.15.0)\n",
      "Collecting scann\n",
      "  Downloading scann-1.2.4-cp37-cp37m-manylinux2014_x86_64.whl (10.6 MB)\n",
      "     |████████████████████████████████| 10.6 MB 4.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.7/site-packages (4.3.0)\n",
      "Requirement already satisfied: google-cloud-bigquery in /opt/conda/lib/python3.7/site-packages (2.30.1)\n",
      "Collecting tensorflow-recommenders\n",
      "  Downloading tensorflow_recommenders-0.6.0-py3-none-any.whl (85 kB)\n",
      "     |████████████████████████████████| 85 kB 6.1 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.7/site-packages (1.7.1)\n",
      "Requirement already satisfied: tensorflow-io in /opt/conda/lib/python3.7/site-packages (0.21.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py) (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from scann) (1.19.5)\n",
      "Collecting tensorflow~=2.7.0\n",
      "  Downloading tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\n",
      "     |██████████████████████████████▊ | 469.2 MB 66.4 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |████████████████████████████████| 489.6 MB 10 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (3.19.1)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (5.4.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (4.0.1)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (2.26.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (1.5.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (0.18.2)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (0.3.4)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (21.2.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (2.2.1)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (1.19.8)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.38.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (1.42.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (21.3)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (2.1.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (2.2.2)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.43.0)\n",
      "Collecting tensorflow-io-gcs-filesystem==0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "     |████████████████████████████████| 2.1 MB 65.4 MB/s            \n",
      "\u001b[?25hCollecting tensorflow-io\n",
      "  Downloading tensorflow_io-0.23.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.1 MB)\n",
      "     |████████████████████████████████| 23.1 MB 23.1 MB/s            \n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem==0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.23.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "     |████████████████████████████████| 2.1 MB 32.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (59.4.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (2.3.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (1.53.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (1.42.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-bigquery) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.7.0->scann) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.7.0->scann) (1.13.3)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.7.0->scann) (1.12)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.7.0->scann) (0.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.7.0->scann) (3.1.0)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-12.0.0-py2.py3-none-manylinux1_x86_64.whl (13.4 MB)\n",
      "     |████████████████████████████████| 13.4 MB 47.6 MB/s            \n",
      "\u001b[?25hCollecting keras<2.8,>=2.7.0rc0\n",
      "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "     |████████████████████████████████| 1.3 MB 44.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: tensorboard~=2.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.7.0->scann) (2.6.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.7.0->scann) (1.1.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.7.0->scann) (0.37.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.7.0->scann) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.7.0->scann) (3.3.0)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "     |████████████████████████████████| 463 kB 87.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib-resources->tensorflow-datasets) (3.6.0)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "     |████████████████████████████████| 129 kB 48.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (4.2.4)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.15.0)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow~=2.7.0->scann) (1.5.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow~=2.7.0->scann) (1.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow~=2.7.0->scann) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow~=2.7.0->scann) (3.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow~=2.7.0->scann) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow~=2.7.0->scann) (2.0.2)\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     |████████████████████████████████| 152 kB 91.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (2.21)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.7.0->scann) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow~=2.7.0->scann) (4.8.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.7.0->scann) (3.1.1)\n",
      "Installing collected packages: google-auth, absl-py, tensorflow-io-gcs-filesystem, tensorflow-estimator, libclang, keras, tensorflow, tensorflow-recommenders, tensorflow-io, scann\n",
      "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\n",
      "tfx-bsl 1.4.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.31.0 which is incompatible.\n",
      "tfx-bsl 1.4.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\n",
      "tfx-bsl 1.4.0 requires tensorflow-metadata<1.5,>=1.4, but you have tensorflow-metadata 1.5.0 which is incompatible.\n",
      "tensorflow-transform 1.4.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\n",
      "tensorflow-transform 1.4.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2, but you have tensorflow 2.7.0 which is incompatible.\n",
      "tensorflow-transform 1.4.0 requires tensorflow-metadata<1.5.0,>=1.4.0, but you have tensorflow-metadata 1.5.0 which is incompatible.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.31.0 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-0.12.0 google-auth-1.35.0 keras-2.7.0 libclang-12.0.0 scann-1.2.4 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-0.23.1 tensorflow-io-gcs-filesystem-0.23.1 tensorflow-recommenders-0.6.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install  absl-py scann tensorflow-datasets google-cloud-bigquery tensorflow-recommenders google-cloud-aiplatform tensorflow-io --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7f27d-74f1-42c2-a82a-7f97109a25bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Two-Tower Recommendation Example\n",
    "\n",
    "This notebook is intended to show an end-end example of training a two-tower recommendation model using [Tensorflow Recommenders](https://www.tensorflow.org/recommenders). \n",
    "\n",
    "The intended use is to provide a working example of the TFRS library and show how training can produce custom embeddings and models for use with the matching engine. For more info on matching engine, see the `bqml-scann` folder for an end-end example\n",
    "\n",
    "This mostly follows the example of a deep retreival model as found on [this page](https://www.tensorflow.org/recommenders/examples/deep_recommenders) \n",
    "\n",
    "The steps are as follows\n",
    "\n",
    "1. Import libraries and set variables\n",
    "2. Pull data from the `css-retail` example, from the query that creates the training data (see commented BQ SQL)\n",
    "3. Define the two tower's classes - product class for the first model, event class for the second. Event is equivalent to user-query events with a resulting puchase\n",
    "  * Note that the user class is a subclass of event and used to model embeddings at the user level\n",
    "4. Develop the two-tower class and set training objectives using the tfrs tasks (retreival task) to compute loss\n",
    "5. Train the model, establish a Tensorboard to assess performance\n",
    "6. Save the model for use in matching engine (or embeddings)\n",
    "\n",
    "\n",
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f8d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "import numpy as np\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    " \n",
    "    \n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd5463d-8c5a-41a9-8c98-bd9f0ed480b9",
   "metadata": {},
   "source": [
    "## Set Vars\n",
    "\n",
    "Many of these paremeters use `ABSL` as flagging and logging will be critical if a pipeline or distributed training is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ec9880-991f-4d36-bc73-8361b109051a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://jsw-bucket2/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'jsw-bucket2' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb -l us-central1 gs://jsw-bucket2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c22d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'two-tower'\n",
    "#PREFIX = 'css_retail'\n",
    "DISPLAY_NAME = f'{PREFIX}-tensorboard'\n",
    "PROJECT= 'CHANGE_ME'\n",
    "REGION='us-central1'\n",
    "\n",
    "STAGING_BUCKET = \"\"\"gs://{}_vertex_training\"\"\".format(PROJECT)\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "LR = 0.0002 #flags.DEFINE_float(\"LR\", 0.001, \"Learning Rate\")\n",
    "EMBEDDING_DIM = 64 #flags.DEFINE_integer(\"EMBEDDING_DIM\", 15, \"Embedding dimension\")\n",
    "MAX_TOKENS = 300_000 #flags.DEFINE_integer(\"MAX_TOKENS\", 15, \"Max embeddings for query and last_n products\")\n",
    "NUM_EPOCHS = 100 #flags.DEFINE_integer(\"NUM_EPOCHS\", 29, \"Number of epochs\")\n",
    "MODEL_DIR = 'model-dirs' #flags.DEFINE_string(\"MODEL_DIR\", 'model-dirs-', \"GCS Bucket to store the model artifact\")\n",
    "DROPOUT = False #flags.DEFINE_bool(\"DROPOUT\", False, \"Use Dropout - T/F bool type\")\n",
    "DROPOUT_RATE = None #flags.DEFINE_float(\"DROPOUT_RATE\", -1.4, \"Dropout rate only works with DROPOUT=True\")\n",
    "#flags.DEFINE_integer(\"N_PRODUCTS\", 19999, \"number of products considered for embedding\")\n",
    "BATCH_SIZE = 256 #flags.DEFINE_integer(\"BATCH_SIZE\", 1023, \"batch size\")\n",
    "ARCH = '[1000,500,100]' #flags.DEFINE_string(\"ARCH\", '[128,64]', \"deep architecture, expressed as a list of ints in string format - will be parsed into list\")\n",
    "SEED = 8947 #flags.DEFINE_integer(\"SEED\", 41781896, \"random seed\")\n",
    "#flags.DEFINE_string(\"TF_RECORDS_DIR\", \"gs://tfrs-central-a\", \"source data in tfrecord format gcs location\")\n",
    "DATASET = 'css_retail'\n",
    "PROJECT_ID = PROJECT\n",
    "JOB_DIR = 'gs://jsw-bucket2/'\n",
    "N_BINS = 10\n",
    "\n",
    "\n",
    "# initialize vertex sdk\n",
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")\n",
    "\n",
    "\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1298f841-26e0-40d6-95e0-5c153f6c62e7",
   "metadata": {},
   "source": [
    "## Create the training tables in Bigquery\n",
    "\n",
    "The SQL only needs to be run once to create the tables. Note the data is a google-internal example and meant to illustrate how to leverage BQ SQL for training custom TFRS models. The data is as follows:\n",
    "\n",
    "1. Users train - this table is a collection of the user's demographics\n",
    "2. Train - this is a creation of user event - we pull from the `users_train` table and also bring in events (purchases)\n",
    "  * Additional events like user queries are recommended at this stage to collect user intent\n",
    "3. Product train - this is essentially a product catalog, used for the product tower training\n",
    "\n",
    "\n",
    "### Customer SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "146cd926-0c33-4643-a075-85fdd5bd38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_data_sql = \"\"\"\n",
    "# CREATE OR REPLACE TABLE `css_retail.users_train` AS\n",
    "# SELECT\n",
    "#     SAFE_CAST(id as STRING) AS userId,\n",
    "#     SAFE_CAST(age as FLOAT64) AS age,\n",
    "#     gender,\n",
    "#     latitude,\n",
    "#     longitude,\n",
    "#     zip,\n",
    "#     traffic_source,\n",
    "#     SAFE_CAST(TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), created_at, DAY) AS FLOAT64) AS customer_lifetime_days\n",
    "# FROM `css_retail.customers` AS customers\n",
    "# \"\"\"\n",
    "# _ = client.query(customer_data_sql)\n",
    "# _.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add15c08-87fc-4b68-8a11-2ff22f5d4d37",
   "metadata": {},
   "source": [
    "### Product SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3447121d-0033-41ec-9e03-e56d9ab62fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_catalog_sql = \"\"\"\n",
    "# CREATE OR REPLACE TABLE `css_retail.product_train` AS \n",
    "# WITH inner_q AS (\n",
    "#     SELECT\n",
    "#         SAFE_CAST(rad.id AS STRING) as productId,\n",
    "#         title,\n",
    "#         description,\n",
    "#         product_metadata.exact_price.original_price AS price,\n",
    "#         ARRAY_TO_STRING(cats.categories, ' ') AS categories\n",
    "#     FROM `css_retail.recommendation_ai_data` AS rad,\n",
    "#     UNNEST(category_hierarchies) AS cats\n",
    "# ) \n",
    "# \tSELECT DISTINCT\n",
    "#     productId,\n",
    "#     title,\n",
    "#     description,\n",
    "#     price,\n",
    "#     categories\n",
    "# FROM inner_q\n",
    "# GROUP BY productId, title, description, price, categories\n",
    "# \"\"\"\n",
    "# ################### LIMIT FOR DEV ##############\n",
    "# _ = client.query(product_catalog_sql)\n",
    "# _.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd35fb-04b7-43a3-9697-a031d1b56b9b",
   "metadata": {},
   "source": [
    "### Event SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6afc1037-f809-4670-b013-046705a99fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# purchase_data_sql = \"\"\"\n",
    "# CREATE OR REPLACE TABLE `css_retail.train` AS\n",
    "# WITH inner_q AS (\n",
    "#     SELECT\n",
    "#         SAFE_CAST(userInfo.userId AS STRING) AS userId,\n",
    "#         SAFE_CAST(eventTime AS TIMESTAMP) AS eventTime,\n",
    "#         productEventDetail.cartId,\n",
    "#         productEventDetail.purchaseTransaction.revenue,\n",
    "#         SAFE_CAST(products.id as string) productId,\n",
    "#         products.quantity,\n",
    "#         products.displayPrice AS price\n",
    "#     FROM `css_retail.purchase_complete` AS purchase,\n",
    "#     UNNEST(productEventDetail.productDetails) AS products\n",
    "# ) SELECT\n",
    "#     user.* ,\n",
    "#     inner_q.* EXCEPT (eventTime, userId, cartId, price, revenue),\n",
    "#     UNIX_MILLIS(eventTime) AS eventTime,\n",
    "#     SAFE_CAST(EXTRACT(HOUR FROM eventTime) AS STRING) AS hour,\n",
    "#     SAFE_CAST(EXTRACT(DAY FROM eventTime) AS STRING) AS day,\n",
    "#     SAFE_CAST(EXTRACT(MONTH FROM eventTime) AS STRING) AS month,\n",
    "#     SAFE_CAST(EXTRACT(DAYOFWEEK FROM eventTime) AS STRING) AS dow,\n",
    "    \n",
    "#     product.* EXCEPT (productId)\n",
    "# FROM inner_q JOIN `css_retail.users_train` as user ON inner_q.userId = user.userId\n",
    "# JOIN `css_retail.product_train` as product ON inner_q.productId = product.productId\n",
    "# \"\"\"\n",
    "# _ = client.query(purchase_data_sql)\n",
    "# _.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b36b0-dcb3-4319-b15c-bd89944b14a8",
   "metadata": {},
   "source": [
    "## Create the tensorflow.io interface for the event and product table in Bigquery\n",
    "\n",
    "Best practices from Google are in this [blog post](https://towardsdatascience.com/how-to-read-bigquery-data-from-tensorflow-2-0-efficiently-9234b69165c8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32c3469c-896f-4c63-ab35-941537d300ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "\n",
    "\n",
    "def read_dataset_prod(client, row_restriction, batch_size=1024):\n",
    "    TABLE_ID = \"product_train\"\n",
    "    COL_NAMES = ['productId', 'title', 'description', 'price', 'categories']\n",
    "    COL_TYPES = [dtypes.string, dtypes.string, dtypes.string, dtypes.float64, dtypes.string]\n",
    "    bqsession = client.read_session(\n",
    "        \"projects/\" + PROJECT_ID,\n",
    "        PROJECT_ID, TABLE_ID, DATASET,\n",
    "        COL_NAMES, COL_TYPES,\n",
    "        requested_streams=2,\n",
    "        row_restriction=row_restriction)\n",
    "    dataset = bqsession.parallel_read_rows()\n",
    "    return dataset.prefetch(1).shuffle(batch_size*10).batch(batch_size)\n",
    "\n",
    "def read_dataset(client, row_restriction, batch_size=1024):\n",
    "    TABLE_ID = \"train\"\n",
    "    COL_NAMES = ['userId', 'age', 'gender', 'latitude', 'longitude', 'zip', 'traffic_source',  'customer_lifetime_days',\n",
    "                'productId', 'quantity', 'eventTime', 'hour', 'day', 'month', 'dow', 'title', 'description', 'price', 'categories']\n",
    "    COL_TYPES = [dtypes.string, dtypes.float64, dtypes.string, dtypes.float64, dtypes.float64, dtypes.string, dtypes.string, dtypes.float64, dtypes.string, \n",
    "                 dtypes.int64, dtypes.int64, dtypes.string, dtypes.string, dtypes.string, dtypes.string, dtypes.string, dtypes.string, dtypes.float64,\n",
    "                dtypes.string]\n",
    "    bqsession = client.read_session(\n",
    "        \"projects/\" + PROJECT_ID,\n",
    "        PROJECT_ID, TABLE_ID, DATASET,\n",
    "        COL_NAMES, COL_TYPES,\n",
    "        requested_streams=2,\n",
    "        row_restriction=row_restriction)\n",
    "    dataset = bqsession.parallel_read_rows()\n",
    "    return dataset.prefetch(1).shuffle(batch_size*10).batch(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010fc7de-71ba-4bff-a3ec-f4e0ac42b70e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Product Model Class\n",
    "\n",
    "Here, we begin to create the custom classes for modeling used in the tfrs framework. The process generally follows\n",
    "\n",
    "1. Design the conceptual architecuture of the two tower classes\n",
    "2. Create vectorizors for high cardnaility variables\n",
    "  * e.g.: Product hierarchy, product SKU/title/id\n",
    "3. Set vectorizor adapts to initialize the naive embeddings\n",
    "  * Be sure to parameterize the data for this step, it is a one-time pass over the data\n",
    "4. Use the vectorizors to create embeddings, user 1d pooling to collect multiple embeddings and average (e.g. collapse multiple terms in search query to one pooled average embedding)\n",
    "5. Add other variables (continuous and low-cardnaility categorical) and concatenate into one embedding\n",
    "6. Build deep layers on top and parameterize with `layer_sizes` string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fed4c1d-ed2f-4328-a900-cafad65c9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, adapt_data):\n",
    "        super().__init__() #necessary to inherit the Model class\n",
    "        \n",
    "        #preprocess stuff\n",
    "        logging.info('Preprocessing Running - product model')\n",
    "        self.sku_count = len(np.unique(\n",
    "            np.concatenate(\n",
    "                list(\n",
    "                    adapt_data.map(lambda x: x[\"productId\"])\n",
    "                )\n",
    "            )\n",
    "        ))\n",
    "        #categorical: sku\n",
    "        self.sku_vectorizor = tf.keras.layers.TextVectorization(max_tokens=self.sku_count,  name='sku_vec', input_shape=())\n",
    "        self.title_vectorizor = tf.keras.layers.TextVectorization(max_tokens=self.sku_count,  name='title_vec', ngrams=2, input_shape=())\n",
    "        self.description_vectorizor = tf.keras.layers.TextVectorization(max_tokens=self.sku_count, name='desc_vec', input_shape=())\n",
    "        self.category_vectorizor = tf.keras.layers.TextVectorization(max_tokens=MAX_TOKENS, name='cat_vec', ngrams=2, input_shape=())\n",
    "\n",
    "\n",
    "        logging.info('Lookups Complete - product model')\n",
    "\n",
    "        #adapt stuff\n",
    "        logging.info('Adapts Running - product model')\n",
    "        self.category_vectorizor.adapt(adapt_data.map(lambda x: x['categories']))\n",
    "        self.title_vectorizor.adapt(adapt_data.map(lambda x: x['title']))\n",
    "        self.description_vectorizor.adapt(adapt_data.map(lambda x: x['description']))\n",
    "        self.sku_vectorizor.adapt(adapt_data.map(lambda x: x['productId']))\n",
    "        \n",
    "        logging.info('Starting Emb Layers - product model')\n",
    "        #embed stuff\n",
    "        self.sku_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.sku_vectorizor,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    self.sku_count+1,\n",
    "                    EMBEDDING_DIM,\n",
    "                    mask_zero=True,\n",
    "                    name=\"sku_emb\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"sku_flat\",\n",
    "                )\n",
    "             ],\n",
    "             name=\"sku_embedding\")\n",
    "        \n",
    "        self.title_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.title_vectorizor,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    self.sku_count+1, \n",
    "                    EMBEDDING_DIM, \n",
    "                    mask_zero=True, \n",
    "                    name=\"title_emb\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"title_flatten\",\n",
    "                )\n",
    "            ], \n",
    "            name=\"title_embedding\"\n",
    "        )\n",
    "        self.description_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.description_vectorizor,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    MAX_TOKENS+1, \n",
    "                    EMBEDDING_DIM, \n",
    "                    mask_zero=True, \n",
    "                    name=\"desc_emb\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"desc_flatten\",\n",
    "                )\n",
    "            ], \n",
    "            name=\"description_embedding\"\n",
    "        )\n",
    "        self.category_embedding = tf.keras.Sequential(\n",
    "           [\n",
    "               self.category_vectorizor,\n",
    "               tf.keras.layers.Embedding(\n",
    "                   MAX_TOKENS+1, \n",
    "                   EMBEDDING_DIM, \n",
    "                   mask_zero=True, \n",
    "                   name=\"category_emb\",\n",
    "               ),\n",
    "               tf.keras.layers.GlobalAveragePooling1D(\n",
    "                   name=\"category_flatten\",\n",
    "               )\n",
    "           ], \n",
    "           name=\"category_embedding\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"dense_layers_product\")\n",
    "        \n",
    "        # Adding weight initialzier\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                activation=\"relu\",\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(\n",
    "                    DROPOUT_RATE\n",
    "                ))\n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(tf.keras.layers.Lambda(\n",
    "            lambda x: tf.nn.l2_normalize(\n",
    "                x,\n",
    "                1,\n",
    "                epsilon=1e-12,\n",
    "                name=\"normalize_dense\"\n",
    "            )\n",
    "        ))\n",
    "    def call(self, data):\n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.description_embedding(data['description']), \n",
    "                self.sku_embedding(data['productId']), \n",
    "                self.category_embedding(data['categories']),  \n",
    "                self.title_embedding(data['title']),  \n",
    "            ], axis=1)\n",
    "        return self.dense_layers(all_embs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b933e20f-f9b3-4d9f-a815-ce377f37bc46",
   "metadata": {},
   "source": [
    "## User Model Class and Data\n",
    "\n",
    "Similar concept as above, note there are low-cardnaility variables like `traffic-source` and `gender`. Do not use embeddings but instead use a pre-established vocabulary with the known class levels.\n",
    "\n",
    "We *do not* create the deep embeddings in the user model as it is a subclass of the event model that creates the user-level embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "274f71b5-a337-4d55-846d-912ff749fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self, adapt_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        logging.info('Preprocessing Running - user model')\n",
    "        \n",
    "        #preprocess stuff\n",
    "        self.user_lookup = tf.keras.layers.TextVectorization(max_tokens=MAX_TOKENS, input_shape=())\n",
    "        logging.info('User Lookup Complete - user model')\n",
    "        \n",
    "        self.gender_vocab = tf.constant(['Female', 'Male'], name=\"gender vocab\")\n",
    "        self.traffic_vocab = tf.constant(['Email', 'Search', 'Display', 'Organic', 'Facebook'], name=\"traffic vocab\")\n",
    "        logging.info('Lifetime Buckets Complete - user model')\n",
    "        self.traffic_source_lookup = tf.keras.layers.TextVectorization(max_tokens=10, input_shape=())\n",
    "        self.zip_lookup = tf.keras.layers.TextVectorization(max_tokens=MAX_TOKENS, input_shape=())\n",
    "        self.gender_lookup = tf.keras.layers.TextVectorization(max_tokens=5, input_shape=())\n",
    "        \n",
    "        logging.info('Adapts Running - user model')\n",
    "        #adapt stuff\n",
    "        self.user_lookup.adapt(adapt_data.map(lambda x: x['userId']))\n",
    "        self.zip_lookup.adapt(adapt_data.map(lambda x: x['zip']))\n",
    "        # self.traffic_source_lookup.adapt(adapt_data.map(lambda x: x['traffic_source']))\n",
    "        self.lt_disc = tf.keras.layers.Discretization(\n",
    "                num_bins=N_BINS,\n",
    "                name=\"lifetime_disc\",\n",
    "            )\n",
    "        self.lt_disc.adapt(adapt_data.map(lambda x: x['customer_lifetime_days']))\n",
    "        \n",
    "        self.age_disc = tf.keras.layers.Discretization(\n",
    "                num_bins=N_BINS,\n",
    "                name='age_disc',\n",
    "            )\n",
    "        self.age_disc.adapt(adapt_data.map(lambda x: x['age']))\n",
    "        \n",
    "        #embed stuff\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            self.user_lookup,\n",
    "            tf.keras.layers.Embedding(\n",
    "                MAX_TOKENS+1,\n",
    "                EMBEDDING_DIM,\n",
    "                mask_zero=True,\n",
    "                name=\"user_emb\",)\n",
    "        ,\n",
    "            tf.keras.layers.GlobalAveragePooling1D(\n",
    "                name=\"user_flat\",\n",
    "                #input_shape=(MAX_TOKENS+1,EMBEDDING_DIM,)\n",
    "            )\n",
    "        ], name=\"user_embedding\")\n",
    "        self.age_embedding = tf.keras.Sequential([\n",
    "            self.age_disc,\n",
    "            tf.keras.layers.Embedding(\n",
    "                N_BINS+1,\n",
    "                EMBEDDING_DIM,\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        \n",
    "        self.lifetime_embedding = tf.keras.Sequential([\n",
    "            self.lt_disc,\n",
    "            tf.keras.layers.Embedding(\n",
    "                N_BINS+1,\n",
    "                EMBEDDING_DIM,\n",
    "            )])\n",
    "        \n",
    "        self.zip_embedding = tf.keras.Sequential([\n",
    "            self.zip_lookup,\n",
    "            tf.keras.layers.Embedding(\n",
    "                MAX_TOKENS+1,\n",
    "                EMBEDDING_DIM,\n",
    "                mask_zero=True,\n",
    "                name=\"zip_emb\",\n",
    "                # input_shape=()\n",
    "            ),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(name=\"zip_flat\"\n",
    "                                                  # input_shape=(MAX_TOKENS+1,EMBEDDING_DIM,)\n",
    "                                                  )\n",
    "        ], name=\"zip_embedding\")\n",
    "        \n",
    "        self.gender_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.gender_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"gender_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"gender_emb\")\n",
    "        \n",
    "        self.traffic_source_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.traffic_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"traffic_source_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"traffic_source_emb\")\n",
    "           \n",
    "    def call(self, data):\n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.user_embedding(data['userId']),\n",
    "                self.age_embedding(data['age']),\n",
    "                self.lifetime_embedding(data['customer_lifetime_days']),\n",
    "                self.traffic_source_embedding(data['traffic_source']),\n",
    "                self.zip_embedding(data['zip']),\n",
    "                self.gender_embedding(data['gender'])], axis=1)\n",
    "        \n",
    "        return all_embs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7517d4-4969-4ab3-9580-bb8509e9f5a2",
   "metadata": {},
   "source": [
    "## Event Model - Bringing together the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c4e3e03-e52e-4840-b2d1-3cf22c535821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, adapt_data):\n",
    "        super().__init__()\n",
    "        logging.info('Preprocessing Running - query model')\n",
    "        ## Bring in user data\n",
    "        logging.info('Obtaining user embeddings')\n",
    "        self.user_embs = UserModel(adapt_data) #for user data, run embeddings\n",
    "        logging.info('finished with user embeddings')\n",
    "        ### preprocess stuff\n",
    "        \n",
    "        self.month_vocab = tf.constant([str(i) for i in range(1,12)], name=\"month_vocab\")\n",
    "        self.day_vocab = tf.constant([str(i) for i in range(1,31)], name=\"day_vocab\")\n",
    "        self.dow_vocab = tf.constant([str(i) for i in range(1,7)], name=\"dow_vocab\")\n",
    "        self.hour_vocab = tf.constant([str(i) for i in range(0,24)], name=\"hour_vocab\")\n",
    "        \n",
    "        logging.info('finished vocabs - EventModel')\n",
    "        \n",
    "        \n",
    "        ### adapt stuff\n",
    "        logging.info('Adapts Complete - EventModel')\n",
    "\n",
    "        ## embed stuff\n",
    "        \n",
    "        self.month_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.month_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"month_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"month_emb\")\n",
    "        \n",
    "        self.hour_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.hour_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"hour_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"hour_emb\")\n",
    "        \n",
    "        self.day_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.day_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"day_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"day_emb\")\n",
    "        \n",
    "        self.dow_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.dow_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"dow_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"dow_emb\")\n",
    "        logging.info('Adapts complete - query model')\n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"dense_layers_query\")\n",
    "\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                activation=\"relu\",\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(\n",
    "                    DROPOUT_RATE\n",
    "                ))\n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(tf.keras.layers.Lambda(\n",
    "            lambda x: tf.nn.l2_normalize(\n",
    "                x,\n",
    "                1,\n",
    "                epsilon=1e-12,\n",
    "                name=\"normalize_dense\"\n",
    "            )\n",
    "        ))\n",
    "\n",
    "# tf.reshape(self.price_normalization(data[\"price\"]), (-1, 1))\n",
    "    def call(self, data):\n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.month_embedding(data['month']), \n",
    "                self.dow_embedding(data['dow']),\n",
    "                self.day_embedding(data['day']), \n",
    "                self.hour_embedding(data['hour']),\n",
    "                self.user_embs(data),\n",
    "                           ]\n",
    "              , axis=1)\n",
    "\n",
    "        return self.dense_layers(all_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4337288-e889-4121-9378-c5821b8cb7b7",
   "metadata": {},
   "source": [
    "## Two-Tower Model\n",
    "\n",
    "Below, we connect the classes into one `tfrs.model.Model` class. This involves passing the adapt data to the model classes previously created (event and product, note user is in event) then define a TFRS task for retrieval `tfrs.task.Retrieval`\n",
    "\n",
    "Then a `compute_loss` method is needed to set and objective (see documentation [here](https://www.tensorflow.org/recommenders/api_docs/python/tfrs/tasks/Retrieval)) for more info\n",
    "\n",
    "Metrics are turned off as it slows down training and forces a pass over the data and candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5a27e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheTwoTowers(tfrs.models.Model):\n",
    "    def __init__(self, layer_sizes, query_adapt_data, cat_adapt_data):\n",
    "        super().__init__()\n",
    "        self.cat_adapt_data = cat_adapt_data.cache()\n",
    "        self.candidate_model = ProductModel(layer_sizes, self.cat_adapt_data)\n",
    "        self.query_model = EventModel(layer_sizes, query_adapt_data)\n",
    "        \n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=self.cat_adapt_data.map(self.candidate_model),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, data, training=False):\n",
    "        query_embeddings = self.query_model(data)\n",
    "        product_embeddings = self.candidate_model(data)\n",
    "\n",
    "        return self.task(\n",
    "            query_embeddings,\n",
    "            product_embeddings,\n",
    "            compute_metrics=not training\n",
    "        )#### turn off metrics to save time on training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a626b-d834-4083-993b-8389220dd8b0",
   "metadata": {},
   "source": [
    "### Helper function to convert a string representing a list to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf8ce8a2-76b4-4ee7-9e06-9ce7c0df87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arch_from_string(arch_string):\n",
    "    q = arch_string.replace(']', '')\n",
    "    q = q.replace('[', '')\n",
    "    q = q.replace(\" \", \"\")\n",
    "    return [int(x) for x in q.split(',')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e85a76-52f6-4a0d-8edb-d42171aadbe0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Script\n",
    "\n",
    "Now much of the work is done and training can begin. One of the details that is not apparent but important is how the loss function works. The retreival task pulls a random sampling of products from the product data as negative examples. These are used to assess the model, and this can be time-consuming, that's why we will be checking the metrics on validation every few epochs\n",
    "\n",
    "Running this can take 5-15 minutes depending on the data size, a full pass is needed for all embedding fields that utilize the adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e49c8631-c446-45a5-bcb2-b8a706474a48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 21:45:40.394744: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2021-12-20 21:45:40.395083: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\n",
      "2021-12-20 21:45:40.527531: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-12-20 21:45:40.527578: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-20 21:45:40.527609: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (demo-notebook): /proc/driver/nvidia/version does not exist\n",
      "2021-12-20 21:45:40.528017: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Setting model adapts and compiling the model\n",
      "INFO:absl:Preprocessing Running - product model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 21:45:43.839765: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:45:43.839824: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Lookups Complete - product model\n",
      "INFO:absl:Adapts Running - product model\n",
      "INFO:absl:Starting Emb Layers - product model\n",
      "INFO:absl:Preprocessing Running - query model\n",
      "INFO:absl:Obtaining user embeddings\n",
      "INFO:absl:Preprocessing Running - user model\n",
      "INFO:absl:User Lookup Complete - user model\n",
      "INFO:absl:Lifetime Buckets Complete - user model\n",
      "INFO:absl:Adapts Running - user model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 21:45:46.725084: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:45:46.725134: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:46:02.207115: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:46:02.207173: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:46:17.778324: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:46:17.778421: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:46:33.015107: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:46:33.015171: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:finished with user embeddings\n",
      "INFO:absl:finished vocabs - EventModel\n",
      "INFO:absl:Adapts Complete - EventModel\n",
      "INFO:absl:Adapts complete - query model\n",
      "INFO:absl:Adapts finish - training next\n"
     ]
    }
   ],
   "source": [
    "client = BigQueryClient()\n",
    "\n",
    "\n",
    "candidates_adapt = read_dataset_prod(client, None, BATCH_SIZE)\n",
    "candidates_adapt = candidates_adapt.map(lambda x: x).cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parsed_dataset_adapt = read_dataset(client, None, BATCH_SIZE)\n",
    "\n",
    "logging.info('Setting model adapts and compiling the model')\n",
    "\n",
    "model = TheTwoTowers( get_arch_from_string(ARCH), query_adapt_data=parsed_dataset_adapt, cat_adapt_data=read_dataset_prod(BigQueryClient(), None, 2048))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(LR))\n",
    "logging.info('Adapts finish - training next')\n",
    "tf.random.set_seed(SEED)\n",
    "#time split at 95th quantile of `eventTime` below\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(JOB_DIR, histogram_freq=0) #histograms are not supported by text-vectorizors yet\n",
    "\n",
    "# test_cache = test.cache()\n",
    "\n",
    "# tf.data.experimental.save(test, 'test_cached')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fa0bf0c-8de5-42e1-9856-814ba625f100",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 21:46:54.222933: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:46:54.222994: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:47:37.717490: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:47:37.717559: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:48:21.353414: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:48:21.353475: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:49:05.997009: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:49:05.997070: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:55:55.077436: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:55:55.077492: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:56:37.510418: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:56:37.510472: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:57:21.199324: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 21:57:21.199407: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:13:55.918083: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:13:55.918148: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:14:39.428692: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:14:39.428762: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:15:22.625612: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:15:22.625679: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:22:57.652862: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:22:57.653294: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:23:39.887992: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:23:39.888054: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:24:22.415417: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:24:22.415495: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:31:52.965879: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:31:52.966364: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:32:34.999914: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:32:34.999978: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:33:17.455470: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:33:17.455540: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:40:44.414335: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:40:44.414402: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:41:26.550540: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:41:26.550603: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:42:08.746283: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:42:08.746348: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:49:32.680597: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:49:32.680653: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:50:14.130862: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:50:14.130997: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:50:55.945532: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:50:55.945594: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:58:22.351719: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:58:22.351787: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:59:05.554306: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:59:05.554371: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:59:48.715347: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 22:59:48.715425: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:07:21.066963: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:07:21.067035: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:08:04.253674: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:08:04.253740: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:08:47.428960: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:08:47.429027: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:16:19.909253: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:16:19.909721: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:17:03.090711: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:17:03.090774: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:17:46.543594: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:17:46.543657: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:25:18.590590: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:25:18.591036: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:26:01.726709: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:26:01.726785: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:26:44.571215: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:26:44.571279: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:34:15.834442: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:34:15.834890: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:34:57.720670: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:34:57.720727: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:35:40.932650: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:35:40.932722: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:43:12.629781: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:43:12.629842: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:43:55.549138: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:43:55.549207: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:44:38.675525: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:44:38.675587: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:52:11.281813: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:52:11.281881: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:52:54.088217: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:52:54.088275: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:53:37.177762: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-20 23:53:37.177836: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:01:08.262089: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:01:08.262580: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:01:51.458262: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:01:51.458330: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:02:34.462843: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:02:34.462903: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:10:07.328782: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:10:07.328847: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:10:49.481500: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:10:49.481561: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:11:31.738406: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:11:31.738473: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:19:02.043158: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:19:02.043649: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:19:44.251402: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:19:44.251494: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:20:26.238319: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:20:26.238395: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:27:52.005822: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:27:52.005909: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:28:34.240087: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:28:34.240144: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:29:16.138920: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:29:16.138983: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:36:43.712570: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:36:43.712630: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:37:26.589695: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:37:26.589763: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:38:08.014077: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:38:08.014143: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:45:34.770162: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:45:34.770246: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:46:16.831502: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:46:16.831558: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:46:58.788675: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:46:58.788735: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:54:21.674809: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:54:21.675259: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:55:03.352517: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:55:03.352583: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:55:45.174909: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 00:55:45.174970: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:03:09.484782: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:03:09.485263: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:03:51.222625: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:03:51.222682: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:04:32.976550: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:04:32.976615: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:12:02.415163: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:12:02.415220: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:12:44.654985: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:12:44.655068: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:13:26.715721: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:13:26.715784: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:20:50.809097: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:20:50.809161: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:21:32.660389: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:21:32.660447: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:22:14.561068: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:22:14.561136: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:29:36.492863: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:29:36.493405: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:30:18.322027: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:30:18.322087: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:31:00.210479: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:31:00.210548: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:38:22.245982: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:38:22.246043: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:39:04.136042: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:39:04.136104: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:39:45.734843: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:39:45.734906: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:47:07.375625: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:47:07.376085: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:47:49.149809: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:47:49.149870: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:48:30.697509: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:48:30.697569: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:55:52.911982: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:55:52.912473: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:56:34.577270: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:56:34.577327: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:57:16.192400: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 01:57:16.192467: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:04:37.507572: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:04:37.507630: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:05:19.505482: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:05:19.505551: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:06:00.390300: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:06:00.390353: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:13:21.859136: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:13:21.859221: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:14:03.551164: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:14:03.551222: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:14:45.223592: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:14:45.223656: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:22:08.909736: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:22:08.910233: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:22:51.425594: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:22:51.425663: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:23:33.678083: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:23:33.678155: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:31:02.916290: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:31:02.916363: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:31:44.997874: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:31:44.997939: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:32:26.700468: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:32:26.700538: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:39:48.985193: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-12-21 02:39:48.985624: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n"
     ]
    }
   ],
   "source": [
    "# test = read_dataset(BigQueryClient(), 'eventTime > 1637994570450', BATCH_SIZE)\n",
    "train = read_dataset(BigQueryClient(), 'eventTime <= 1629906782450', BATCH_SIZE)\n",
    "test_client = BigQueryClient()\n",
    "test = read_dataset(test_client, 'eventTime > 1629906782450', BATCH_SIZE)\n",
    "test = test.cache()\n",
    "\n",
    "layer_history = model.fit(\n",
    "    train,\n",
    "    validation_data=test,\n",
    "    validation_freq=3, # checking on metrics every x epoch\n",
    "    callbacks=[tensorboard_cb],\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03ffdea1-b4ba-4bef-87e7-65426de126c6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/317 [==============================] - 411s 1s/step - factorized_top_k/top_1_categorical_accuracy: 1.4823e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0010 - factorized_top_k/top_10_categorical_accuracy: 0.0020 - factorized_top_k/top_50_categorical_accuracy: 0.0069 - factorized_top_k/top_100_categorical_accuracy: 0.0124 - loss: 1398.0457 - regularization_loss: 0.0000e+00 - total_loss: 1398.0457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.00014823416131548584,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.0010129334405064583,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.0019888083916157484,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.0069052414037287235,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.012365199625492096,\n",
       " 'loss': 228.4626007080078,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 228.4626007080078}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8b47a-617b-4cf8-94e6-6342560c2da6",
   "metadata": {},
   "source": [
    "## Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d98c1b8-f3a6-4818-bfe9-47bf06c2ae9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fcc93763c10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(NUM_EPOCHS), layer_history.history[\"loss\"], label=ARCH)\n",
    "plt.title(\"Loss vs epoch\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss Metric\");\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dc692c-fc2c-44ee-b3f0-a9add3bf29cf",
   "metadata": {},
   "source": [
    "## Save the query/event model\n",
    "\n",
    "These SCaNN indicies can be used just as they were in the bqml-scann example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69c2e572-9d73-4b80-a463-f4b484eba962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow_recommenders.layers.factorized_top_k.ScaNN object at 0x7fcc9361dbd0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow_recommenders.layers.factorized_top_k.ScaNN object at 0x7fcc9361dbd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-21 17:30:44.562364: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: gs://jsw-bucket2/assets\n",
      "INFO:tensorflow:Assets written to: gs://jsw-bucket2/assets\n"
     ]
    }
   ],
   "source": [
    "# Export the candidate model.\n",
    "import tempfile\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "\n",
    "candidates = read_dataset_prod(BigQueryClient(), None, batch_size=1024)\n",
    "scann_index = tfrs.layers.factorized_top_k.ScaNN(model.candidate_model, num_reordering_candidates=1000)\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    path = os.path.join(tmp, \"model\")\n",
    "\n",
    "    # Save the index.\n",
    "    tf.saved_model.save(scann_index, JOB_DIR,  options=tf.saved_model.SaveOptions(namespace_whitelist=[\"Scann\"]))\n",
    "\n",
    "    # Load it back; can also be done in TensorFlow Serving.\n",
    "    loaded = tf.saved_model.load(JOB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d30e40-8db8-41cb-90a3-e4a6461d60b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
