{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6ed4d9-f864-4944-91f1-1f382ea71584",
   "metadata": {},
   "source": [
    "## Setup - install packages and restart kernel to begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff8a74a-ac9e-4a77-954a-1e93c5784b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install  absl-py scann tensorflow-datasets google-cloud-bigquery tensorflow-recommenders google-cloud-aiplatform tensorflow-io --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf930114-e334-451d-8d57-40d084c8edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-bigquery --user --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7f27d-74f1-42c2-a82a-7f97109a25bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Two-Tower Recommendation Example\n",
    "\n",
    "This notebook is intended to show an end-end example of training a two-tower recommendation model using [Tensorflow Recommenders](https://www.tensorflow.org/recommenders). \n",
    "\n",
    "The intended use is to provide a working example of the TFRS library and show how training can produce custom embeddings and models for use with the matching engine. For more info on matching engine, see the `bqml-scann` folder for an end-end example\n",
    "\n",
    "This mostly follows the example of a deep retreival model as found on [this page](https://www.tensorflow.org/recommenders/examples/deep_recommenders) \n",
    "\n",
    "The steps are as follows\n",
    "\n",
    "1. Import libraries and set variables\n",
    "2. Pull data from the `css-retail` example, from the query that creates the training data (see commented BQ SQL)\n",
    "3. Define the two tower's classes - product class for the first model, event class for the second. Event is equivalent to user-query events with a resulting puchase\n",
    "  * Note that the user class is a subclass of event and used to model embeddings at the user level\n",
    "4. Develop the two-tower class and set training objectives using the tfrs tasks (retreival task) to compute loss\n",
    "5. Train the model, establish a Tensorboard to assess performance\n",
    "6. Save the model for use in matching engine (or embeddings)\n",
    "\n",
    "\n",
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f8d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "import numpy as np\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    " \n",
    "    \n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd5463d-8c5a-41a9-8c98-bd9f0ed480b9",
   "metadata": {},
   "source": [
    "## Set Vars\n",
    "\n",
    "Many of these paremeters use `ABSL` as flagging and logging will be critical if a pipeline or distributed training is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92c22d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'two-tower'\n",
    "#PREFIX = 'css_retail'\n",
    "DISPLAY_NAME = f'{PREFIX}-tensorboard'\n",
    "PROJECT= 'vertex-stuff'\n",
    "REGION='us-central1'\n",
    "\n",
    "STAGING_BUCKET = \"\"\"gs://{}_vertex_training\"\"\".format(PROJECT) #lowes-reccomendation-tensorboard-logs-us-central1 - this \n",
    "#TENSORBOARD = 'projects/258043323883/locations/us-central1/tensorboards/4236655796332527616' #note really can only get this after gcloud beta ai tensorboards create...\n",
    "#VERTEX_SA = 'vertex-tb@lowes-reccomendation.iam.gserviceaccount.com'\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "LR = 0.0002 #flags.DEFINE_float(\"LR\", 0.001, \"Learning Rate\")\n",
    "EMBEDDING_DIM = 64 #flags.DEFINE_integer(\"EMBEDDING_DIM\", 15, \"Embedding dimension\")\n",
    "MAX_TOKENS = 300_000 #flags.DEFINE_integer(\"MAX_TOKENS\", 15, \"Max embeddings for query and last_n products\")\n",
    "NUM_EPOCHS = 100 #flags.DEFINE_integer(\"NUM_EPOCHS\", 29, \"Number of epochs\")\n",
    "MODEL_DIR = 'model-dirs' #flags.DEFINE_string(\"MODEL_DIR\", 'model-dirs-lowes', \"GCS Bucket to store the model artifact\")\n",
    "DROPOUT = False #flags.DEFINE_bool(\"DROPOUT\", False, \"Use Dropout - T/F bool type\")\n",
    "DROPOUT_RATE = None #flags.DEFINE_float(\"DROPOUT_RATE\", -1.4, \"Dropout rate only works with DROPOUT=True\")\n",
    "#flags.DEFINE_integer(\"N_PRODUCTS\", 19999, \"number of products considered for embedding\")\n",
    "BATCH_SIZE = 256 #flags.DEFINE_integer(\"BATCH_SIZE\", 1023, \"batch size\")\n",
    "ARCH = '[1000,500,100]' #flags.DEFINE_string(\"ARCH\", '[128,64]', \"deep architecture, expressed as a list of ints in string format - will be parsed into list\")\n",
    "SEED = 8947 #flags.DEFINE_integer(\"SEED\", 41781896, \"random seed\")\n",
    "#flags.DEFINE_string(\"TF_RECORDS_DIR\", \"gs://tfrs-central-a\", \"source data in tfrecord format gcs location\")\n",
    "\n",
    "#TODO FLAG\n",
    "DATASET = 'css_retail'\n",
    "PROJECT_ID = 'vertex-stuff'\n",
    "JOB_DIR = 'gs://jsw-bucket/'\n",
    "N_BINS = 10\n",
    "\n",
    "\n",
    "# initialize vertex sdk\n",
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")\n",
    "\n",
    "\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1298f841-26e0-40d6-95e0-5c153f6c62e7",
   "metadata": {},
   "source": [
    "## Create the training tables in Bigquery\n",
    "\n",
    "The SQL only needs to be run once to create the tables. Note the data is a google-internal example and meant to illustrate how to leverage BQ SQL for training custom TFRS models. The data is as follows:\n",
    "\n",
    "1. Users train - this table is a collection of the user's demographics\n",
    "2. Train - this is a creation of user event - we pull from the `users_train` table and also bring in events (purchases)\n",
    "  * Additional events like user queries are recommended at this stage to collect user intent\n",
    "3. Product train - this is essentially a product catalog, used for the product tower training\n",
    "\n",
    "\n",
    "### Customer SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc25888-25c3-44a0-9668-a0338cb37106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "146cd926-0c33-4643-a075-85fdd5bd38ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 Not found: Dataset matching-engine-blog:css_retail was not found in location US\n\n(job ID: b83a8866-ea99-42c4-b40a-8bf4e9dffdb1)\n\n                                       -----Query Job SQL Follows-----                                       \n\n    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |\n   1:\n   2:CREATE OR REPLACE TABLE `css_retail.users_train` AS\n   3:SELECT\n   4:    SAFE_CAST(id as STRING) AS userId,\n   5:    SAFE_CAST(age as FLOAT64) AS age,\n   6:    gender,\n   7:    latitude,\n   8:    longitude,\n   9:    zip,\n  10:    traffic_source,\n  11:    SAFE_CAST(TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), created_at, DAY) AS FLOAT64) AS customer_lifetime_days\n  12:FROM `css_retail.customers` AS customers\n    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18165/3421366877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \"\"\"\n\u001b[1;32m     14\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustomer_data_sql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[1;32m   1445\u001b[0m                 \u001b[0mdo_get_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_get_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m             \u001b[0mdo_get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGoogleAPICallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0msleep_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             )\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mdo_get_result\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1435\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_job_retry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_retry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1437\u001b[0;31m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m                 \u001b[0;31m# Since the job could already be \"done\" (e.g. got a finished job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/bigquery/job/base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mDEFAULT_RETRY\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"retry\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_AsyncJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcancelled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 Not found: Dataset matching-engine-blog:css_retail was not found in location US\n\n(job ID: b83a8866-ea99-42c4-b40a-8bf4e9dffdb1)\n\n                                       -----Query Job SQL Follows-----                                       \n\n    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |\n   1:\n   2:CREATE OR REPLACE TABLE `css_retail.users_train` AS\n   3:SELECT\n   4:    SAFE_CAST(id as STRING) AS userId,\n   5:    SAFE_CAST(age as FLOAT64) AS age,\n   6:    gender,\n   7:    latitude,\n   8:    longitude,\n   9:    zip,\n  10:    traffic_source,\n  11:    SAFE_CAST(TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), created_at, DAY) AS FLOAT64) AS customer_lifetime_days\n  12:FROM `css_retail.customers` AS customers\n    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |"
     ]
    }
   ],
   "source": [
    "customer_data_sql = \"\"\"\n",
    "CREATE OR REPLACE TABLE `css_retail.users_train` AS\n",
    "SELECT\n",
    "    SAFE_CAST(id as STRING) AS userId,\n",
    "    SAFE_CAST(age as FLOAT64) AS age,\n",
    "    gender,\n",
    "    latitude,\n",
    "    longitude,\n",
    "    zip,\n",
    "    traffic_source,\n",
    "    SAFE_CAST(TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), created_at, DAY) AS FLOAT64) AS customer_lifetime_days\n",
    "FROM `css_retail.customers` AS customers\n",
    "\"\"\"\n",
    "_ = client.query(customer_data_sql)\n",
    "_.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd35fb-04b7-43a3-9697-a031d1b56b9b",
   "metadata": {},
   "source": [
    "### Event SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afc1037-f809-4670-b013-046705a99fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# purchase_data_sql = \"\"\"\n",
    "# CREATE OR REPLACE TABLE `css_retail.train` AS\n",
    "# WITH inner_q AS (\n",
    "#     SELECT\n",
    "#         SAFE_CAST(userInfo.userId AS STRING) AS userId,\n",
    "#         SAFE_CAST(eventTime AS TIMESTAMP) AS eventTime,\n",
    "#         productEventDetail.cartId,\n",
    "#         productEventDetail.purchaseTransaction.revenue,\n",
    "#         SAFE_CAST(products.id as string) productId,\n",
    "#         products.quantity,\n",
    "#         products.displayPrice AS price\n",
    "#     FROM `css_retail.purchase_complete` AS purchase,\n",
    "#     UNNEST(productEventDetail.productDetails) AS products\n",
    "# ) SELECT\n",
    "#     user.* ,\n",
    "#     inner_q.* EXCEPT (eventTime, userId, cartId, price, revenue),\n",
    "#     UNIX_MILLIS(eventTime) AS eventTime,\n",
    "#     SAFE_CAST(EXTRACT(HOUR FROM eventTime) AS STRING) AS hour,\n",
    "#     SAFE_CAST(EXTRACT(DAY FROM eventTime) AS STRING) AS day,\n",
    "#     SAFE_CAST(EXTRACT(MONTH FROM eventTime) AS STRING) AS month,\n",
    "#     SAFE_CAST(EXTRACT(DAYOFWEEK FROM eventTime) AS STRING) AS dow,\n",
    "    \n",
    "#     product.* EXCEPT (productId)\n",
    "# FROM inner_q JOIN `css_retail.users_train` as user ON inner_q.userId = user.userId\n",
    "# JOIN `css_retail.product_train` as product ON inner_q.productId = product.productId\n",
    "# \"\"\"\n",
    "# _ = client.query(purchase_data_sql)\n",
    "# _.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628437e3-cfb8-459a-9d46-9cbfb48e03d5",
   "metadata": {},
   "source": [
    "### Product SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6c3bf-70b5-4b39-8c7f-09984e4729e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = bigquery.Client()\n",
    "# product_catalog_sql = \"\"\"\n",
    "# CREATE OR REPLACE TABLE `css_retail.product_train` AS \n",
    "# WITH inner_q AS (\n",
    "#     SELECT\n",
    "#         SAFE_CAST(rad.id AS STRING) as productId,\n",
    "#         title,\n",
    "#         description,\n",
    "#         product_metadata.exact_price.original_price AS price,\n",
    "#         ARRAY_TO_STRING(cats.categories, ' ') AS categories\n",
    "#     FROM `css_retail.recommendation_ai_data` AS rad,\n",
    "#     UNNEST(category_hierarchies) AS cats\n",
    "# ) \n",
    "# \tSELECT DISTINCT\n",
    "#     productId,\n",
    "#     title,\n",
    "#     description,\n",
    "#     price,\n",
    "#     categories\n",
    "# FROM inner_q\n",
    "# GROUP BY productId, title, description, price, categories\n",
    "# \"\"\"\n",
    "# ################### LIMIT FOR DEV ##############\n",
    "# _ = client.query(product_catalog_sql)\n",
    "# _.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b36b0-dcb3-4319-b15c-bd89944b14a8",
   "metadata": {},
   "source": [
    "## Create the tensorflow.io interface for the event and product table in Bigquery\n",
    "\n",
    "Best practices from Google are in this [blog post](https://towardsdatascience.com/how-to-read-bigquery-data-from-tensorflow-2-0-efficiently-9234b69165c8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c3469c-896f-4c63-ab35-941537d300ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:42:46.317099: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2021-12-20 16:42:46.317447: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\n",
      "2021-12-20 16:42:46.455317: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-12-20 16:42:46.455371: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-20 16:42:46.455404: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (me-demo): /proc/driver/nvidia/version does not exist\n",
      "2021-12-20 16:42:46.456070: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "\n",
    "\n",
    "def read_dataset_prod(client, row_restriction, batch_size=1024):\n",
    "    TABLE_ID = \"product_train\"\n",
    "    COL_NAMES = ['productId', 'title', 'description', 'price', 'categories']\n",
    "    COL_TYPES = [dtypes.string, dtypes.string, dtypes.string, dtypes.float64, dtypes.string]\n",
    "    bqsession = client.read_session(\n",
    "        \"projects/\" + PROJECT_ID,\n",
    "        PROJECT_ID, TABLE_ID, DATASET,\n",
    "        COL_NAMES, COL_TYPES,\n",
    "        requested_streams=2,\n",
    "        row_restriction=row_restriction)\n",
    "    dataset = bqsession.parallel_read_rows()\n",
    "    return dataset.prefetch(1).shuffle(batch_size*10).batch(batch_size)\n",
    "\n",
    "def read_dataset(client, row_restriction, batch_size=1024):\n",
    "    TABLE_ID = \"train\"\n",
    "    COL_NAMES = ['userId', 'age', 'gender', 'latitude', 'longitude', 'zip', 'traffic_source',  'customer_lifetime_days',\n",
    "                'productId', 'quantity', 'eventTime', 'hour', 'day', 'month', 'dow', 'title', 'description', 'price', 'categories']\n",
    "    COL_TYPES = [dtypes.string, dtypes.float64, dtypes.string, dtypes.float64, dtypes.float64, dtypes.string, dtypes.string, dtypes.float64, dtypes.string, \n",
    "                 dtypes.int64, dtypes.int64, dtypes.string, dtypes.string, dtypes.string, dtypes.string, dtypes.string, dtypes.string, dtypes.float64,\n",
    "                dtypes.string]\n",
    "    bqsession = client.read_session(\n",
    "        \"projects/\" + PROJECT_ID,\n",
    "        PROJECT_ID, TABLE_ID, DATASET,\n",
    "        COL_NAMES, COL_TYPES,\n",
    "        requested_streams=2,\n",
    "        row_restriction=row_restriction)\n",
    "    dataset = bqsession.parallel_read_rows()\n",
    "    return dataset.prefetch(1).shuffle(batch_size*10).batch(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a417370c-e6df-4c6a-8684-5a0a8ddd47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_bigquery(table_name, schema, unused, row_restriction=None):\n",
    "#     tensorflow_io_bigquery_client = BigQueryClient()\n",
    "#     read_session = tensorflow_io_bigquery_client.read_session(\n",
    "#         \"projects/\" + PROJECT_ID,\n",
    "#         PROJECT_ID, table_name, DATASET,\n",
    "#     list(field.name for field in schema \n",
    "#         if not field.name in unused),\n",
    "#     list(dtypes.double if field.field_type in ['FLOAT64']\n",
    "#         else dtypes.int64 if field.field_type in ['INT64', 'INT32']\n",
    "#          else dtypes.string for field in schema\n",
    "#         if not field.name in unused),\n",
    "#       requested_streams=10,\n",
    "#     row_restriction=row_restriction)\n",
    "    \n",
    "#     dataset = read_session.parallel_read_rows()\n",
    "#     # parsed_data = dataset.map(lambda x: x)\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010fc7de-71ba-4bff-a3ec-f4e0ac42b70e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Product Model Class\n",
    "\n",
    "Here, we begin to create the custom classes for modeling used in the tfrs framework. The process generally follows\n",
    "\n",
    "1. Design the conceptual architecuture of the two tower classes\n",
    "2. Create vectorizors for high cardnaility variables\n",
    "  * e.g.: Product hierarchy, product SKU/title/id\n",
    "3. Set vectorizor adapts to initialize the naive embeddings\n",
    "  * Be sure to parameterize the data for this step, it is a one-time pass over the data\n",
    "4. Use the vectorizors to create embeddings, user 1d pooling to collect multiple embeddings and average (e.g. collapse multiple terms in search query to one pooled average embedding)\n",
    "5. Add other variables (continuous and low-cardnaility categorical) and concatenate into one embedding\n",
    "6. Build deep layers on top and parameterize with `layer_sizes` string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fed4c1d-ed2f-4328-a900-cafad65c9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, adapt_data):\n",
    "        super().__init__() #necessary to inherit the Model class\n",
    "        \n",
    "        #preprocess stuff\n",
    "        logging.info('Preprocessing Running - product model')\n",
    "        self.sku_count = len(np.unique(\n",
    "            np.concatenate(\n",
    "                list(\n",
    "                    adapt_data.map(lambda x: x[\"productId\"])\n",
    "                )\n",
    "            )\n",
    "        ))\n",
    "        #categorical: sku\n",
    "        self.sku_vectorizor = tf.keras.layers.TextVectorization(max_tokens=self.sku_count,  name='sku_vec', input_shape=())\n",
    "        self.title_vectorizor = tf.keras.layers.TextVectorization(max_tokens=self.sku_count,  name='title_vec', ngrams=2, input_shape=())\n",
    "        self.description_vectorizor = tf.keras.layers.TextVectorization(max_tokens=self.sku_count, name='desc_vec', input_shape=())\n",
    "        self.category_vectorizor = tf.keras.layers.TextVectorization(max_tokens=MAX_TOKENS, name='cat_vec', ngrams=2, input_shape=())\n",
    "\n",
    "        # self.price_normalization = tf.keras.layers.Normalization(axis=None) #not used here but to show how simple scalar is used\n",
    "\n",
    "        logging.info('Lookups Complete - product model')\n",
    "\n",
    "        #adapt stuff\n",
    "        logging.info('Adapts Running - product model')\n",
    "        self.category_vectorizor.adapt(adapt_data.map(lambda x: x['categories']))\n",
    "        self.title_vectorizor.adapt(adapt_data.map(lambda x: x['title']))\n",
    "        self.description_vectorizor.adapt(adapt_data.map(lambda x: x['description']))\n",
    "        self.sku_vectorizor.adapt(adapt_data.map(lambda x: x['productId']))\n",
    "        # self.price_normalization.adapt(adapt_data.map(lambda x: x['price']))\n",
    "        \n",
    "        logging.info('Starting Emb Layers - product model')\n",
    "        #embed stuff\n",
    "        self.sku_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.sku_vectorizor,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    self.sku_count+1,\n",
    "                    EMBEDDING_DIM,\n",
    "                    mask_zero=True,\n",
    "                    name=\"sku_emb\",\n",
    "                    # input_shape=(self.sku_count,),\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"sku_flat\",\n",
    "                    # input_shape=(self.sku_count+1,EMBEDDING_DIM,)\n",
    "                )\n",
    "             ],\n",
    "             name=\"sku_embedding\")\n",
    "        \n",
    "        self.title_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.title_vectorizor,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    self.sku_count+1, \n",
    "                    EMBEDDING_DIM, \n",
    "                    mask_zero=True, \n",
    "                    name=\"title_emb\",\n",
    "                    # input_shape=(self.sku_count,)\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"title_flatten\",\n",
    "                    # input_shape=(self.sku_count+1,EMBEDDING_DIM,)\n",
    "                )\n",
    "            ], \n",
    "            name=\"title_embedding\"\n",
    "        )\n",
    "        self.description_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.description_vectorizor,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    MAX_TOKENS+1, \n",
    "                    EMBEDDING_DIM, \n",
    "                    mask_zero=True, \n",
    "                    name=\"desc_emb\",\n",
    "                    # input_shape=(self.sku_count,)\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"desc_flatten\",\n",
    "                    # input_shape=(self.sku_count+1,EMBEDDING_DIM,)\n",
    "                )\n",
    "            ], \n",
    "            name=\"description_embedding\"\n",
    "        )\n",
    "        self.category_embedding = tf.keras.Sequential(\n",
    "           [\n",
    "               self.category_vectorizor,\n",
    "               tf.keras.layers.Embedding(\n",
    "                   MAX_TOKENS+1, \n",
    "                   EMBEDDING_DIM, \n",
    "                   mask_zero=True, \n",
    "                   name=\"category_emb\",\n",
    "                   # input_shape=(MAX_TOKENS,)\n",
    "               ),\n",
    "               tf.keras.layers.GlobalAveragePooling1D(\n",
    "                   name=\"category_flatten\",\n",
    "                   # input_shape=(MAX_TOKENS+1,EMBEDDING_DIM,)\n",
    "               )\n",
    "           ], \n",
    "           name=\"category_embedding\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"dense_layers_product\")\n",
    "        \n",
    "        # Adding weight initialzier\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                activation=\"relu\",\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(\n",
    "                    DROPOUT_RATE\n",
    "                ))\n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(tf.keras.layers.Lambda(\n",
    "            lambda x: tf.nn.l2_normalize(\n",
    "                x,\n",
    "                1,\n",
    "                epsilon=1e-12,\n",
    "                name=\"normalize_dense\"\n",
    "            )\n",
    "        ))\n",
    "    def call(self, data):\n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.description_embedding(data['description']), \n",
    "                self.sku_embedding(data['productId']), \n",
    "                self.category_embedding(data['categories']),  \n",
    "                self.title_embedding(data['title']),  \n",
    "                # tf.reshape(self.price_normalization(data[\"price\"]), (-1, 1)),\n",
    "            ], axis=1)\n",
    "        return self.dense_layers(all_embs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b933e20f-f9b3-4d9f-a815-ce377f37bc46",
   "metadata": {},
   "source": [
    "## User Model Class and Data\n",
    "\n",
    "Similar concept as above, note there are low-cardnaility variables like `traffic-source` and `gender`. Do not use embeddings but instead use a pre-established vocabulary with the known class levels.\n",
    "\n",
    "We *do not* create the deep embeddings in the user model as it is a subclass of the event model that creates the user-level embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "274f71b5-a337-4d55-846d-912ff749fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self, adapt_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        logging.info('Preprocessing Running - user model')\n",
    "        \n",
    "        #preprocess stuff\n",
    "        self.user_lookup = tf.keras.layers.TextVectorization(max_tokens=MAX_TOKENS, input_shape=())\n",
    "        logging.info('User Lookup Complete - user model')\n",
    "        \n",
    "        self.gender_vocab = tf.constant(['Female', 'Male'], name=\"gender vocab\")\n",
    "        self.traffic_vocab = tf.constant(['Email', 'Search', 'Display', 'Organic', 'Facebook'], name=\"traffic vocab\")\n",
    "        logging.info('Lifetime Buckets Complete - user model')\n",
    "        self.traffic_source_lookup = tf.keras.layers.TextVectorization(max_tokens=10, input_shape=())\n",
    "        self.zip_lookup = tf.keras.layers.TextVectorization(max_tokens=MAX_TOKENS, input_shape=())\n",
    "        self.gender_lookup = tf.keras.layers.TextVectorization(max_tokens=5, input_shape=())\n",
    "        \n",
    "        logging.info('Adapts Running - user model')\n",
    "        #adapt stuff\n",
    "        # self.gender_lookup.adapt(adapt_data.map(lambda x: x['gender']))\n",
    "        self.user_lookup.adapt(adapt_data.map(lambda x: x['userId']))\n",
    "        self.zip_lookup.adapt(adapt_data.map(lambda x: x['zip']))\n",
    "        # self.traffic_source_lookup.adapt(adapt_data.map(lambda x: x['traffic_source']))\n",
    "        self.lt_disc = tf.keras.layers.Discretization(\n",
    "                num_bins=N_BINS,\n",
    "                name=\"lifetime_disc\",\n",
    "                # input_shape=()\n",
    "            )\n",
    "        self.lt_disc.adapt(adapt_data.map(lambda x: x['customer_lifetime_days']))\n",
    "        \n",
    "        self.age_disc = tf.keras.layers.Discretization(\n",
    "                num_bins=N_BINS,\n",
    "                name='age_disc',\n",
    "                # input_shape=()\n",
    "            )\n",
    "        self.age_disc.adapt(adapt_data.map(lambda x: x['age']))\n",
    "        \n",
    "        #embed stuff\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            self.user_lookup,\n",
    "            tf.keras.layers.Embedding(\n",
    "                MAX_TOKENS+1,\n",
    "                EMBEDDING_DIM,\n",
    "                mask_zero=True,\n",
    "                name=\"user_emb\",)\n",
    "                # input_shape=()),\n",
    "        ,\n",
    "            tf.keras.layers.GlobalAveragePooling1D(\n",
    "                name=\"user_flat\",\n",
    "                #input_shape=(MAX_TOKENS+1,EMBEDDING_DIM,)\n",
    "            )\n",
    "        ], name=\"user_embedding\")\n",
    "        self.age_embedding = tf.keras.Sequential([\n",
    "            self.age_disc,\n",
    "            tf.keras.layers.Embedding(\n",
    "                N_BINS+1,\n",
    "                EMBEDDING_DIM,\n",
    "                # input_shape=(N_BINS,)\n",
    "            )\n",
    "        ])\n",
    "        #     ,\n",
    "        #     tf.keras.layers.GlobalAveragePooling1D(\n",
    "        #         name=\"age_flat\",\n",
    "        #         input_shape=(N_BINS+1 ,EMBEDDING_DIM,)\n",
    "        #     )\n",
    "        # ], name=\"age_embedding\")\n",
    "        \n",
    "        self.lifetime_embedding = tf.keras.Sequential([\n",
    "            self.lt_disc,\n",
    "            tf.keras.layers.Embedding(\n",
    "                N_BINS+1,\n",
    "                EMBEDDING_DIM,\n",
    "                # input_shape=(N_BINS,)\n",
    "            )])\n",
    "        #     ,\n",
    "        #     tf.keras.layers.GlobalAveragePooling1D(\n",
    "        #         name=\"lt_flat\",\n",
    "        #         input_shape=(N_BINS+1,EMBEDDING_DIM,)\n",
    "        #     )\n",
    "        # ], name=\"customer_lifetime_embedding\")\n",
    "        \n",
    "        # self.traffic_source_embedding = tf.keras.Sequential([\n",
    "        #     self.traffic_source_lookup,\n",
    "        #     tf.keras.layers.Embedding(\n",
    "        #         11,\n",
    "        #         EMBEDDING_DIM,\n",
    "        #         mask_zero=True,\n",
    "        #         name=\"traffic_source_emb\",\n",
    "        #         # input_shape=()\n",
    "        #     ),\n",
    "        #     tf.keras.layers.GlobalAveragePooling1D(\n",
    "        #         name=\"traffic_source_flat\",\n",
    "        #         # input_shape=(N_BINS+1,EMBEDDING_DIM,)\n",
    "        #     )\n",
    "        # ], name=\"traffic_source_embedding\")\n",
    "        \n",
    "        self.zip_embedding = tf.keras.Sequential([\n",
    "            self.zip_lookup,\n",
    "            tf.keras.layers.Embedding(\n",
    "                MAX_TOKENS+1,\n",
    "                EMBEDDING_DIM,\n",
    "                mask_zero=True,\n",
    "                name=\"zip_emb\",\n",
    "                # input_shape=()\n",
    "            ),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(name=\"zip_flat\"\n",
    "                                                  # input_shape=(MAX_TOKENS+1,EMBEDDING_DIM,)\n",
    "                                                  )\n",
    "        ], name=\"zip_embedding\")\n",
    "        \n",
    "        self.gender_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.gender_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"gender_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"gender_emb\")\n",
    "        \n",
    "        self.traffic_source_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.traffic_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"traffic_source_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"traffic_source_emb\")\n",
    "        \n",
    "#         self.gender_embedding = tf.keras.Sequential([\n",
    "#             self.gender_lookup,\n",
    "#             tf.keras.layers.Embedding(\n",
    "#                 6,\n",
    "#                 EMBEDDING_DIM,\n",
    "#                 mask_zero=True,\n",
    "#                 name=\"gender_emb\",\n",
    "#                 # input_shape=()\n",
    "#             ),\n",
    "#             tf.keras.layers.GlobalAveragePooling1D(\n",
    "#                 name=\"gender_flat\",\n",
    "#                 # input_shape=(5,EMBEDDING_DIM,)\n",
    "                \n",
    "#             )\n",
    "#         ], name=\"gender_embedding\")\n",
    "           \n",
    "    def call(self, data):\n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.user_embedding(data['userId']),\n",
    "                self.age_embedding(data['age']),\n",
    "                self.lifetime_embedding(data['customer_lifetime_days']),\n",
    "                self.traffic_source_embedding(data['traffic_source']),\n",
    "                self.zip_embedding(data['zip']),\n",
    "                self.gender_embedding(data['gender'])], axis=1)\n",
    "        \n",
    "        return all_embs \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7517d4-4969-4ab3-9580-bb8509e9f5a2",
   "metadata": {},
   "source": [
    "## Event Model - Bringing together the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c4e3e03-e52e-4840-b2d1-3cf22c535821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, adapt_data):\n",
    "        super().__init__()\n",
    "        logging.info('Preprocessing Running - query model')\n",
    "        ## Bring in user data\n",
    "        logging.info('Obtaining user embeddings')\n",
    "        self.user_embs = UserModel(adapt_data) #for user data, run embeddings\n",
    "        logging.info('finished with user embeddings')\n",
    "        # self.product_embs = \n",
    "        ### preprocess stuff\n",
    "        \n",
    "        self.month_vocab = tf.constant([str(i) for i in range(1,12)], name=\"month_vocab\")\n",
    "        self.day_vocab = tf.constant([str(i) for i in range(1,31)], name=\"day_vocab\")\n",
    "        self.dow_vocab = tf.constant([str(i) for i in range(1,7)], name=\"dow_vocab\")\n",
    "        self.hour_vocab = tf.constant([str(i) for i in range(0,24)], name=\"hour_vocab\")\n",
    "        \n",
    "        logging.info('finished vocabs - EventModel')\n",
    "        \n",
    "        # self.eventtime_normalization = tf.keras.layers.Normalization()\n",
    "        \n",
    "        ### adapt stuff\n",
    "        # self.eventtime_normalization.adapt(adapt_data.map(lambda x: x['eventTime']))\n",
    "        logging.info('Adapts Complete - EventModel')\n",
    "\n",
    "        ## embed stuff\n",
    "        \n",
    "        self.month_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.month_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"month_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"month_emb\")\n",
    "        \n",
    "        self.hour_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.hour_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"hour_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"hour_emb\")\n",
    "        \n",
    "        self.day_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.day_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"day_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"day_emb\")\n",
    "        \n",
    "        self.dow_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.dow_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"dow_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"dow_emb\")\n",
    "        logging.info('Adapts complete - query model')\n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"dense_layers_query\")\n",
    "\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                activation=\"relu\",\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(\n",
    "                    DROPOUT_RATE\n",
    "                ))\n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(tf.keras.layers.Lambda(\n",
    "            lambda x: tf.nn.l2_normalize(\n",
    "                x,\n",
    "                1,\n",
    "                epsilon=1e-12,\n",
    "                name=\"normalize_dense\"\n",
    "            )\n",
    "        ))\n",
    "\n",
    "# tf.reshape(self.price_normalization(data[\"price\"]), (-1, 1))\n",
    "    def call(self, data):\n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.month_embedding(data['month']), \n",
    "                self.dow_embedding(data['dow']),\n",
    "                self.day_embedding(data['day']), \n",
    "                self.hour_embedding(data['hour']),\n",
    "                self.user_embs(data),\n",
    "                # tf.reshape(self.eventtime_normalization(data['eventTime']), (-1, 1))\n",
    "                           ]\n",
    "              , axis=1)\n",
    "\n",
    "        return self.dense_layers(all_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4337288-e889-4121-9378-c5821b8cb7b7",
   "metadata": {},
   "source": [
    "## Two-Tower Model\n",
    "\n",
    "Below, we connect the classes into one `tfrs.model.Model` class. This involves passing the adapt data to the model classes previously created (event and product, note user is in event) then define a TFRS task for retrieval `tfrs.task.Retrieval`\n",
    "\n",
    "Then a `compute_loss` method is needed to set and objective (see documentation [here](https://www.tensorflow.org/recommenders/api_docs/python/tfrs/tasks/Retrieval)) for more info\n",
    "\n",
    "Metrics are turned off as it slows down training and forces a pass over the data and candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a27e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheTwoTowers(tfrs.models.Model):\n",
    "    def __init__(self, layer_sizes, query_adapt_data, cat_adapt_data):\n",
    "        super().__init__()\n",
    "        self.cat_adapt_data = cat_adapt_data.cache()\n",
    "        self.candidate_model = ProductModel(layer_sizes, self.cat_adapt_data)\n",
    "        self.query_model = EventModel(layer_sizes, query_adapt_data)\n",
    "        \n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=self.cat_adapt_data.map(self.candidate_model),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, data, training=False):\n",
    "        query_embeddings = self.query_model(data)\n",
    "        product_embeddings = self.candidate_model(data)\n",
    "\n",
    "        return self.task(\n",
    "            query_embeddings,\n",
    "            product_embeddings,\n",
    "            compute_metrics=not training\n",
    "        )#### turn off metrics to save time on training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a626b-d834-4083-993b-8389220dd8b0",
   "metadata": {},
   "source": [
    "### Helper function to convert a string representing a list to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf8ce8a2-76b4-4ee7-9e06-9ce7c0df87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arch_from_string(arch_string):\n",
    "    q = arch_string.replace(']', '')\n",
    "    q = q.replace('[', '')\n",
    "    q = q.replace(\" \", \"\")\n",
    "    return [int(x) for x in q.split(',')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e85a76-52f6-4a0d-8edb-d42171aadbe0",
   "metadata": {},
   "source": [
    "## Training Script\n",
    "\n",
    "Now much of the work is done and training can begin. One of the details that is not apparent but important is how the loss function works. The retreival task pulls a random sampling of products from the product data as negative examples. These are used to assess the model, and this can be time-consuming, that's why we will be checking the metrics on validation every few epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e49c8631-c446-45a5-bcb2-b8a706474a48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "Error reading from Cloud BigQuery: Permission denied on resource project vertex-stuff. [Op:IO>BigQueryReadSession]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10334/1515218010.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcandidates_adapt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dataset_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcandidates_adapt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandidates_adapt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10334/1966843953.py\u001b[0m in \u001b[0;36mread_dataset_prod\u001b[0;34m(client, row_restriction, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mCOL_NAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOL_TYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mrequested_streams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         row_restriction=row_restriction)\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbqsession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_read_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_io/python/ops/bigquery_dataset_ops.py\u001b[0m in \u001b[0;36mread_session\u001b[0;34m(self, parent, project_id, table_id, dataset_id, selected_fields, output_types, default_values, row_restriction, requested_streams, data_format)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mdefault_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mrow_restriction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow_restriction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         )\n\u001b[1;32m    214\u001b[0m         return BigQueryReadSession(\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mio_big_query_read_session\u001b[0;34m(client, parent, project_id, table_id, dataset_id, selected_fields, output_types, default_values, requested_streams, data_format, row_restriction, container, shared_name, name)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: Error reading from Cloud BigQuery: Permission denied on resource project vertex-stuff. [Op:IO>BigQueryReadSession]"
     ]
    }
   ],
   "source": [
    "client = BigQueryClient()\n",
    "\n",
    "\n",
    "candidates_adapt = read_dataset_prod(client, None, BATCH_SIZE)\n",
    "candidates_adapt = candidates_adapt.map(lambda x: x).cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parsed_dataset_adapt = read_dataset(client, None, BATCH_SIZE)\n",
    "\n",
    "logging.info('Setting model adapts and compiling the model')\n",
    "\n",
    "model = TheTwoTowers( get_arch_from_string(ARCH), query_adapt_data=parsed_dataset_adapt, cat_adapt_data=read_dataset_prod(BigQueryClient(), None, 2048))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(LR))\n",
    "logging.info('Adapts finish - training next')\n",
    "tf.random.set_seed(SEED)\n",
    "#time split at 95th quantile of `eventTime` below\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(JOB_DIR, histogram_freq=0) #histograms are not supported by text-vectorizors yet\n",
    "\n",
    "# test_cache = test.cache()\n",
    "\n",
    "# tf.data.experimental.save(test, 'test_cached')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0bf0c-8de5-42e1-9856-814ba625f100",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-15 17:54:24.109605: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:54:24.109665: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:54:24.534688: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-11-15 17:54:24.534735: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-11-15 17:54:25.059348: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-11-15 17:54:25.060413: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
      "2021-11-15 17:54:25.192897: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 620 callback api events and 619 activity events. \n",
      "2021-11-15 17:54:25.209098: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-11-15 17:54:25.551443: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25\n",
      "\n",
      "2021-11-15 17:54:25.778130: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25/test1111.trace.json.gz\n",
      "2021-11-15 17:54:25.870679: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25\n",
      "\n",
      "2021-11-15 17:54:26.045287: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25/test1111.memory_profile.json.gz\n",
      "2021-11-15 17:54:26.851816: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25\n",
      "Dumped tool data for xplane.pb to gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25/test1111.xplane.pb\n",
      "Dumped tool data for overview_page.pb to gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25/test1111.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25/test1111.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25/test1111.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25/test1111.kernel_stats.pb\n",
      "\n",
      "2021-11-15 17:54:42.969363: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:54:42.969402: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:54:59.582956: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:54:59.582999: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:55:16.945376: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:55:16.945427: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:00:45.910360: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:00:45.910404: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:01:02.230945: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:01:02.230989: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:01:18.699587: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:01:18.699628: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:07:00.456562: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:07:00.456608: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:07:16.561040: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:07:16.561079: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:07:32.920214: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:07:32.920272: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:13:14.339019: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:13:14.339062: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:13:31.232734: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:13:31.232777: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:13:47.342576: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:13:47.342617: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:19:29.272869: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:19:29.272915: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:19:45.783052: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:19:45.783092: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:20:02.447060: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:20:02.447103: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n"
     ]
    }
   ],
   "source": [
    "# test = read_dataset(BigQueryClient(), 'eventTime > 1637994570450', BATCH_SIZE)\n",
    "train = read_dataset(BigQueryClient(), 'eventTime <= 1629906782450', BATCH_SIZE)\n",
    "test_client = BigQueryClient()\n",
    "test = read_dataset(test_client, 'eventTime > 1629906782450', BATCH_SIZE)\n",
    "test = test.cache()\n",
    "\n",
    "layer_history = model.fit(\n",
    "    train,\n",
    "    validation_data=test,\n",
    "    validation_freq=3,\n",
    "    callbacks=[tensorboard_cb],\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ffdea1-b4ba-4bef-87e7-65426de126c6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate(test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8b47a-617b-4cf8-94e6-6342560c2da6",
   "metadata": {},
   "source": [
    "## Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d907be5-8917-49e7-9614-aad63f7ad566",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41fe0f-53f6-4f62-b963-e08f8c7f50d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled = read_bigquery('train', EVENT_USER_SCHEMA, UNUSED_COLUMNS_U).shuffle(10000).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "# train_records\n",
    "# train = shuffled.skip(310_000).cache()\n",
    "\n",
    "\n",
    "model.evaluate(tf.data.experimental.load('test_cached'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98c1b8-f3a6-4818-bfe9-47bf06c2ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(NUM_EPOCHS), layer_history.history[\"loss\"], label=ARCH)\n",
    "plt.title(\"Loss vs epoch\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss Metric\");\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dc692c-fc2c-44ee-b3f0-a9add3bf29cf",
   "metadata": {},
   "source": [
    "## Save the query/event model\n",
    "\n",
    "These SCaNN indicies can be used just as they were in the bqml-scann example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2e572-9d73-4b80-a463-f4b484eba962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the candidate model.\n",
    "import tempfile\n",
    "candidates = read_bigquery('product_train', PRODUCT_SCHEMA, UNUSED_COLUMNS_P).shuffle(200)\n",
    "scann_index = tfrs.layers.factorized_top_k.ScaNN(model.candidate_model, num_reordering_candidates=1000)\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    path = os.path.join(tmp, \"model\")\n",
    "\n",
    "    # Save the index.\n",
    "    tf.saved_model.save(scann_index, JOB_DIR,  options=tf.saved_model.SaveOptions(namespace_whitelist=[\"Scann\"]))\n",
    "\n",
    "    # Load it back; can also be done in TensorFlow Serving.\n",
    "    loaded = tf.saved_model.load(JOB_DIR)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
