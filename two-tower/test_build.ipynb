{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8a74a-ac9e-4a77-954a-1e93c5784b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install  absl-py scann tensorflow-datasets google-cloud-bigquery tensorflow-recommenders google-cloud-aiplatform tensorflow-io --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7f27d-74f1-42c2-a82a-7f97109a25bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Two-Tower Recommendation Example\n",
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a416c-f1e2-4333-9728-fff9f6048805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export TF_GPU_THREAD_MODE=gpu_private"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f8d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "import numpy as np\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    " \n",
    "    \n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd5463d-8c5a-41a9-8c98-bd9f0ed480b9",
   "metadata": {},
   "source": [
    "## Set Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92c22d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'two-tower'\n",
    "#PREFIX = 'css_retail'\n",
    "DISPLAY_NAME = f'{PREFIX}-tensorboard'\n",
    "PROJECT= 'vertex-stuff'\n",
    "REGION='us-central1'\n",
    "\n",
    "STAGING_BUCKET = \"\"\"gs://{}_vertex_training\"\"\".format(PROJECT) #lowes-reccomendation-tensorboard-logs-us-central1 - this \n",
    "#TENSORBOARD = 'projects/258043323883/locations/us-central1/tensorboards/4236655796332527616' #note really can only get this after gcloud beta ai tensorboards create...\n",
    "#VERTEX_SA = 'vertex-tb@lowes-reccomendation.iam.gserviceaccount.com'\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "LR = 0.0002 #flags.DEFINE_float(\"LR\", 0.001, \"Learning Rate\")\n",
    "EMBEDDING_DIM = 64 #flags.DEFINE_integer(\"EMBEDDING_DIM\", 15, \"Embedding dimension\")\n",
    "MAX_TOKENS = 300_000 #flags.DEFINE_integer(\"MAX_TOKENS\", 15, \"Max embeddings for query and last_n products\")\n",
    "NUM_EPOCHS = 100 #flags.DEFINE_integer(\"NUM_EPOCHS\", 29, \"Number of epochs\")\n",
    "MODEL_DIR = 'model-dirs' #flags.DEFINE_string(\"MODEL_DIR\", 'model-dirs-lowes', \"GCS Bucket to store the model artifact\")\n",
    "DROPOUT = False #flags.DEFINE_bool(\"DROPOUT\", False, \"Use Dropout - T/F bool type\")\n",
    "DROPOUT_RATE = None #flags.DEFINE_float(\"DROPOUT_RATE\", -1.4, \"Dropout rate only works with DROPOUT=True\")\n",
    "#flags.DEFINE_integer(\"N_PRODUCTS\", 19999, \"number of products considered for embedding\")\n",
    "BATCH_SIZE = 256 #flags.DEFINE_integer(\"BATCH_SIZE\", 1023, \"batch size\")\n",
    "ARCH = '[1000,500,100]' #flags.DEFINE_string(\"ARCH\", '[128,64]', \"deep architecture, expressed as a list of ints in string format - will be parsed into list\")\n",
    "SEED = 8947 #flags.DEFINE_integer(\"SEED\", 41781896, \"random seed\")\n",
    "#flags.DEFINE_string(\"TF_RECORDS_DIR\", \"gs://tfrs-central-a\", \"source data in tfrecord format gcs location\")\n",
    "\n",
    "#TODO FLAG\n",
    "DATASET = 'css_retail'\n",
    "PROJECT_ID = 'vertex-stuff'\n",
    "JOB_DIR = 'gs://jsw-bucket/'\n",
    "N_BINS = 10\n",
    "\n",
    "\n",
    "# initialize vertex sdk\n",
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1298f841-26e0-40d6-95e0-5c153f6c62e7",
   "metadata": {},
   "source": [
    "## Shape data for training in BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fc6c3bf-70b5-4b39-8c7f-09984e4729e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = bigquery.Client()\n",
    "# product_catalog_sql = \"\"\"\n",
    "# CREATE OR REPLACE TABLE `css_retail.product_train` AS \n",
    "# WITH inner_q AS (\n",
    "#     SELECT\n",
    "#         SAFE_CAST(rad.id AS STRING) as productId,\n",
    "#         title,\n",
    "#         description,\n",
    "#         product_metadata.exact_price.original_price AS price,\n",
    "#         ARRAY_TO_STRING(cats.categories, ' ') AS categories\n",
    "#     FROM `css_retail.recommendation_ai_data` AS rad,\n",
    "#     UNNEST(category_hierarchies) AS cats\n",
    "# ) \n",
    "# \tSELECT DISTINCT\n",
    "#     productId,\n",
    "#     title,\n",
    "#     description,\n",
    "#     price,\n",
    "#     categories\n",
    "# FROM inner_q\n",
    "# GROUP BY productId, title, description, price, categories\n",
    "# \"\"\"\n",
    "# ################### LIMIT FOR DEV ##############\n",
    "# _ = client.query(product_catalog_sql)\n",
    "# _.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32c3469c-896f-4c63-ab35-941537d300ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "\n",
    "\n",
    "def read_dataset_prod(client, row_restriction, batch_size=1024):\n",
    "    TABLE_ID = \"product_train\"\n",
    "    COL_NAMES = ['productId', 'title', 'description', 'price', 'categories']\n",
    "    COL_TYPES = [dtypes.string, dtypes.string, dtypes.string, dtypes.float64, dtypes.string]\n",
    "    bqsession = client.read_session(\n",
    "        \"projects/\" + PROJECT_ID,\n",
    "        PROJECT_ID, TABLE_ID, DATASET,\n",
    "        COL_NAMES, COL_TYPES,\n",
    "        requested_streams=2,\n",
    "        row_restriction=row_restriction)\n",
    "    dataset = bqsession.parallel_read_rows()\n",
    "    return dataset.prefetch(1).shuffle(batch_size*10).batch(batch_size)\n",
    "\n",
    "def read_dataset(client, row_restriction, batch_size=1024):\n",
    "    TABLE_ID = \"train\"\n",
    "    COL_NAMES = ['userId', 'age', 'gender', 'latitude', 'longitude', 'zip', 'traffic_source',  'customer_lifetime_days',\n",
    "                'productId', 'quantity', 'eventTime', 'hour', 'day', 'month', 'dow', 'title', 'description', 'price', 'categories']\n",
    "    COL_TYPES = [dtypes.string, dtypes.float64, dtypes.string, dtypes.float64, dtypes.float64, dtypes.string, dtypes.string, dtypes.float64, dtypes.string, \n",
    "                 dtypes.int64, dtypes.int64, dtypes.string, dtypes.string, dtypes.string, dtypes.string, dtypes.string, dtypes.string, dtypes.float64,\n",
    "                dtypes.string]\n",
    "    bqsession = client.read_session(\n",
    "        \"projects/\" + PROJECT_ID,\n",
    "        PROJECT_ID, TABLE_ID, DATASET,\n",
    "        COL_NAMES, COL_TYPES,\n",
    "        requested_streams=2,\n",
    "        row_restriction=row_restriction)\n",
    "    dataset = bqsession.parallel_read_rows()\n",
    "    return dataset.prefetch(1).shuffle(batch_size*10).batch(batch_size)\n",
    "\n",
    "client = BigQueryClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a417370c-e6df-4c6a-8684-5a0a8ddd47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_bigquery(table_name, schema, unused, row_restriction=None):\n",
    "#     tensorflow_io_bigquery_client = BigQueryClient()\n",
    "#     read_session = tensorflow_io_bigquery_client.read_session(\n",
    "#         \"projects/\" + PROJECT_ID,\n",
    "#         PROJECT_ID, table_name, DATASET,\n",
    "#     list(field.name for field in schema \n",
    "#         if not field.name in unused),\n",
    "#     list(dtypes.double if field.field_type in ['FLOAT64']\n",
    "#         else dtypes.int64 if field.field_type in ['INT64', 'INT32']\n",
    "#          else dtypes.string for field in schema\n",
    "#         if not field.name in unused),\n",
    "#       requested_streams=10,\n",
    "#     row_restriction=row_restriction)\n",
    "    \n",
    "#     dataset = read_session.parallel_read_rows()\n",
    "#     # parsed_data = dataset.map(lambda x: x)\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010fc7de-71ba-4bff-a3ec-f4e0ac42b70e",
   "metadata": {},
   "source": [
    "## Product Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fed4c1d-ed2f-4328-a900-cafad65c9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, adapt_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        #preprocess stuff\n",
    "        logging.info('Preprocessing Running - product model')\n",
    "        self.sku_count = len(np.unique(\n",
    "            np.concatenate(\n",
    "                list(\n",
    "                    adapt_data.map(lambda x: x[\"productId\"])\n",
    "                )\n",
    "            )\n",
    "        ))\n",
    "        #categorical: sku\n",
    "        self.sku_vectorizor = tf.keras.layers.TextVectorization(max_tokens=self.sku_count,  name='sku_vec', input_shape=())\n",
    "        self.title_vectorizor = tf.keras.layers.TextVectorization(max_tokens=self.sku_count,  name='title_vec', ngrams=2, input_shape=())\n",
    "        self.description_vectorizor = tf.keras.layers.TextVectorization(max_tokens=self.sku_count, name='desc_vec', input_shape=())\n",
    "        self.category_vectorizor = tf.keras.layers.TextVectorization(max_tokens=MAX_TOKENS, name='cat_vec', ngrams=2, input_shape=())\n",
    "\n",
    "        # self.price_normalization = tf.keras.layers.Normalization(axis=None)\n",
    "\n",
    "        logging.info('Lookups Complete - product model')\n",
    "\n",
    "        #adapt stuff\n",
    "        logging.info('Adapts Running - product model')\n",
    "        self.category_vectorizor.adapt(adapt_data.map(lambda x: x['categories']))\n",
    "        self.title_vectorizor.adapt(adapt_data.map(lambda x: x['title']))\n",
    "        self.description_vectorizor.adapt(adapt_data.map(lambda x: x['description']))\n",
    "        self.sku_vectorizor.adapt(adapt_data.map(lambda x: x['productId']))\n",
    "        # self.price_normalization.adapt(adapt_data.map(lambda x: x['price']))\n",
    "        \n",
    "        logging.info('Starting Emb Layers - product model')\n",
    "        #embed stuff\n",
    "        self.sku_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.sku_vectorizor,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    self.sku_count+1,\n",
    "                    EMBEDDING_DIM,\n",
    "                    mask_zero=True,\n",
    "                    name=\"sku_emb\",\n",
    "                    # input_shape=(self.sku_count,),\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"sku_flat\",\n",
    "                    # input_shape=(self.sku_count+1,EMBEDDING_DIM,)\n",
    "                )\n",
    "             ],\n",
    "             name=\"sku_embedding\")\n",
    "        \n",
    "        self.title_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.title_vectorizor,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    self.sku_count+1, \n",
    "                    EMBEDDING_DIM, \n",
    "                    mask_zero=True, \n",
    "                    name=\"title_emb\",\n",
    "                    # input_shape=(self.sku_count,)\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"title_flatten\",\n",
    "                    # input_shape=(self.sku_count+1,EMBEDDING_DIM,)\n",
    "                )\n",
    "            ], \n",
    "            name=\"title_embedding\"\n",
    "        )\n",
    "        self.description_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.description_vectorizor,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    MAX_TOKENS+1, \n",
    "                    EMBEDDING_DIM, \n",
    "                    mask_zero=True, \n",
    "                    name=\"desc_emb\",\n",
    "                    # input_shape=(self.sku_count,)\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"desc_flatten\",\n",
    "                    # input_shape=(self.sku_count+1,EMBEDDING_DIM,)\n",
    "                )\n",
    "            ], \n",
    "            name=\"description_embedding\"\n",
    "        )\n",
    "        self.category_embedding = tf.keras.Sequential(\n",
    "           [\n",
    "               self.category_vectorizor,\n",
    "               tf.keras.layers.Embedding(\n",
    "                   MAX_TOKENS+1, \n",
    "                   EMBEDDING_DIM, \n",
    "                   mask_zero=True, \n",
    "                   name=\"category_emb\",\n",
    "                   # input_shape=(MAX_TOKENS,)\n",
    "               ),\n",
    "               tf.keras.layers.GlobalAveragePooling1D(\n",
    "                   name=\"category_flatten\",\n",
    "                   # input_shape=(MAX_TOKENS+1,EMBEDDING_DIM,)\n",
    "               )\n",
    "           ], \n",
    "           name=\"category_embedding\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"dense_layers_product\")\n",
    "        \n",
    "        # Adding weight initialzier\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                activation=\"relu\",\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(\n",
    "                    DROPOUT_RATE\n",
    "                ))\n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(tf.keras.layers.Lambda(\n",
    "            lambda x: tf.nn.l2_normalize(\n",
    "                x,\n",
    "                1,\n",
    "                epsilon=1e-12,\n",
    "                name=\"normalize_dense\"\n",
    "            )\n",
    "        ))\n",
    "    def call(self, data):\n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.description_embedding(data['description']), \n",
    "                self.sku_embedding(data['productId']), \n",
    "                self.category_embedding(data['categories']),  \n",
    "                self.title_embedding(data['title']),  \n",
    "                # tf.reshape(self.price_normalization(data[\"price\"]), (-1, 1)),\n",
    "            ], axis=1)\n",
    "        return self.dense_layers(all_embs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b933e20f-f9b3-4d9f-a815-ce377f37bc46",
   "metadata": {},
   "source": [
    "## User Model Class and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b8b39ce-2eb9-489b-96e1-0de273b3d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_data_sql = \"\"\"\n",
    "# CREATE OR REPLACE TABLE `css_retail.users_train` AS\n",
    "# SELECT\n",
    "#     SAFE_CAST(id as STRING) AS userId,\n",
    "#     SAFE_CAST(age as FLOAT64) AS age,\n",
    "#     gender,\n",
    "#     latitude,\n",
    "#     longitude,\n",
    "#     zip,\n",
    "#     traffic_source,\n",
    "#     SAFE_CAST(TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), created_at, DAY) AS FLOAT64) AS customer_lifetime_days\n",
    "# FROM `css_retail.customers` AS customers\n",
    "# \"\"\"\n",
    "# _ = client.query(customer_data_sql)\n",
    "# _.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "274f71b5-a337-4d55-846d-912ff749fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self, adapt_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        logging.info('Preprocessing Running - user model')\n",
    "        \n",
    "        #preprocess stuff\n",
    "        self.user_lookup = tf.keras.layers.TextVectorization(max_tokens=MAX_TOKENS, input_shape=())\n",
    "        logging.info('User Lookup Complete - user model')\n",
    "        \n",
    "        self.gender_vocab = tf.constant(['Female', 'Male'], name=\"gender vocab\")\n",
    "        self.traffic_vocab = tf.constant(['Email', 'Search', 'Display', 'Organic', 'Facebook'], name=\"traffic vocab\")\n",
    "        logging.info('Lifetime Buckets Complete - user model')\n",
    "        self.traffic_source_lookup = tf.keras.layers.TextVectorization(max_tokens=10, input_shape=())\n",
    "        self.zip_lookup = tf.keras.layers.TextVectorization(max_tokens=MAX_TOKENS, input_shape=())\n",
    "        self.gender_lookup = tf.keras.layers.TextVectorization(max_tokens=5, input_shape=())\n",
    "        \n",
    "        logging.info('Adapts Running - user model')\n",
    "        #adapt stuff\n",
    "        # self.gender_lookup.adapt(adapt_data.map(lambda x: x['gender']))\n",
    "        self.user_lookup.adapt(adapt_data.map(lambda x: x['userId']))\n",
    "        self.zip_lookup.adapt(adapt_data.map(lambda x: x['zip']))\n",
    "        # self.traffic_source_lookup.adapt(adapt_data.map(lambda x: x['traffic_source']))\n",
    "        self.lt_disc = tf.keras.layers.Discretization(\n",
    "                num_bins=N_BINS,\n",
    "                name=\"lifetime_disc\",\n",
    "                # input_shape=()\n",
    "            )\n",
    "        self.lt_disc.adapt(adapt_data.map(lambda x: x['customer_lifetime_days']))\n",
    "        \n",
    "        self.age_disc = tf.keras.layers.Discretization(\n",
    "                num_bins=N_BINS,\n",
    "                name='age_disc',\n",
    "                # input_shape=()\n",
    "            )\n",
    "        self.age_disc.adapt(adapt_data.map(lambda x: x['age']))\n",
    "        \n",
    "        #embed stuff\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            self.user_lookup,\n",
    "            tf.keras.layers.Embedding(\n",
    "                MAX_TOKENS+1,\n",
    "                EMBEDDING_DIM,\n",
    "                mask_zero=True,\n",
    "                name=\"user_emb\",)\n",
    "                # input_shape=()),\n",
    "        ,\n",
    "            tf.keras.layers.GlobalAveragePooling1D(\n",
    "                name=\"user_flat\",\n",
    "                #input_shape=(MAX_TOKENS+1,EMBEDDING_DIM,)\n",
    "            )\n",
    "        ], name=\"user_embedding\")\n",
    "        self.age_embedding = tf.keras.Sequential([\n",
    "            self.age_disc,\n",
    "            tf.keras.layers.Embedding(\n",
    "                N_BINS+1,\n",
    "                EMBEDDING_DIM,\n",
    "                # input_shape=(N_BINS,)\n",
    "            )\n",
    "        ])\n",
    "        #     ,\n",
    "        #     tf.keras.layers.GlobalAveragePooling1D(\n",
    "        #         name=\"age_flat\",\n",
    "        #         input_shape=(N_BINS+1 ,EMBEDDING_DIM,)\n",
    "        #     )\n",
    "        # ], name=\"age_embedding\")\n",
    "        \n",
    "        self.lifetime_embedding = tf.keras.Sequential([\n",
    "            self.lt_disc,\n",
    "            tf.keras.layers.Embedding(\n",
    "                N_BINS+1,\n",
    "                EMBEDDING_DIM,\n",
    "                # input_shape=(N_BINS,)\n",
    "            )])\n",
    "        #     ,\n",
    "        #     tf.keras.layers.GlobalAveragePooling1D(\n",
    "        #         name=\"lt_flat\",\n",
    "        #         input_shape=(N_BINS+1,EMBEDDING_DIM,)\n",
    "        #     )\n",
    "        # ], name=\"customer_lifetime_embedding\")\n",
    "        \n",
    "        # self.traffic_source_embedding = tf.keras.Sequential([\n",
    "        #     self.traffic_source_lookup,\n",
    "        #     tf.keras.layers.Embedding(\n",
    "        #         11,\n",
    "        #         EMBEDDING_DIM,\n",
    "        #         mask_zero=True,\n",
    "        #         name=\"traffic_source_emb\",\n",
    "        #         # input_shape=()\n",
    "        #     ),\n",
    "        #     tf.keras.layers.GlobalAveragePooling1D(\n",
    "        #         name=\"traffic_source_flat\",\n",
    "        #         # input_shape=(N_BINS+1,EMBEDDING_DIM,)\n",
    "        #     )\n",
    "        # ], name=\"traffic_source_embedding\")\n",
    "        \n",
    "        self.zip_embedding = tf.keras.Sequential([\n",
    "            self.zip_lookup,\n",
    "            tf.keras.layers.Embedding(\n",
    "                MAX_TOKENS+1,\n",
    "                EMBEDDING_DIM,\n",
    "                mask_zero=True,\n",
    "                name=\"zip_emb\",\n",
    "                # input_shape=()\n",
    "            ),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(name=\"zip_flat\"\n",
    "                                                  # input_shape=(MAX_TOKENS+1,EMBEDDING_DIM,)\n",
    "                                                  )\n",
    "        ], name=\"zip_embedding\")\n",
    "        \n",
    "        self.gender_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.gender_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"gender_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"gender_emb\")\n",
    "        \n",
    "        self.traffic_source_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.traffic_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"traffic_source_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"traffic_source_emb\")\n",
    "        \n",
    "#         self.gender_embedding = tf.keras.Sequential([\n",
    "#             self.gender_lookup,\n",
    "#             tf.keras.layers.Embedding(\n",
    "#                 6,\n",
    "#                 EMBEDDING_DIM,\n",
    "#                 mask_zero=True,\n",
    "#                 name=\"gender_emb\",\n",
    "#                 # input_shape=()\n",
    "#             ),\n",
    "#             tf.keras.layers.GlobalAveragePooling1D(\n",
    "#                 name=\"gender_flat\",\n",
    "#                 # input_shape=(5,EMBEDDING_DIM,)\n",
    "                \n",
    "#             )\n",
    "#         ], name=\"gender_embedding\")\n",
    "           \n",
    "    def call(self, data):\n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.user_embedding(data['userId']),\n",
    "                self.age_embedding(data['age']),\n",
    "                self.lifetime_embedding(data['customer_lifetime_days']),\n",
    "                self.traffic_source_embedding(data['traffic_source']),\n",
    "                self.zip_embedding(data['zip']),\n",
    "                self.gender_embedding(data['gender'])], axis=1)\n",
    "        \n",
    "        return all_embs \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7517d4-4969-4ab3-9580-bb8509e9f5a2",
   "metadata": {},
   "source": [
    "## Event Model - Bringing together the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15ff5f40-d9a4-4a01-a1e0-254441d85d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# purchase_data_sql = \"\"\"\n",
    "# CREATE OR REPLACE TABLE `css_retail.train` AS\n",
    "# WITH inner_q AS (\n",
    "#     SELECT\n",
    "#         SAFE_CAST(userInfo.userId AS STRING) AS userId,\n",
    "#         SAFE_CAST(eventTime AS TIMESTAMP) AS eventTime,\n",
    "#         productEventDetail.cartId,\n",
    "#         productEventDetail.purchaseTransaction.revenue,\n",
    "#         SAFE_CAST(products.id as string) productId,\n",
    "#         products.quantity,\n",
    "#         products.displayPrice AS price\n",
    "#     FROM `css_retail.purchase_complete` AS purchase,\n",
    "#     UNNEST(productEventDetail.productDetails) AS products\n",
    "# ) SELECT\n",
    "#     user.* ,\n",
    "#     inner_q.* EXCEPT (eventTime, userId, cartId, price, revenue),\n",
    "#     UNIX_MILLIS(eventTime) AS eventTime,\n",
    "#     SAFE_CAST(EXTRACT(HOUR FROM eventTime) AS STRING) AS hour,\n",
    "#     SAFE_CAST(EXTRACT(DAY FROM eventTime) AS STRING) AS day,\n",
    "#     SAFE_CAST(EXTRACT(MONTH FROM eventTime) AS STRING) AS month,\n",
    "#     SAFE_CAST(EXTRACT(DAYOFWEEK FROM eventTime) AS STRING) AS dow,\n",
    "    \n",
    "#     product.* EXCEPT (productId)\n",
    "# FROM inner_q JOIN `css_retail.users_train` as user ON inner_q.userId = user.userId\n",
    "# JOIN `css_retail.product_train` as product ON inner_q.productId = product.productId\n",
    "# \"\"\"\n",
    "# _ = client.query(purchase_data_sql)\n",
    "# _.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c4e3e03-e52e-4840-b2d1-3cf22c535821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, adapt_data):\n",
    "        super().__init__()\n",
    "        logging.info('Preprocessing Running - query model')\n",
    "        ## Bring in user data\n",
    "        logging.info('Obtaining user embeddings')\n",
    "        self.user_embs = UserModel(adapt_data) #for user data, run embeddings\n",
    "        logging.info('finished with user embeddings')\n",
    "        # self.product_embs = \n",
    "        ### preprocess stuff\n",
    "        \n",
    "        self.month_vocab = tf.constant([str(i) for i in range(1,12)], name=\"month_vocab\")\n",
    "        self.day_vocab = tf.constant([str(i) for i in range(1,31)], name=\"day_vocab\")\n",
    "        self.dow_vocab = tf.constant([str(i) for i in range(1,7)], name=\"dow_vocab\")\n",
    "        self.hour_vocab = tf.constant([str(i) for i in range(0,24)], name=\"hour_vocab\")\n",
    "        \n",
    "        logging.info('finished vocabs - EventModel')\n",
    "        \n",
    "        # self.eventtime_normalization = tf.keras.layers.Normalization()\n",
    "        \n",
    "        ### adapt stuff\n",
    "        # self.eventtime_normalization.adapt(adapt_data.map(lambda x: x['eventTime']))\n",
    "        logging.info('Adapts Complete - EventModel')\n",
    "\n",
    "        ## embed stuff\n",
    "        \n",
    "        self.month_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.month_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"month_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"month_emb\")\n",
    "        \n",
    "        self.hour_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.hour_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"hour_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"hour_emb\")\n",
    "        \n",
    "        self.day_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.day_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"day_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"day_emb\")\n",
    "        \n",
    "        self.dow_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.dow_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"dow_lookup\",\n",
    "                output_mode='count',\n",
    "            input_shape=(1,)\n",
    "            )\n",
    "        ], name=\"dow_emb\")\n",
    "        logging.info('Adapts complete - query model')\n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"dense_layers_query\")\n",
    "\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                activation=\"relu\",\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(\n",
    "                    DROPOUT_RATE\n",
    "                ))\n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(tf.keras.layers.Lambda(\n",
    "            lambda x: tf.nn.l2_normalize(\n",
    "                x,\n",
    "                1,\n",
    "                epsilon=1e-12,\n",
    "                name=\"normalize_dense\"\n",
    "            )\n",
    "        ))\n",
    "\n",
    "# tf.reshape(self.price_normalization(data[\"price\"]), (-1, 1))\n",
    "    def call(self, data):\n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.month_embedding(data['month']), \n",
    "                self.dow_embedding(data['dow']),\n",
    "                self.day_embedding(data['day']), \n",
    "                self.hour_embedding(data['hour']),\n",
    "                self.user_embs(data),\n",
    "                # tf.reshape(self.eventtime_normalization(data['eventTime']), (-1, 1))\n",
    "                           ]\n",
    "              , axis=1)\n",
    "\n",
    "        return self.dense_layers(all_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4337288-e889-4121-9378-c5821b8cb7b7",
   "metadata": {},
   "source": [
    "## Two-Tower Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5a27e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheTwoTowers(tfrs.models.Model):\n",
    "    def __init__(self, layer_sizes, query_adapt_data, cat_adapt_data):\n",
    "        super().__init__()\n",
    "        self.cat_adapt_data = cat_adapt_data.cache()\n",
    "        self.candidate_model = ProductModel(layer_sizes, self.cat_adapt_data)\n",
    "        self.query_model = EventModel(layer_sizes, query_adapt_data)\n",
    "        \n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=self.cat_adapt_data.map(self.candidate_model),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, data, training=False):\n",
    "        query_embeddings = self.query_model(data)\n",
    "        product_embeddings = self.candidate_model(data)\n",
    "\n",
    "        return self.task(\n",
    "            query_embeddings,\n",
    "            product_embeddings,\n",
    "            compute_metrics=not training\n",
    "        )#### turn off metrics to save time on training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf8ce8a2-76b4-4ee7-9e06-9ce7c0df87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arch_from_string(arch_string):\n",
    "    q = arch_string.replace(']', '')\n",
    "    q = q.replace('[', '')\n",
    "    q = q.replace(\" \", \"\")\n",
    "    return [int(x) for x in q.split(',')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e85a76-52f6-4a0d-8edb-d42171aadbe0",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e49c8631-c446-45a5-bcb2-b8a706474a48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Setting model adapts and compiling the model\n",
      "INFO:absl:Preprocessing Running - product model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-15 17:53:31.349025: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:53:31.349067: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Lookups Complete - product model\n",
      "INFO:absl:Adapts Running - product model\n",
      "INFO:absl:Starting Emb Layers - product model\n",
      "INFO:absl:Preprocessing Running - query model\n",
      "INFO:absl:Obtaining user embeddings\n",
      "INFO:absl:Preprocessing Running - user model\n",
      "INFO:absl:User Lookup Complete - user model\n",
      "INFO:absl:Lifetime Buckets Complete - user model\n",
      "INFO:absl:Adapts Running - user model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-15 17:53:33.304759: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:53:33.304810: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:53:45.296772: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:53:45.296819: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:53:56.951182: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:53:56.951237: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:54:08.805798: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:54:08.805852: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:finished with user embeddings\n",
      "INFO:absl:finished vocabs - EventModel\n",
      "INFO:absl:Adapts Complete - EventModel\n",
      "INFO:absl:Adapts complete - query model\n",
      "INFO:absl:Adapts finish - training next\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-15 17:54:21.051075: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-11-15 17:54:21.051113: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-11-15 17:54:21.175842: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-11-15 17:54:21.176008: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n"
     ]
    }
   ],
   "source": [
    "client = BigQueryClient()\n",
    "\n",
    "\n",
    "candidates_adapt = read_dataset_prod(client, None, BATCH_SIZE)\n",
    "candidates_adapt = candidates_adapt.map(lambda x: x).cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parsed_dataset_adapt = read_dataset(client, None, BATCH_SIZE)\n",
    "\n",
    "logging.info('Setting model adapts and compiling the model')\n",
    "\n",
    "model = TheTwoTowers( get_arch_from_string(ARCH), query_adapt_data=parsed_dataset_adapt, cat_adapt_data=read_dataset_prod(BigQueryClient(), None, 2048))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(LR))\n",
    "logging.info('Adapts finish - training next')\n",
    "tf.random.set_seed(SEED)\n",
    "#time split at 95th quantile of `eventTime` below\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(JOB_DIR, histogram_freq=0)\n",
    "\n",
    "# test_cache = test.cache()\n",
    "\n",
    "# tf.data.experimental.save(test, 'test_cached')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0bf0c-8de5-42e1-9856-814ba625f100",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-15 17:54:24.109605: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:54:24.109665: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:54:24.534688: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-11-15 17:54:24.534735: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-11-15 17:54:25.059348: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-11-15 17:54:25.060413: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
      "2021-11-15 17:54:25.192897: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 620 callback api events and 619 activity events. \n",
      "2021-11-15 17:54:25.209098: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-11-15 17:54:25.551443: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25\n",
      "\n",
      "2021-11-15 17:54:25.778130: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25/test1111.trace.json.gz\n",
      "2021-11-15 17:54:25.870679: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25\n",
      "\n",
      "2021-11-15 17:54:26.045287: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25/test1111.memory_profile.json.gz\n",
      "2021-11-15 17:54:26.851816: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25\n",
      "Dumped tool data for xplane.pb to gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25/test1111.xplane.pb\n",
      "Dumped tool data for overview_page.pb to gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25/test1111.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25/test1111.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25/test1111.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to gs://jsw-bucket/train/plugins/profile/2021_11_15_17_54_25/test1111.kernel_stats.pb\n",
      "\n",
      "2021-11-15 17:54:42.969363: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:54:42.969402: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:54:59.582956: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:54:59.582999: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:55:16.945376: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 17:55:16.945427: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:00:45.910360: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:00:45.910404: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:01:02.230945: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:01:02.230989: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:01:18.699587: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:01:18.699628: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:07:00.456562: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:07:00.456608: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:07:16.561040: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:07:16.561079: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:07:32.920214: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:07:32.920272: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:13:14.339019: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:13:14.339062: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:13:31.232734: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:13:31.232777: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:13:47.342576: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:13:47.342617: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:19:29.272869: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:19:29.272915: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:19:45.783052: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:19:45.783092: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:20:02.447060: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2021-11-15 18:20:02.447103: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n"
     ]
    }
   ],
   "source": [
    "# test = read_dataset(BigQueryClient(), 'eventTime > 1637994570450', BATCH_SIZE)\n",
    "train = read_dataset(BigQueryClient(), 'eventTime <= 1629906782450', BATCH_SIZE)\n",
    "test_client = BigQueryClient()\n",
    "test = read_dataset(test_client, 'eventTime > 1629906782450', BATCH_SIZE)\n",
    "test = test.cache()\n",
    "\n",
    "layer_history = model.fit(\n",
    "    train,\n",
    "    validation_data=test,\n",
    "    validation_freq=3,\n",
    "    callbacks=[tensorboard_cb],\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ffdea1-b4ba-4bef-87e7-65426de126c6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate(test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8b47a-617b-4cf8-94e6-6342560c2da6",
   "metadata": {},
   "source": [
    "## Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d907be5-8917-49e7-9614-aad63f7ad566",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41fe0f-53f6-4f62-b963-e08f8c7f50d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled = read_bigquery('train', EVENT_USER_SCHEMA, UNUSED_COLUMNS_U).shuffle(10000).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "# train_records\n",
    "# train = shuffled.skip(310_000).cache()\n",
    "\n",
    "\n",
    "model.evaluate(tf.data.experimental.load('test_cached'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98c1b8-f3a6-4818-bfe9-47bf06c2ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(NUM_EPOCHS), layer_history.history[\"loss\"], label=ARCH)\n",
    "plt.title(\"Loss vs epoch\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss Metric\");\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dc692c-fc2c-44ee-b3f0-a9add3bf29cf",
   "metadata": {},
   "source": [
    "## Save the query/event model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2e572-9d73-4b80-a463-f4b484eba962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the candidate model.\n",
    "import tempfile\n",
    "candidates = read_bigquery('product_train', PRODUCT_SCHEMA, UNUSED_COLUMNS_P).shuffle(200)\n",
    "scann_index = tfrs.layers.factorized_top_k.ScaNN(model.candidate_model, num_reordering_candidates=1000)\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    path = os.path.join(tmp, \"model\")\n",
    "\n",
    "    # Save the index.\n",
    "    tf.saved_model.save(scann_index, JOB_DIR,  options=tf.saved_model.SaveOptions(namespace_whitelist=[\"Scann\"]))\n",
    "\n",
    "    # Load it back; can also be done in TensorFlow Serving.\n",
    "    loaded = tf.saved_model.load(JOB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380d313-9f2c-44d0-b2aa-4520b0613aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the query model.\n",
    "import tempfile\n",
    "candidates = read_bigquery('product_train', PRODUCT_SCHEMA, UNUSED_COLUMNS_P).shuffle(200)\n",
    "scann_index = tfrs.layers.factorized_top_k.ScaNN(model.query_model, num_reordering_candidates=1000)\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    path = os.path.join(tmp, \"model\")\n",
    "\n",
    "    # Save the index.\n",
    "    tf.saved_model.save(scann_index, JOB_DIR,  options=tf.saved_model.SaveOptions(namespace_whitelist=[\"Scann\"]))\n",
    "\n",
    "    # Load it back; can also be done in TensorFlow Serving.\n",
    "    loaded = tf.saved_model.load(JOB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e262ad-9605-4c26-bf26-7857026b7ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
