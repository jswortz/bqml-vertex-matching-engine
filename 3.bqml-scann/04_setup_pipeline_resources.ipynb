{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48149fb2",
   "metadata": {},
   "source": [
    "# Prep for running pipelines\n",
    "### Mostly around:\n",
    "- Component `.yaml` for dataflow embedding export\n",
    "- One-time setup of network resources (e.g.: VPC)\n",
    "- Configuration of BigQuery flex slots (for the bqml modeling compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3ac14",
   "metadata": {},
   "source": [
    "### Dataflow yaml creation\n",
    "\n",
    "Many examples available from [here](https://github.com/kubeflow/pipelines/tree/master/components/gcp/dataflow/launch_template) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39cc0793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataflow-launch_python-component.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataflow-launch_python-component.yaml\n",
    "\n",
    "# Copyright 2018 The Kubeflow Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "name: Launch Python\n",
    "description: |\n",
    "  Launch a self-executing beam python file.\n",
    "metadata:\n",
    "  labels:\n",
    "    add-pod-env: 'true'\n",
    "inputs:\n",
    "  - name: project\n",
    "    description: 'The ID of the GCP project to run the Dataflow job.'\n",
    "    type: String\n",
    "  - name: location\n",
    "    description: 'The GCP region to run the Dataflow job.'\n",
    "    type: String\n",
    "  - name: python_file_path\n",
    "    description: 'The gcs or local path to the python file to run.'\n",
    "    type: String\n",
    "  - name: staging_dir\n",
    "    description: >-\n",
    "      Optional. The GCS directory for keeping staging files.\n",
    "      A random subdirectory will be created under the directory to keep job info\n",
    "      for resuming the job in case of failure and it will be passed as\n",
    "      `staging_location` and `temp_location` command line args of the beam code.\n",
    "    default: ''\n",
    "    type: String\n",
    "  - name: requirements_file_path\n",
    "    description: 'Optional, the gcs or local path to the pip requirements file'\n",
    "    default: ''\n",
    "    type: String\n",
    "  - name: args\n",
    "    description: 'The list of args to pass to the python file.'\n",
    "    default: '[]'\n",
    "    type: typing.List\n",
    "  - name: wait_interval\n",
    "    default: '30'\n",
    "    description: 'Optional wait interval between calls to get job status. Defaults to 30.'\n",
    "    type: Integer\n",
    "outputs:\n",
    "  - name: job_id\n",
    "    description: 'The id of the created dataflow job.'\n",
    "    type: String\n",
    "  - name: MLPipeline UI metadata\n",
    "    type: UI metadata\n",
    "implementation:\n",
    "  container:\n",
    "    image: gcr.io/ml-pipeline/ml-pipeline-gcp:1.7.0-rc.3\n",
    "    command: ['python', '-u', '-m', 'kfp_component.launcher']\n",
    "    args: [\n",
    "      --ui_metadata_path, {outputPath: MLPipeline UI metadata},\n",
    "      kfp_component.google.dataflow, launch_python,\n",
    "      --python_file_path, {inputValue: python_file_path},\n",
    "      --project_id, {inputValue: project},\n",
    "      --region, {inputValue: location},\n",
    "      --staging_dir, {inputValue: staging_dir},\n",
    "      --requirements_file_path, {inputValue: requirements_file_path},\n",
    "      --args, {inputValue: args},\n",
    "      --wait_interval, {inputValue: wait_interval},\n",
    "      --job_id_output_path, {outputPath: job_id},\n",
    "    ]\n",
    "    env:\n",
    "      KFP_POD_NAME: \"{{pod.name}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5098415",
   "metadata": {},
   "source": [
    "### Creation of vpc and other resources here - *ONLY RUN ONCE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5508538",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN ONLY ONCE - NOTE THESE ARE CREATED IN NOTEBOOK 05\n",
    "\n",
    "# PROJECT_ID = 'rec-ai-demo-326116'  # @param {type:\"string\"}\n",
    "# NETWORK_NAME = \"default\"  # @param {type:\"string\"}\n",
    "# PEERING_RANGE_NAME = 'google-reserved-range'\n",
    "# BUCKET = 'rec_bq_jsw' # Change to the bucket you created.\n",
    "\n",
    "NETWORK_NAME = 'default'\n",
    "MATCH_SERVICE_PORT = 10000\n",
    "PEERING_RANGE_NAME='google-reserved-range'\n",
    "PROJECT_ID = \"USER-SET\"  # @param {type:\"string\"}\n",
    "REGION = \"USER-SET\"  # @param {type: \"string\"}\n",
    "# Run this only once - this sets up your sub nets to provide high-speed predictions\n",
    "\n",
    "# Create a VPC network\n",
    "! gcloud compute networks create {NETWORK_NAME} --bgp-routing-mode=regional --subnet-mode=auto --project={PROJECT_ID}\n",
    "\n",
    "# Add necessary firewall rules\n",
    "! gcloud compute firewall-rules create {NETWORK_NAME}-allow-icmp --network {NETWORK_NAME} --priority 65534 --project {PROJECT_ID} --allow icmp\n",
    "\n",
    "! gcloud compute firewall-rules create {NETWORK_NAME}-allow-internal --network {NETWORK_NAME} --priority 65534 --project {PROJECT_ID} --allow all --source-ranges 10.128.0.0/9\n",
    "\n",
    "! gcloud compute firewall-rules create {NETWORK_NAME}-allow-rdp --network {NETWORK_NAME} --priority 65534 --project {PROJECT_ID} --allow tcp:3389\n",
    "\n",
    "! gcloud compute firewall-rules create {NETWORK_NAME}-allow-ssh --network {NETWORK_NAME} --priority 65534 --project {PROJECT_ID} --allow tcp:22\n",
    "\n",
    "# Reserve IP range\n",
    "! gcloud compute addresses create {PEERING_RANGE_NAME} --global --prefix-length=16 --network={NETWORK_NAME} --purpose=VPC_PEERING --project={PROJECT_ID} --description=\"peering range for uCAIP Haystack.\"\n",
    "\n",
    "# Set up peering with service networking\n",
    "! gcloud services vpc-peerings connect --service=servicenetworking.googleapis.com --network={NETWORK_NAME} --ranges={PEERING_RANGE_NAME} --project={PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264d5a2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run one time - set up flex slots on BQ for model computations\n",
    "\n",
    "Background info on BigQuery reservations can be found [here](https://cloud.google.com/bigquery/docs/reservations-intro)\n",
    "\n",
    "It may take a few minutes to run this after enablement of the api (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28cd5eda-18c9-4a06-8860-18f3ca2954d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable bigqueryreservation.googleapis.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887cc970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"USER-SET\"  # @param {type:\"string\"}\n",
    "\n",
    "BQ_REGION = 'US' # Change to your BigQuery region.\n",
    "RESERVATION = 'bqml'\n",
    "SLOTS=10\n",
    "CC_SLOTS = 100\n",
    "RESERVATION_ID = PROJECT_ID + \":\" + BQ_REGION + \".\" + RESERVATION\n",
    "!bq mk --project_id=$PROJECT_ID  --location=$BQ_REGION --capacity_commitment --slots=$CC_SLOTS --plan=FLEX\n",
    "!bq mk --reservation --project_id=$PROJECT_ID --slots=$SLOTS --location=$BQ_REGION $RESERVATION\n",
    "!bq mk --reservation_assignment --reservation_id=$RESERVATION_ID --job_type=ML_EXTERNAL --assignee_type=PROJECT --assignee_id=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8574c75e",
   "metadata": {},
   "source": [
    "### Upload the embeddings and stored procs for use in Vertex Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a81a063-c79b-4702-b9d9-40764c05e205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://jsw-bucket-match/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'jsw-bucket-match' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "BUCKET = 'USER-SET' # Change to the bucket you created as well\n",
    "!gsutil mb -l $REGION gs://$BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b652db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://embeddings_exporter/beam_kfp2.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  4.4 KiB/  4.4 KiB]                                                \n",
      "Operation completed over 1 objects/4.4 KiB.                                      \n",
      "Copying file://sql_scripts/sp_TrainItemMatchingModel.sql [Content-Type=application/x-sql]...\n",
      "Copying file://sql_scripts/sp_ExractEmbeddings.sql [Content-Type=application/x-sql]...\n",
      "Copying file://sql_scripts/sp_ComputePMI.sql [Content-Type=application/x-sql]...\n",
      "/ [3 files][  3.8 KiB/  3.8 KiB]                                                \n",
      "Operation completed over 3 objects/3.8 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Note we will be leveraging code copied up from the embeddings_exporter folder\n",
    "!gsutil cp -r embeddings_exporter/ gs://$BUCKET\n",
    "    \n",
    "#do the same with the sproc files\n",
    "!gsutil cp -r sql_scripts/ gs://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c60daa-94e4-4199-85bf-172d1d6ea605",
   "metadata": {},
   "source": [
    "### Setup dataflow api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cbfa651-950c-4d8c-abda-95ce09f02610",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable dataflow.googleapis.com"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
