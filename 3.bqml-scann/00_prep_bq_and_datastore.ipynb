{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAdAvk8LquRK"
   },
   "source": [
    "# Import the sample data into BigQuery and Datastore\n",
    "\n",
    "This notebook is the first of two notebooks that guide you through completing the prerequisites for running the [Real-time Item-to-item Recommendation with BigQuery ML Matrix Factorization and ScaNN](https://github.com/GoogleCloudPlatform/analytics-componentized-patterns/tree/master/retail/recommendation-system/bqml-scann) solution.\n",
    "\n",
    "Use this notebook to complete the following tasks:\n",
    "\n",
    "1. Importing the `product` table \n",
    "1. Creating the `vw_item_groups` view that contains the item data used to compute item co-occurence.\n",
    "1. Exporting product information to Datastore to make it available for lookup when making similar product recommendations. \n",
    "\n",
    "Before starting this notebook, you must [set up the GCP environment](https://github.com/GoogleCloudPlatform/analytics-componentized-patterns/tree/master/retail/recommendation-system/bqml-scann#set-up-the-gcp-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9tnKiq4q6as"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Install the required Python packages, configure the environment variables, and authenticate your GCP account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSNZnVZbEVO_"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EMtPGiyVtZTj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import apache_beam as beam\n",
    "from apache_beam.io.gcp.datastore.v1new.datastoreio import WriteToDatastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHAObQoaEfC2"
   },
   "source": [
    "### Configure GCP environment settings\n",
    "\n",
    "Update the following variables to reflect the values for your GCP environment:\n",
    "\n",
    "+ `PROJECT_ID`: The ID of the Google Cloud project you are using to implement this solution.\n",
    "+ `BUCKET`: The name of the Cloud Storage bucket you created to use with this solution. The `BUCKET` value should be just the bucket name, so `myBucket` rather than `gs://myBucket`.\n",
    "+ `BQ_REGION`: The region to use for the BigQuery dataset.\n",
    "+ `DF_REGION`: The region to use for the Dataflow job. Choose the same region that you used for the `BQ_REGION` variable to avoid issues around reading/writing in different locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "81apxD89q6Co"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'USER-SET' # Change to your project.\n",
    "BUCKET = 'USER-SET' # Change to the bucket you created.\n",
    "BQ_REGION = 'USER-SET' # Change to your BigQuery region.\n",
    "DF_REGION = 'USER-SET' # Change to your Dataflow region.\n",
    "BQ_DATASET_NAME = 'css_retail'\n",
    "BQ_TABLE_NAME = 'products'\n",
    "DS_KIND = 'product'\n",
    "\n",
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgjoyd3CrBQi"
   },
   "source": [
    "### Authenticate your GCP account\n",
    "This is required if you run the notebook in Colab. If you use an AI Platform notebook, you should already be authenticated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hU_altC3pTmd"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    print(\"Colab user is authenticated.\")\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBlQ3RrinurU"
   },
   "source": [
    "### Create the BigQuery dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLLs69WFEqTU"
   },
   "source": [
    "### Define the Dataflow pipeline\n",
    "\n",
    "The pipeline selectproductsgs where the `track_data_title` field isn't NULL and the `track_data_id` field is greater than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-PAwjZmEwq8"
   },
   "source": [
    "### Run the Dataflow pipeline\n",
    "\n",
    "This pipeline takes approximately 15 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data is using the `CSS_RETAIL` dataset\n",
    "```sql\n",
    "SELECT \n",
    "  USER_ID, \t\n",
    "  ORDER_ID \n",
    "FROM \n",
    "  order_items\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EO6yUMUwuxr-"
   },
   "source": [
    "## Create the `vw_item_groups` view\n",
    "\n",
    "Create the `recommendations.vw_item_groups` view to focus oproductnt data.\n",
    "\n",
    "To adapt this view to your own data, you would need to map your item identifier, for example product SKU, to `item_Id`, and your context identifier, for example purchase order number, to `group_Id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "z4Qya0Mtux6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 969.11query/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery \n",
    "\n",
    "CREATE or REPLACE VIEW `css_retail.vw_item_groups`\n",
    "AS\n",
    "SELECT \n",
    "  userInfo.userID as group_id, \n",
    "  pd.id as item_id \n",
    "FROM \n",
    "  `css_retail.purchase_complete`,\n",
    "  UNNEST(productEventDetail.productDetails) as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wr5mm4J3rehj"
   },
   "source": [
    "## Export product information to Datastore\n",
    "\n",
    "Export data from the `track_title` and `artist` fields to Datastore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvBuPJTcresj"
   },
   "source": [
    "### Define the Dataflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yOANhNR3q_Tc"
   },
   "outputs": [],
   "source": [
    "def create_entity(product_info, kind):\n",
    "\n",
    "    from apache_beam.io.gcp.datastore.v1new.types import Entity\n",
    "    from apache_beam.io.gcp.datastore.v1new.types import Key\n",
    "\n",
    "    product_id = product_info.pop(\"id\")\n",
    "    key = Key([kind, product_id])\n",
    "    product_entity = Entity(key)\n",
    "    product_entity.set_properties(product_info)\n",
    "    return product_entity\n",
    "\n",
    "def run_export_to_datatore_pipeline(args):\n",
    "\n",
    "    query = f'''\n",
    "      SELECT  \n",
    "        id, \n",
    "        MAX(COST) cost, \n",
    "        MAX(DEPARTMENT) department,\n",
    "        MAX(CATEGORY) category,\n",
    "        MAX(SUB_CATEGORY) sub_category,\n",
    "        MAX(NAME) name,\n",
    "        MAX(RETAIL_PRICE) RETAIL_PRICE\n",
    "      FROM \n",
    "        `{BQ_DATASET_NAME}.{BQ_TABLE_NAME}`\n",
    "        GROUP BY ID\n",
    "    '''\n",
    "\n",
    "    pipeline_options = beam.options.pipeline_options.PipelineOptions(**args)\n",
    "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
    "\n",
    "      _ = (\n",
    "        pipeline\n",
    "        | 'ReadFromBigQuery' >> beam.io.Read(beam.io.ReadFromBigQuery(\n",
    "            project=PROJECT_ID, query=query, use_standard_sql=True))\n",
    "        | 'ConvertToDatastoreEntity' >> beam.Map(create_entity, DS_KIND)\n",
    "        | 'WriteToDatastore' >> WriteToDatastore(project=PROJECT_ID)\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cs_Fyqu7rdsU"
   },
   "source": [
    "### Run the Dataflow pipeline\n",
    "\n",
    "This pipeline takes approximately 15 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "D0zcYCsyrdzH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline args are set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "DATASET = 'css_retail'\n",
    "RUNNER = 'DataflowRunner'\n",
    "\n",
    "job_name = f'load-datastore-{datetime.utcnow().strftime(\"%y%m%d%H%M%S\")}'\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': RUNNER,\n",
    "    'project': PROJECT_ID,\n",
    "    'temp_location': f'gs://{BUCKET}/dataflow_tmp',\n",
    "    'region': DF_REGION\n",
    "}\n",
    "\n",
    "print(\"Pipeline args are set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation \"operations/acf.p2-424192748592-e78d75c3-58b0-46fd-ab0a-e87ea5319418\" finished successfully.\n"
     ]
    }
   ],
   "source": [
    "# Enable the service\n",
    "!gcloud services enable datastore.googleapis.com dataflow.googleapis.com\n",
    "\n",
    "### Also enable datastore in the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "74x5kPvQsG7i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-fd373465-a062-4ff0-bf61-3dd5f61d6bd3.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-fd373465-a062-4ff0-bf61-3dd5f61d6bd3.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.15 s, sys: 267 ms, total: 5.41 s\n",
      "Wall time: 57min 20s\n",
      "Pipeline is done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Running pipeline...\")\n",
    "%time run_export_to_datatore_pipeline(args)\n",
    "print(\"Pipeline is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhQ7HVnCBmqc"
   },
   "source": [
    "After running the pipeline, you can view the product entries on the [Datastore Entities page](https://pantheon.corp.google.com/datastore/entities):\n",
    "\n",
    "<img src=\"figures/datastore.png\" style=\"width:600px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9AGcKZRfSc-"
   },
   "source": [
    "## License\n",
    "\n",
    "Copyright 2020 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n",
    "\n",
    "See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "**This is not an official Google product but sample code provided for an educational purpose**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "00_prep_bq_and_datastore.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "tf2-gpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
