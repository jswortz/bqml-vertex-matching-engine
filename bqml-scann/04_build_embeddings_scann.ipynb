{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pk9Wij8ppfNY"
   },
   "source": [
    "## Setup\r\n",
    "\r\n",
    "Import the required libraries, configure the environment variables, and authenticate your GCP account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "id": "M-H_wPdmpfNY",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/jupyter/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/jupyter/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/jupyter/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/jupyter/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/jupyter/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/jupyter/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/jupyter/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: google-cloud-aiplatform in /home/jupyter/.local/lib/python3.7/site-packages (0.7.1)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.4.3-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.20.0)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.42.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.31.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-aiplatform) (20.9)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.19.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (3.16.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2021.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.53.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.35.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (57.4.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.40.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.2.2)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.7.2)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.3.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.10)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/jupyter/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "Installing collected packages: google-cloud-aiplatform\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/home/jupyter/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: google-cloud-aiplatform 0.7.1\n",
      "    Uninstalling google-cloud-aiplatform-0.7.1:\n",
      "      Successfully uninstalled google-cloud-aiplatform-0.7.1\n",
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/jupyter/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx 1.2.0 requires google-cloud-aiplatform<0.8,>=0.5.0, but you have google-cloud-aiplatform 1.4.3 which is incompatible.\n",
      "tfx 1.2.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2, but you have tensorflow 2.6.0 which is incompatible.\u001b[0m\n",
      "Successfully installed google-cloud-aiplatform-1.4.3\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/jupyter/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/jupyter/.local/lib/python3.7/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install google-cloud-aiplatform --upgrade --user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzoXTCD5pfNZ"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_7vHSotqpfNa"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_tk1WOppfNa"
   },
   "source": [
    "### Configure GCP environment settings\r\n",
    "\r\n",
    "Update the following variables to reflect the values for your GCP environment:\r\n",
    "\r\n",
    "+ `PROJECT_ID`: The ID of the Google Cloud project you are using to implement this solution.\r\n",
    "+ `BUCKET`: The name of the Cloud Storage bucket you created to use with this solution. The `BUCKET` value should be just the bucket name, so `myBucket` rather than `gs://myBucket`.\r\n",
    "+ `REGION`: The region to use for the AI Platform Training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FxhEWsL4pfNb"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'rec-ai-demo-326116' # Change to your project.\n",
    "BUCKET = 'rec_bq_jsw' # Change to the bucket you created.\n",
    "REGION = 'us-central1' # Change to your AI Platform Training region.\n",
    "EMBEDDING_FILES_PREFIX = f'gs://{BUCKET}/bqml/item_embeddings/embeddings-*'\n",
    "OUTPUT_INDEX_DIR = f'gs://{BUCKET}/bqml/scann_index'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPdz87S3pfNb"
   },
   "source": [
    "### Authenticate your GCP account\n",
    "This is required if you run the notebook in Colab. If you use an AI Platform notebook, you should already be authenticated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fBXROib7pfNc"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "  print(\"Colab user is authenticated.\")\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaiIfZw9pfNc"
   },
   "source": [
    "## Build the ANN index\r\n",
    "\r\n",
    "Use the `build` method implemented in the [indexer.py](index_builder/builder/indexer.py) module to load the embeddings from the CSV files, create the ANN index model and train it on the embedding data, and save the SavedModel file to Cloud Storage. You pass the following three parameters to this method:\r\n",
    "\r\n",
    "+ `embedding_files_path`, which specifies the Cloud Storage location from which to load the embedding vectors.\r\n",
    "+ `num_leaves`, which provides the value for a hyperparameter that tunes the model based on the trade-off between retrieval latency and recall. A higher `num_leaves` value will use more data and provide better recall, but will also increase latency. If `num_leaves` is set to `None` or `0`, the `num_leaves` value is the square root of the number of items.\r\n",
    "+ `output_dir`, which specifies the Cloud Storage location to write the ANN index SavedModel file to.\r\n",
    "\r\n",
    "Other configuration options for the model are set based on the [rules-of-thumb](https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md#rules-of-thumb) provided by ScaNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwcdyrDiGcep"
   },
   "source": [
    "### Build the index locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "l5TGzINqpfNc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexer started...\n",
      "1 embedding files are found.\n",
      "Loading embeddings in file 1 of 1...\n",
      "2933 embeddings are loaded.\n",
      "Start building the ScaNN index...\n",
      "ScaNN index is built.\n",
      "Saving index as a SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 18:35:20.541242: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-09-20 18:35:20.541294: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-09-20 18:35:20.541317: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tf-26): /proc/driver/nvidia/version does not exist\n",
      "2021-09-20 18:35:20.541676: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-20 18:35:20.548060: I scann/partitioning/partitioner_factory_base.cc:71] Size of sampled dataset for training partition: 2933\n",
      "2021-09-20 18:35:20.560437: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:102] PartitionerFactory ran in 12.261756ms.\n",
      "2021-09-20 18:35:20.711247: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://rec_bq_jsw/bqml/scann_index/assets\n",
      "Index is saved to gs://rec_bq_jsw/bqml/scann_index\n",
      "Saving tokens file...\n",
      "Item file is saved to gs://rec_bq_jsw/bqml/scann_index/tokens.\n",
      "Indexer finished.\n"
     ]
    }
   ],
   "source": [
    "import scann\n",
    "\n",
    "from index_builder.builder import indexer\n",
    "indexer.build(EMBEDDING_FILES_PREFIX, OUTPUT_INDEX_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aonq2MptpfNd"
   },
   "source": [
    "### Build the index using AI Platform Training\r\n",
    "\r\n",
    "Submit an AI Platform Training job to build the ScaNN index at scale. The [index_builder](index_builder) directory contains the expected [training application packaging structure](https://cloud.google.com/ai-platform/training/docs/packaging-trainer) for submitting the AI Platform Training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting index_builder/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile index_builder/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-6\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY builder /builder\n",
    "\n",
    "RUN pip install -r builder/requirements.txt\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"builder.task\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  29.18kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-6\n",
      " ---> b91f525ceaed\n",
      "Step 2/5 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> 7027c1ee8522\n",
      "Step 3/5 : COPY builder /builder\n",
      " ---> 62c581f525a8\n",
      "Step 4/5 : RUN pip install -r builder/requirements.txt\n",
      " ---> Running in 78f66c7d2eef\n",
      "Collecting scann\n",
      "  Downloading scann-1.2.3-cp37-cp37m-manylinux2014_x86_64.whl (10.6 MB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from scann->-r builder/requirements.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: tensorflow~=2.6.0 in /opt/conda/lib/python3.7/site-packages (from scann->-r builder/requirements.txt (line 1)) (2.6.0)\n",
      "Requirement already satisfied: keras~=2.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (2.6.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (0.37.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (0.4.0)\n",
      "Collecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (1.1.2)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (3.3.0)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (2.6.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (3.16.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (1.38.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (0.13.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (2.6.0)\n",
      "Requirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (5.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (1.5.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (57.4.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (4.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (1.26.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow~=2.6.0->scann->-r builder/requirements.txt (line 1)) (3.5.0)\n",
      "Installing collected packages: typing-extensions, six, scann\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.0\n",
      "    Uninstalling typing-extensions-3.10.0.0:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "Successfully installed scann-1.2.3 six-1.15.0 typing-extensions-3.7.4.3\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-io 0.18.0 requires tensorflow-io-gcs-filesystem==0.18.0, which is not installed.\n",
      "tensorflow-transform 1.3.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.13.0 which is incompatible.\n",
      "tensorflow-transform 1.3.0 requires pyarrow<3,>=1, but you have pyarrow 5.0.0 which is incompatible.\n",
      "tensorflow-io 0.18.0 requires tensorflow<2.6.0,>=2.5.0, but you have tensorflow 2.6.0 which is incompatible.\n",
      "black 21.8b0 requires typing-extensions>=3.10.0.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "apache-beam 2.32.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\n",
      "apache-beam 2.32.0 requires pyarrow<5.0.0,>=0.15.1, but you have pyarrow 5.0.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 78f66c7d2eef\n",
      " ---> c61ea2a5c2bb\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"-m\", \"builder.task\"]\n",
      " ---> Running in d2f9b92138c9\n",
      "Removing intermediate container d2f9b92138c9\n",
      " ---> b982e81e2a5b\n",
      "Successfully built b982e81e2a5b\n",
      "Successfully tagged gcr.io/rec-ai-demo-326116/multiworker:scann-demo\n",
      "The push refers to repository [gcr.io/rec-ai-demo-326116/multiworker]\n",
      "\n",
      "\u001b[1Bd793c38f: Preparing \n",
      "\u001b[1B00110b07: Preparing \n",
      "\u001b[1B33e0fe55: Preparing \n",
      "\u001b[1B13b9ece4: Preparing \n",
      "\u001b[1B19a8e49f: Preparing \n",
      "\u001b[1B03c68904: Preparing \n",
      "\u001b[1Bd019f083: Preparing \n",
      "\u001b[1Bf8a77cb2: Preparing \n",
      "\u001b[1B1b6f48b7: Preparing \n",
      "\u001b[1B34304903: Preparing \n",
      "\u001b[1B888feca9: Preparing \n",
      "\u001b[1B925dc9db: Preparing \n",
      "\u001b[1B6b51a209: Preparing \n",
      "\u001b[1B624c1e2a: Preparing \n",
      "\u001b[1B6b37150a: Preparing \n",
      "\u001b[1Bf9bb5809: Preparing \n",
      "\u001b[1Bc23f344f: Preparing \n",
      "\u001b[1Be3e335fa: Preparing \n",
      "\u001b[1Bd17f5e73: Preparing \n",
      "\u001b[1B10cd358e: Preparing \n",
      "\u001b[1Bcb82bbfe: Preparing \n",
      "\u001b[1B4c13e943: Preparing \n",
      "\u001b[1Bde02ee49: Preparing \n",
      "\u001b[1Bcbff0d01: Preparing \n",
      "\u001b[1B147df924: Preparing \n",
      "\u001b[1B694fa88a: Preparing \n",
      "\u001b[1B80d56a4a: Preparing \n",
      "\u001b[1B9aa875ad: Preparing \n",
      "\u001b[1Bbcda12bb: Preparing \n",
      "\u001b[1Be4598d48: Preparing \n",
      "\u001b[1Be2d7dd70: Preparing \n",
      "\u001b[1B0b0c2d4d: Preparing \n",
      "\u001b[33B793c38f: Pushed   56.77MB/56.7MBB\u001b[32A\u001b[2K\u001b[28A\u001b[2K\u001b[24A\u001b[2K\u001b[33A\u001b[2K\u001b[19A\u001b[2K\u001b[16A\u001b[2K\u001b[18A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[6A\u001b[2K\u001b[2A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[32A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2Kscann-demo: digest: sha256:57ee0deabd36fb56c0272341b9edcdc1a3b883fe11d073ea03b94a1bb425e5ed size: 7255\n"
     ]
    }
   ],
   "source": [
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/multiworker:scann-demo\"\n",
    "!docker build -t $IMAGE_URI index_builder\n",
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this utility function will create the job spec - note this could be adapted to include hyperparameters and other key vertex job config objects\n",
    "\n",
    "def prepare_worker_pool_specs(\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type=\"n1-highcpu-16\",\n",
    "    reduction_server_image_uri=b\"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\"machine_type\": machine_type}\n",
    "\n",
    "    container_spec = {\n",
    "        \"image_uri\": image_uri,\n",
    "        \"args\": args,\n",
    "        \"command\": cmd,\n",
    "    }\n",
    "\n",
    "    chief_spec = {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"container_spec\": container_spec,\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": replica_count - 1,\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"container_spec\": container_spec,\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": reduction_server_count,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": reduction_server_machine_type,\n",
    "            },\n",
    "            \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "\n",
    "    return worker_pool_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "# initialize vertex sdk\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=f'{OUTPUT_INDEX_DIR}/jobs/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--embedding-files-path=gs://rec_bq_jsw/bqml/item_embeddings/embeddings-*',\n",
       " '--output-dir=gs://rec_bq_jsw/bqml/scann_index',\n",
       " '--num-leaves=500']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "uCVEI3mjpfNd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://rec_bq_jsw/bqml/scann_index contents...\n",
      "Creating output: gs://rec_bq_jsw/bqml/scann_index\n",
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/733956866731/locations/us-central1/customJobs/8635100330645782528\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/733956866731/locations/us-central1/customJobs/8635100330645782528')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/8635100330645782528?project=733956866731\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/733956866731/locations/us-central1/customJobs/8635100330645782528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/733956866731/locations/us-central1/customJobs/8635100330645782528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/733956866731/locations/us-central1/customJobs/8635100330645782528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/733956866731/locations/us-central1/customJobs/8635100330645782528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/733956866731/locations/us-central1/customJobs/8635100330645782528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/733956866731/locations/us-central1/customJobs/8635100330645782528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/733956866731/locations/us-central1/customJobs/8635100330645782528 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob run completed. Resource name: projects/733956866731/locations/us-central1/customJobs/8635100330645782528\n"
     ]
    }
   ],
   "source": [
    "if tf.io.gfile.exists(OUTPUT_INDEX_DIR):\n",
    "  print(\"Removing {} contents...\".format(OUTPUT_INDEX_DIR))\n",
    "  tf.io.gfile.rmtree(OUTPUT_INDEX_DIR)\n",
    "\n",
    "print(\"Creating output: {}\".format(OUTPUT_INDEX_DIR))\n",
    "tf.io.gfile.makedirs(OUTPUT_INDEX_DIR)\n",
    "\n",
    "timestamp = datetime.utcnow().strftime('%y%m%d%H%M%S')\n",
    "job_name = f'ks_bqml_build_scann_index_{timestamp}'\n",
    "\n",
    "\n",
    "worker_args = [f'--embedding-files-path={EMBEDDING_FILES_PREFIX}',\n",
    "               f'--output-dir={OUTPUT_INDEX_DIR}',\n",
    "               f'--num-leaves=500'\n",
    "            ]\n",
    "WORKER_CMD = [\"python\", \"builder/task.py\"]\n",
    "WORKER_ARGS = worker_args\n",
    "REPLICA_COUNT = 1\n",
    "WORKER_MACHINE_TYPE = \"n1-standard-16\"\n",
    "ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "\n",
    "worker_pool_specs = prepare_worker_pool_specs(\n",
    "    image_uri=IMAGE_URI,\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    ")\n",
    "\n",
    "timestamp = datetime.utcnow().strftime('%y%m%d%H%M%S')\n",
    "job_name = f'ks_bqml_build_scann_index_{timestamp}'\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "staging_bucket=OUTPUT_INDEX_DIR)\n",
    "\n",
    "\n",
    "job.run(sync=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzS533hgpfNe"
   },
   "source": [
    "After the AI Platform Training job finishes, check that the `scann_index` folder has been created in your Cloud Storage bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hgEdM632pfNe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rec_bq_jsw/bqml/scann_index/\n",
      "gs://rec_bq_jsw/bqml/scann_index/saved_model.pb\n",
      "gs://rec_bq_jsw/bqml/scann_index/tokens\n",
      "gs://rec_bq_jsw/bqml/scann_index/assets/\n",
      "gs://rec_bq_jsw/bqml/scann_index/variables/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $OUTPUT_INDEX_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcSPTZeDpfNf"
   },
   "source": [
    "## Test the ANN index\r\n",
    "\r\n",
    "Test the ANN index by using the `ScaNNMatcher` class implemented in the [index_server/matching.py](index_server/matching.py) module.\r\n",
    "\r\n",
    "Run the following code snippets to create an item embedding from random generated values and pass it to `scann_matcher`, which returns the items IDs for the five items that are the approximate nearest neighbors of the embedding you submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nQXdRKV4pfNf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ScaNN index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 19:30:22.895189: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaNN index is loadded.\n"
     ]
    }
   ],
   "source": [
    "from index_server.matching import ScaNNMatcher\n",
    "scann_matcher = ScaNNMatcher(OUTPUT_INDEX_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "xlUc_GsBpfNf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10719', '26718', '7534', '5701', '26608']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = np.random.rand(50)\n",
    "scann_matcher.match(vector, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHXed9gdpfNg"
   },
   "source": [
    "## License\n",
    "\n",
    "Copyright 2020 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n",
    "\n",
    "See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "**This is not an official Google product but sample code provided for an educational purpose**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "04_build_embeddings_scann.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-6.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
