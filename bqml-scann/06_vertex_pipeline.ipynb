{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e399fbb4",
   "metadata": {},
   "source": [
    "# Vertex pipeline\n",
    "\n",
    "This notebook will take the work of defining the BQML PMI matrix factorization and ANN deployment found in the 01 and 02 notebooks, then will create a queryable ScaNN index as seen in notebook 05. If there is a need to convert the keras model, the steps below should explain how to productionize, many of these steps can be explained in this [repo](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/pipelines/pipelines_intro_kfp.ipynb)\n",
    "\n",
    "The goal of this notebook is:\n",
    "1. Define the tasks that will be encapsulated in pipeline components\n",
    "2. Define the pipeline\n",
    "3. Run and monitor the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff4cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "PROJECT_ID = \"rec-ai-demo-326116\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET = 'rec_bq_jsw' # Change to the bucket you created.\n",
    "BUCKET_NAME = f'gs://{BUCKET}'\n",
    "DATASET_NAME=\"css_retail\"\n",
    "embeddings_table_name = 'item_embeddings'\n",
    "output_dir = f'gs://{BUCKET}/bqml/item_embeddings'\n",
    "temp_location = os.path.join(output_dir, 'tmp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5185c04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 733956866731-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# Get your GCP project id from gcloud\n",
    "shell_output = !gcloud auth list 2>/dev/null\n",
    "SERVICE_ACCOUNT = shell_output[2].strip()\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90997b81",
   "metadata": {},
   "source": [
    "### Set service account access for Vertex Pipelines\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188a11dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://rec_bq_jsw/\n",
      "No changes made to gs://rec_bq_jsw/\n"
     ]
    }
   ],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_NAME\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feed5610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "# API service endpoint\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/intro\".format(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867a73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f472c4",
   "metadata": {},
   "source": [
    "## Initialize Vertex SDK for Python\n",
    "Initialize the Vertex SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58777418",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea5e837",
   "metadata": {},
   "source": [
    "### Set up Biq Query DDLs for the pipelines\n",
    "These functions will be leveraged to make bq calls and train the BQML models. Guide found [here](https://medium.com/google-cloud/using-bigquery-and-bigquery-ml-from-kubeflow-pipelines-991a2fa4bea8) - this will be a reusable component to interface with BQ via query strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f7797",
   "metadata": {},
   "source": [
    "### Define Python function-based pipeline components\n",
    "In this tutorial, you define a simple pipeline that has three steps, where each step is defined as a component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c14d6d",
   "metadata": {},
   "source": [
    "### First component: compute the `sp_ComputePMI` stored proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64af4340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f2cc427",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file=\"bqml_scann_pipeline.yaml\", \n",
    "           base_image=\"python:3.7\", \n",
    "           packages_to_install=['google-cloud-bigquery']\n",
    "          )\n",
    "def run_a_bq_call(\n",
    "  project: str, query: str) -> NamedTuple('Outputs', [('COMPLETE_CODE', str)]):\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    bq_client = bigquery.Client(project=project)\n",
    "    r = bq_client.query(query).result()\n",
    "\n",
    "    return (\n",
    "    str(200),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c763e6e0",
   "metadata": {},
   "source": [
    "## Declare the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bab6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41c894d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this loads the sprocs found in {BUCKET}/sql_scripts - loaded up in 00_prep_bq...\n",
    "\n",
    "@component(packages_to_install=['google-cloud-bigquery', 'apache-beam[gcp]', 'gsutil']\n",
    "          )\n",
    "def load_sprocs(\n",
    "  bucket: str, dataset_name: str) -> NamedTuple('Outputs', [('COMPLETE_CODE', str)]):\n",
    "    import os\n",
    "    \n",
    "    sql_scripts = dict()\n",
    "    SQL_SCRIPTS_DIR = f'{bucket}/sql_scripts'\n",
    "    BQ_DATASET_NAME = dataset_name\n",
    "\n",
    "    os.mkdir(\"downloads_\")\n",
    "    os.system(f\"gsutil cp {SQL_SCRIPTS_DIR}/*.sql downloads_/\")\n",
    "    SQL_SCRIPTS_DIR = \"downloads_/\"\n",
    "\n",
    "    for script_file in [file for file in os.listdir(SQL_SCRIPTS_DIR) if '.sql' in file]:\n",
    "        script_file_path = os.path.join(SQL_SCRIPTS_DIR, script_file)\n",
    "        sql_script = open(script_file_path, 'r').read()\n",
    "        sql_script = sql_script.replace('@DATASET_NAME', BQ_DATASET_NAME)\n",
    "        sql_scripts[script_file] = sql_script\n",
    "    for script_file in sql_scripts:\n",
    "        print(f'Executing {script_file} script...')\n",
    "        query = sql_scripts[script_file]\n",
    "        query_job = client.query(query)\n",
    "        result = query_job.result()\n",
    "\n",
    "    return (\n",
    "    str(200),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1429db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_cooc_matrix_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS css_retail.item_cooc\n",
    "AS SELECT 0 AS item1_Id, 0 AS item2_Id, 0 AS cooc, 0 AS pmi;\n",
    "\"\"\"\n",
    "\n",
    "create_bqml_model_query = \"\"\"\n",
    "CREATE MODEL IF NOT EXISTS css_retail.item_matching_model\n",
    "OPTIONS(\n",
    "    MODEL_TYPE='matrix_factorization', \n",
    "    USER_COL='item1_Id', \n",
    "    ITEM_COL='item2_Id',\n",
    "    RATING_COL='score'\n",
    ")\n",
    "AS\n",
    "SELECT 0 AS item1_Id, 0 AS item2_Id, 0 AS score;\n",
    "\"\"\"\n",
    "\n",
    "compute_PMI_query = \"\"\"\n",
    "DECLARE min_item_frequency INT64;\n",
    "DECLARE max_group_size INT64;\n",
    "\n",
    "SET min_item_frequency = 15;\n",
    "SET max_group_size = 100;\n",
    "\n",
    "CALL css_retail.sp_ComputePMI(min_item_frequency, max_group_size);\n",
    "\"\"\"\n",
    "\n",
    "train_item_matching_query = \"\"\"\n",
    "DECLARE dimensions INT64 DEFAULT 50;\n",
    "CALL css_retail.sp_TrainItemMatchingModel(dimensions)\n",
    "\"\"\"\n",
    "\n",
    "extract_embeddings_query = \"\"\"\n",
    "CALL css_retail.sp_ExractEmbeddings() \n",
    "\"\"\"\n",
    "\n",
    "export_embeddings_query = f\"\"\"\n",
    "CREATE TEMP FUNCTION array_int_to_string(int_array ARRAY<FLOAT64>) \n",
    "  RETURNS ARRAY<STRING> LANGUAGE js as \"return int_array.map(x => x+'')\";\n",
    "  \n",
    "EXPORT DATA\n",
    "OPTIONS (uri='{BUCKET_NAME}/bqml/item_embeddings/*.csv',\n",
    "  format='CSV',\n",
    "  overwrite=true) AS\n",
    "select item_id, array_to_string(array_int_to_string(embedding), ',')  as embedding_string \n",
    "from `{PROJECT_ID}.{DATASET_NAME}.item_embeddings`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82b77684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCREATE TEMP FUNCTION array_int_to_string(int_array ARRAY<FLOAT64>) \\n  RETURNS ARRAY<STRING> LANGUAGE js as \"return int_array.map(x => x+\\'\\')\";\\n  \\nEXPORT DATA\\nOPTIONS (uri=\\'gs://rec_bq_jsw/bqml/item_embeddings/*.csv\\',\\n  format=\\'CSV\\',\\n  overwrite=true) AS\\nselect item_id, array_to_string(array_int_to_string(embedding), \\',\\')  as embedding_string \\nfrom `rec-ai-demo-326116.css_retail.item_embeddings`\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_embeddings_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4574b7",
   "metadata": {},
   "source": [
    "### Dataflow beam component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "030f90d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Notebook 2 task - export the trained embeddings to cloud storage\n",
    "# @component(base_image=\"apache/beam_python3.7_sdk\", \n",
    "#            packages_to_install=['google-cloud-bigquery']\n",
    "#           )\n",
    "# def beam_export_to_GCS(project: str, bq_dataset_name: str, embeddings_table_name: str, \n",
    "#         output_dir: str, region: str)-> NamedTuple('Outputs', [('COMPLETE_CODE', str)]):\n",
    "#     import os\n",
    "#     import apache_beam as beam\n",
    "#     from apache_beam.runners.dataflow import dataflow_runner\n",
    "    \n",
    "#     EMBEDDING_FILE_PREFIX = 'embeddings'\n",
    "\n",
    "#     def to_csv(entry):\n",
    "#         item_Id = entry['item_Id']\n",
    "#         embedding = entry['embedding']\n",
    "#         csv_string = f'{item_Id},'\n",
    "#         csv_string += ','.join([str(value) for value in embedding])\n",
    "#         return csv_string\n",
    "    \n",
    "#     def get_query(dataset_name, table_name):\n",
    "#         query = f'''\n",
    "#         SELECT \n",
    "#           item_Id,\n",
    "#           embedding\n",
    "#         FROM \n",
    "#           `{dataset_name}.{table_name}`;\n",
    "#         '''\n",
    "#         return query\n",
    "\n",
    "\n",
    "#     pipeline_options = beam.options.pipeline_options.PipelineOptions(runner='dataflow',\n",
    "#                                                                      project=project,\n",
    "#                                                                      job_name=\"copy-embeddings\",\n",
    "#                                                                      region='US',\n",
    "#                                                                      temp_location=output_dir+\"/tmp\"\n",
    "#                                                                     )\n",
    "#     proj = pipeline_options.get_all_options()['project']\n",
    "    \n",
    "#     with beam.Pipeline(options=pipeline_options) as pipeline:\n",
    "\n",
    "#         query = get_query(bq_dataset_name, embeddings_table_name)\n",
    "#         output_prefix = os.path.join(output_dir, EMBEDDING_FILE_PREFIX)\n",
    "\n",
    "#         _ = (\n",
    "#         pipeline\n",
    "#         | 'ReadFromBigQuery' >> beam.io.ReadFromBigQuery(\n",
    "#             project=proj, query=query, use_standard_sql=True, flatten_results=False)\n",
    "#         | 'ConvertToCsv' >> beam.Map(to_csv)\n",
    "#         | 'WriteToCloudStorage' >> beam.io.WriteToText(\n",
    "#             file_path_prefix = output_prefix,\n",
    "#             file_name_suffix = \".csv\")\n",
    "#         )\n",
    "#     return (\n",
    "#     str(200),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "234d2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"bqml-scann-demo\",\n",
    "             description=\"a bqml matching engine demo\",\n",
    "             pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(project: str=PROJECT_ID, compute_PMI_query: str=compute_PMI_query,\n",
    "            train_item_matching_query: str=train_item_matching_query,\n",
    "            extract_embeddings_query: str=extract_embeddings_query,\n",
    "            dataset_name: str=DATASET_NAME, embeddings_table_name: str=embeddings_table_name,\n",
    "             output_dir: str=output_dir, region: str=REGION,\n",
    "             create_cooc_matrix_query: str = create_cooc_matrix_query,\n",
    "             create_bqml_model_query: str = create_bqml_model_query,\n",
    "             bucket_name: str = BUCKET_NAME,\n",
    "             export_embeddings_query: str = export_embeddings_query\n",
    "):\n",
    "    \n",
    "    ###### NOTEBOOK 0 TASKS\n",
    "    create_cooc_matrix = run_a_bq_call(project, create_cooc_matrix_query)\n",
    "    \n",
    "    create_bqml_model = run_a_bq_call(project, create_bqml_model_query).after(create_cooc_matrix)\n",
    "    \n",
    "    loaded_sprocs = load_sprocs(bucket = bucket_name, dataset_name=dataset_name).after(create_bqml_model)\n",
    "    \n",
    "    ###### NOTEBOOK 1 TASKS\n",
    "    compute_bq_pmi_task = run_a_bq_call(project, compute_PMI_query).after(loaded_sprocs)\n",
    "    \n",
    "    train_bq_item_match_task = run_a_bq_call(project, train_item_matching_query).after(loaded_sprocs)\n",
    "    ###### NOTEBOOK 2 TASKS\n",
    "    create_bq_embeddings_task = run_a_bq_call(project, extract_embeddings_query).after(train_bq_item_match_task)\n",
    "    \n",
    "    export_emb_gcs = run_a_bq_call(project, export_embeddings_query).after(create_bq_embeddings_task)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6989316",
   "metadata": {},
   "source": [
    "### Now that the reusable components are set in a pipeline, declare the paramaters, queries, logic, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea73e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  # noqa: F811\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"intro_pipeline.json\".replace(\" \", \"_\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fda910c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210923190217\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210923190217')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/bqml-scann-demo-20210923190217?project=733956866731\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210923190217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210923190217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210923190217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210923190217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210923190217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210923190217\n"
     ]
    }
   ],
   "source": [
    "DISPLAY_NAME = \"intro_\" + TIMESTAMP\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"intro_pipeline.json\".replace(\" \", \"_\"),\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae14fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bigquery_ddl(project_id: str, query_string: str, \n",
    "    location: str):\n",
    "    \"\"\"\n",
    "    Runs BigQuery query and returns a table/model name\n",
    "    \"\"\"\n",
    "    print(query_string)\n",
    "        \n",
    "    from google.cloud import bigquery\n",
    "    from google.api_core.future import polling\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud.bigquery import retry as bq_retry\n",
    "    \n",
    "    bqclient = bigquery.Client(project=project_id, location=location)\n",
    "    job = bqclient.query(query_string, retry=bq_retry.DEFAULT_RETRY)\n",
    "    job._retry = polling.DEFAULT_RETRY\n",
    "    \n",
    "    while job.running():\n",
    "        from time import sleep\n",
    "        sleep(0.1)\n",
    "        print('Running ...')\n",
    "    return print(\"Job complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bde81deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXPORT DATA\n",
      "OPTIONS (uri='gs://rec_bq_jsw/bqml/item_embeddings/*.csv',\n",
      "  format='CSV',\n",
      "  overwrite=true) AS\n",
      "SELECT * FROM `css_retail.item_embeddings`\n",
      "\n",
      "Running ...\n",
      "Job complete!\n"
     ]
    }
   ],
   "source": [
    "j = run_bigquery_ddl(query_string=export_embeddings_query,\n",
    "                                     project_id=PROJECT_ID,\n",
    "                                     location='US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not j.error_result:\n",
    "    print(\"gj!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.display_name"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
