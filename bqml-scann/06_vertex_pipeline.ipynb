{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9718f3c6",
   "metadata": {},
   "source": [
    "# Vertex pipeline\n",
    "\n",
    "This notebook will take the work of defining the BQML PMI matrix factorization and ANN deployment found in the 01 and 02 notebooks, then will create a queryable ScaNN index as seen in notebook 05. If there is a need to convert the keras model, the steps below should explain how to productionize, many of these steps can be explained in this [repo](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/pipelines/pipelines_intro_kfp.ipynb)\n",
    "\n",
    "The goal of this notebook is:\n",
    "1. Define the tasks that will be encapsulated in pipeline components\n",
    "2. Define the pipeline\n",
    "3. Run and monitor the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ae3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "PROJECT_ID = \"rec-ai-demo-326116\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET = 'rec_bq_jsw' # Change to the bucket you created.\n",
    "BUCKET_NAME = f'gs://{BUCKET}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb8833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 733956866731-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# Get your GCP project id from gcloud\n",
    "shell_output = !gcloud auth list 2>/dev/null\n",
    "SERVICE_ACCOUNT = shell_output[2].strip()\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5519b4f",
   "metadata": {},
   "source": [
    "### Set service account access for Vertex Pipelines\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "522c9e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://rec_bq_jsw/\n",
      "No changes made to gs://rec_bq_jsw/\n"
     ]
    }
   ],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_NAME\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b2571d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "# API service endpoint\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/intro\".format(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "147ba83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dafaec6",
   "metadata": {},
   "source": [
    "## Initialize Vertex SDK for Python\n",
    "Initialize the Vertex SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a1f880",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718bc2e4",
   "metadata": {},
   "source": [
    "### Set up Biq Query DDLs for the pipelines\n",
    "These functions will be leveraged to make bq calls and train the BQML models. Guide found [here](https://medium.com/google-cloud/using-bigquery-and-bigquery-ml-from-kubeflow-pipelines-991a2fa4bea8) - this will be a reusable component to interface with BQ via query strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208bc83",
   "metadata": {},
   "source": [
    "### Define Python function-based pipeline components\n",
    "In this tutorial, you define a simple pipeline that has three steps, where each step is defined as a component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1c9ee2",
   "metadata": {},
   "source": [
    "### First component: compute the `sp_ComputePMI` stored proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36418f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a3d985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file=\"bqml_scann_pipeline.yaml\", \n",
    "           base_image=\"python:3.7\", \n",
    "           packages_to_install=['google-cloud-bigquery']\n",
    "          )\n",
    "def run_a_bq_call(\n",
    "  project: str, query: str) -> NamedTuple('Outputs', [('COMPLETE_CODE', str)]):\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    bq_client = bigquery.Client(project=project)\n",
    "    r = bq_client.query(query).result()\n",
    "\n",
    "    return (\n",
    "    str(r),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24aff82",
   "metadata": {},
   "source": [
    "## Declare the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "602f2018",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_PMI_query = \"\"\"\n",
    "DECLARE min_item_frequency INT64;\n",
    "DECLARE max_group_size INT64;\n",
    "\n",
    "SET min_item_frequency = 15;\n",
    "SET max_group_size = 100;\n",
    "\n",
    "CALL css_retail.sp_ComputePMI(min_item_frequency, max_group_size);\n",
    "\"\"\"\n",
    "\n",
    "train_item_matching_query = \"\"\"\n",
    "DECLARE dimensions INT64 DEFAULT 50;\n",
    "CALL css_retail.sp_TrainItemMatchingModel(dimensions)\n",
    "\"\"\"\n",
    "\n",
    "extract_embeddings_query = \"\"\"\n",
    "CALL css_retail.sp_ExractEmbeddings() \n",
    "\"\"\"\n",
    "\n",
    "export_embeddings_query = f\"\"\"\n",
    "EXPORT DATA\n",
    "OPTIONS (uri='{BUCKET_NAME}/bqml/item_embeddings/*.csv',\n",
    "  format='CSV',\n",
    "  overwrite=true) AS\n",
    "SELECT * FROM `css_retail.item_embeddings`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dd901bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Notebook 2 task - export the trained embeddings to cloud storage\n",
    "\n",
    "# runner = 'DataflowRunner'\n",
    "# timestamp = datetime.utcnow().strftime('%y%m%d%H%M%S')\n",
    "# job_name = f'ks-bqml-export-embeddings-{timestamp}'\n",
    "# bq_dataset_name = BQ_DATASET_NAME\n",
    "# embeddings_table_name = 'item_embeddings'\n",
    "# output_dir = f'gs://{BUCKET}/bqml/item_embeddings'\n",
    "# project = PROJECT_ID\n",
    "# temp_location = os.path.join(output_dir, 'tmp')\n",
    "# region = REGION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdf67898",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"bqml-scann-demo\",\n",
    "             description=\"a bqml matching engine demo\",\n",
    "             pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(project: str=PROJECT_ID, compute_PMI_query: str=compute_PMI_query,\n",
    "            train_item_matching_query: str=train_item_matching_query,\n",
    "            extract_embeddings_query: str=extract_embeddings_query):\n",
    "    \n",
    "    \n",
    "    ###### NOTEBOOK 1 TASKS\n",
    "    compute_bq_pmi_task = run_a_bq_call(project, compute_PMI_query)\n",
    "    \n",
    "    train_bq_item_match_task = run_a_bq_call(project, train_item_matching_query).after(compute_bq_pmi_task)\n",
    "    ###### NOTEBOOK 2 TASKS\n",
    "    create_bq_embeddings_task = run_a_bq_call(project, extract_embeddings_query).after(train_bq_item_match_task)\n",
    "    \n",
    "    ##### DEFINE THE ORDER\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d435dbc7",
   "metadata": {},
   "source": [
    "### Now that the reusable components are set in a pipeline, declare the paramaters, queries, logic, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cab2926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  # noqa: F811\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"intro_pipeline.json\".replace(\" \", \"_\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a1670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210922214719\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210922214719')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/bqml-scann-demo-20210922214719?project=733956866731\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210922214719 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210922214719 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210922214719 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210922214719 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210922214719 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/733956866731/locations/us-central1/pipelineJobs/bqml-scann-demo-20210922214719 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "DISPLAY_NAME = \"intro_\" + TIMESTAMP\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"intro_pipeline.json\".replace(\" \", \"_\"),\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "993c25bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bigquery_ddl(project_id: str, query_string: str, \n",
    "    location: str):\n",
    "    \"\"\"\n",
    "    Runs BigQuery query and returns a table/model name\n",
    "    \"\"\"\n",
    "    print(query_string)\n",
    "        \n",
    "    from google.cloud import bigquery\n",
    "    from google.api_core.future import polling\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud.bigquery import retry as bq_retry\n",
    "    \n",
    "    bqclient = bigquery.Client(project=project_id, location=location)\n",
    "    job = bqclient.query(query_string, retry=bq_retry.DEFAULT_RETRY)\n",
    "    job._retry = polling.DEFAULT_RETRY\n",
    "    \n",
    "    while job.running():\n",
    "        from time import sleep\n",
    "        sleep(0.1)\n",
    "        print('Running ...')\n",
    "    return print(\"Job complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a42825bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXPORT DATA\n",
      "OPTIONS (uri='gs://rec_bq_jsw/bqml/item_embeddings/*.csv',\n",
      "  format='CSV',\n",
      "  overwrite=true) AS\n",
      "SELECT * FROM `css_retail.item_embeddings`\n",
      "\n",
      "Running ...\n",
      "Job complete!\n"
     ]
    }
   ],
   "source": [
    "j = run_bigquery_ddl(query_string=export_embeddings_query,\n",
    "                                     project_id=PROJECT_ID,\n",
    "                                     location='US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e64b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not j.error_result:\n",
    "    print(\"gj!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10708f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.display_name"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
