{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkgce5cdOcW7"
   },
   "source": [
    "# Part 2: Process the item embedding data in BigQuery and export it to Cloud Storage\n",
    "\n",
    "This notebook is the second of five notebooks that guide you through running the [Real-time Item-to-item Recommendation with BigQuery ML Matrix Factorization and ScaNN](https://github.com/GoogleCloudPlatform/analytics-componentized-patterns/tree/master/retail/recommendation-system/bqml-scann) solution.\n",
    "\n",
    "Use this notebook to complete the following tasks:\n",
    "\n",
    "1. Process the song embeddings data in BigQuery to generate a single embedding vector for each song.\n",
    "1. Use a Dataflow pipeline to write the embedding vector data to CSV files and export the files to a Cloud Storage bucket. \n",
    "\n",
    "Before starting this notebook, you must run the [01_train_bqml_mf_pmi](01_train_bqml_mf_pmi.ipynb) notebook to calculate item PMI data and then train a matrix factorization model with it.\n",
    "\n",
    "After completing this notebook, run the [03_create_embedding_lookup_model](03_create_embedding_lookup_model.ipynb) notebook to create a model to serve the item embedding data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SW1RHsqGPNzE"
   },
   "source": [
    "## Setup\r\n",
    "\r\n",
    "Import the required libraries, configure the environment variables, and authenticate your GCP account.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/jupyter/.local/lib/python3.7/site-packages (2.5.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.0-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
      "\u001b[K     |████████████████████▊           | 296.9 MB 142.1 MB/s eta 0:00:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 458.3 MB 6.9 kB/s  eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.38.1)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow) (0.11.0)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 81.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras~=2.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.5.0)\n",
      "Installing collected packages: tensorflow-estimator, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.5.0\n",
      "    Uninstalling tensorflow-estimator-2.5.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
      "  Rolling back uninstall of tensorflow-estimator\n",
      "  Moving to /home/jupyter/.local/lib/python3.7/site-packages/tensorflow_estimator-2.5.0.dist-info/\n",
      "   from /home/jupyter/.local/lib/python3.7/site-packages/~ensorflow_estimator-2.5.0.dist-info\n",
      "  Moving to /home/jupyter/.local/lib/python3.7/site-packages/tensorflow_estimator/\n",
      "   from /home/jupyter/.local/lib/python3.7/site-packages/~ensorflow_estimator\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/opt/conda/lib/python3.7/site-packages/tensorflow_estimator/__init__.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mp6ETYF2R-0q"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdSKSzqvR_qY"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OcUKzLnuR_wa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 15:08:52.251995: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "/opt/conda/lib/python3.7/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28485/203105041.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_io\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_io\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_io/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"tensorflow_io\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVERSION\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_io/core/python/api/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# tensorflow_io.core.python.ops is implicitly imported (along with file system)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIODataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIOTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_io/core/python/ops/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0m_main_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tensorflow/core/kernels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_main_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m       \u001b[0m_ll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_main_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;31m# Load third party dynamic kernels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/load_library.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(library_location)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernel_libraries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m       \u001b[0mpy_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_LoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: /opt/conda/lib/python3.7/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow_io as tf_io\n",
    "import apache_beam as beam\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22rDpO3JPcy9"
   },
   "source": [
    "### Configure GCP environment settings\r\n",
    "\r\n",
    "Update the following variables to reflect the values for your GCP environment:\r\n",
    "\r\n",
    "+ `PROJECT_ID`: The ID of the Google Cloud project you are using to implement this solution.\r\n",
    "+ `BUCKET`: The name of the Cloud Storage bucket you created to use with this solution. The `BUCKET` value should be just the bucket name, so `myBucket` rather than `gs://myBucket`.\r\n",
    "+ `REGION`: The region to use for the Dataflow job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nyx4vEd7Oa9I"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'yourProject' # Change to your project.\n",
    "BUCKET = 'yourBucketName' # Change to the bucket you created.\n",
    "REGION = 'yourDataflowRegion' # Change to your Dataflow region.\n",
    "BQ_DATASET_NAME = 'recommendations'\n",
    "\n",
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d89ZwydPhQX"
   },
   "source": [
    "### Authenticate your GCP account\n",
    "This is required if you run the notebook in Colab. If you use an AI Platform notebook, you should already be authenticated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ICvdRicPhl8"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "  print(\"Colab user is authenticated.\")\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1gmEmHbSaQD"
   },
   "source": [
    "## Process the item embeddings data\r\n",
    "\r\n",
    "You run the [sp_ExractEmbeddings](sql_scripts/sp_ExractEmbeddings.sql) stored procedure to process the item embeddings data and write the results to the `item_embeddings` table.\r\n",
    "\r\n",
    "This stored procedure works as follows:\r\n",
    "\r\n",
    "1. Uses the [ML.WEIGHTS](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-weights) function to extract the item embedding matrices from the `item_matching_model` model.\r\n",
    "1. Aggregates these matrices to generate a single embedding vector for each item.\r\n",
    "\r\n",
    "    Because BigQuery ML matrix factorization models are designed for user-item recommendation use cases, they generate two embedding matrices, one for users, and the other of items. However, in this use case, both embedding matrices represent items, but in different axes of the feedback matrix. For more information about how the feedback matrix is calculated, see [Understanding item embeddings](https://cloud.google.com/solutions/real-time-item-matching#understanding_item_embeddings).\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utkyuwJUyTlb"
   },
   "source": [
    "### Run the `sp_ExractEmbeddings` stored procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DK0olptba8qi"
   },
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT_ID\n",
    "\n",
    "CALL recommendations.sp_ExractEmbeddings() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UvHD7BJ8Gk0"
   },
   "source": [
    "Get a count of the records in the `item_embeddings` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQsJenNFzVJ7"
   },
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT_ID\n",
    "\n",
    "SELECT COUNT(*) embedding_count\n",
    "FROM recommendations.item_embeddings;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sx8JNJbA8PxC"
   },
   "source": [
    "See a sample of the data in the `item_embeddings` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4kTGcaRzVJ7"
   },
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT_ID\n",
    "\n",
    "SELECT *\n",
    "FROM recommendations.item_embeddings\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3LKaxlNSkrv"
   },
   "source": [
    "## Export the item embedding vector data\n",
    "\n",
    "Export the item embedding data to Cloud Storage by using a Dataflow pipeline. This pipeline does the following:\n",
    "\n",
    "1. Reads the item embedding records from the `item_embeddings` table in BigQuery.\n",
    "1. Writes each item embedding record to a CSV file.\n",
    "1. Writes the item embedding CSV files to a Cloud Storage bucket.\n",
    "\n",
    "The pipeline in implemented in the [embeddings_exporter/pipeline.py](embeddings_exporter/pipeline.py) module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8HLFGGl5oac"
   },
   "source": [
    "### Configure the pipeline variables\r\n",
    "\r\n",
    "Configure the variables needed by the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZKaoBwnSk6U"
   },
   "outputs": [],
   "source": [
    "runner = 'DataflowRunner'\n",
    "timestamp = datetime.utcnow().strftime('%y%m%d%H%M%S')\n",
    "job_name = f'ks-bqml-export-embeddings-{timestamp}'\n",
    "bq_dataset_name = BQ_DATASET_NAME\n",
    "embeddings_table_name = 'item_embeddings'\n",
    "output_dir = f'gs://{BUCKET}/bqml/item_embeddings'\n",
    "project = PROJECT_ID\n",
    "temp_location = os.path.join(output_dir, 'tmp')\n",
    "region = REGION\n",
    "\n",
    "print(f'runner: {runner}')\n",
    "print(f'job_name: {job_name}')\n",
    "print(f'bq_dataset_name: {bq_dataset_name}')\n",
    "print(f'embeddings_table_name: {embeddings_table_name}')\n",
    "print(f'output_dir: {output_dir}')\n",
    "print(f'project: {project}')\n",
    "print(f'temp_location: {temp_location}')\n",
    "print(f'region: {region}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyiIh-ATzVJ8"
   },
   "outputs": [],
   "source": [
    "try: os.chdir(os.path.join(os.getcwd(), 'embeddings_exporter'))\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHxODaoFzVJ8"
   },
   "source": [
    "### Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBarPrE_-LJr"
   },
   "source": [
    "It takes about 5 minutes to run the pipeline. You can see the graph for the running pipeline in the [Dataflow Console](https://console.cloud.google.com/dataflow/jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WngoWnt2zVJ9"
   },
   "outputs": [],
   "source": [
    "if tf_io.gfile.exists(output_dir):\n",
    "  print(\"Removing {} contents...\".format(output_dir))\n",
    "  tf_io.gfile.rmtree(output_dir)\n",
    "\n",
    "print(\"Creating output: {}\".format(output_dir))\n",
    "tf_io.gfile.makedirs(output_dir)\n",
    "\n",
    "!python runner.py \\\n",
    "  --runner={runner} \\\n",
    "  --job_name={job_name} \\\n",
    "  --bq_dataset_name={bq_dataset_name} \\\n",
    "  --embeddings_table_name={embeddings_table_name} \\\n",
    "  --output_dir={output_dir} \\\n",
    "  --project={project} \\\n",
    "  --temp_location={temp_location} \\\n",
    "  --region={region}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLXGq4CA_Oz0"
   },
   "source": [
    "### List the CSV files that were written to Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee89jHK5zVJ9"
   },
   "outputs": [],
   "source": [
    "!gsutil ls {output_dir}/embeddings-*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fp1bOyVCgBnH"
   },
   "source": [
    "## License\n",
    "\n",
    "Copyright 2020 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n",
    "\n",
    "See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "**This is not an official Google product but sample code provided for an educational purpose**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02_export_bqml_mf_embeddings.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-6.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
