{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21e1a5a",
   "metadata": {},
   "source": [
    "# Two - Tower Retreival Model\n",
    "\n",
    "### Key resources:\n",
    "* Many pages [here] include great techniques to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d148f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gs://tfrs-tf-records/file_00-12227.tfrec', 'gs://tfrs-tf-records/file_01-12228.tfrec']\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client()\n",
    "files = []\n",
    "for blob in client.list_blobs('tfrs-tf-records'):\n",
    "    files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "    \n",
    "print(files[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43230467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "\n",
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "         #query features\n",
    "        \"query\": tf.io.FixedLenFeature([1], tf.string),\n",
    "        \"last_viewed\": tf.io.FixedLenFeature([1], tf.string),\n",
    "\n",
    "        #candidate features\n",
    "        \"IVM_s\": tf.io.FixedLenFeature([1], tf.string),\n",
    "        \"description\": tf.io.FixedLenFeature([1], tf.string),\n",
    "        \"price_td\": tf.io.FixedLenFeature([1], tf.float32),\n",
    "        \"PriceRange_s\": tf.io.FixedLenFeature([1],  tf.string),\n",
    "        \"productTypeCombo_ss\": tf.io.FixedLenFeature([1], tf.string),\n",
    "        \"visual\": tf.io.FixedLenFeature([2048,], tf.float32),\n",
    "        \"month\": tf.io.FixedLenFeature([1], tf.string),\n",
    "        \"hour\": tf.io.FixedLenFeature([1], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    return example\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 32\n",
    "MAX_TOKENS = 1_000_000\n",
    "\n",
    "\n",
    "TF_RECORDS_DIR = 'gs://tfrs-tf-records'\n",
    "\n",
    "# Set dev dataset CHANGE THIS LATER TO THE WHOLE DIR\n",
    "raw_dataset = tf.data.TFRecordDataset(files)\n",
    "\n",
    "    \n",
    "parsed_dataset = raw_dataset.map(parse_tfrecord_fn, num_parallel_calls=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f36366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        \n",
    "        month_vocab = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "        hour_vocab = month_vocab + [\"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"00\"]\n",
    "\n",
    "        self.month_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                vocabulary=month_vocab, mask_token=None, name=\"month_lookup\"),\n",
    "            tf.keras.layers.Embedding(\n",
    "                len(month_vocab)+1, EMBEDDING_DIM, mask_zero=True, name=\"month_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        \n",
    "        self.hour_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                vocabulary=hour_vocab, mask_token=None, name=\"hour_lookup\"),\n",
    "            tf.keras.layers.Embedding(len(hour_vocab)+1, EMBEDDING_DIM, mask_zero=True, name=\"hour_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "    \n",
    "        self.query_vectorizor = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            ngrams=2, name=\"query_tv\")\n",
    "        \n",
    "        self.last_viewed_vectorizor = tf.keras.layers.experimental.preprocessing.TextVectorization(ngrams=2, name=\"last_viewed_tv\")\n",
    "        \n",
    "        self.query_embedding = tf.keras.Sequential([\n",
    "            self.query_vectorizor,\n",
    "            tf.keras.layers.Embedding(MAX_TOKENS, EMBEDDING_DIM , mask_zero=True, name=\"query_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        \n",
    "        self.last_viewed_embedding = tf.keras.Sequential([\n",
    "            self.last_viewed_vectorizor,\n",
    "            tf.keras.layers.Embedding(MAX_TOKENS, EMBEDDING_DIM , mask_zero=True, name=\"last_v_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        \n",
    "        ### adapt stuff\n",
    "        self.query_vectorizor.adapt(parsed_dataset.map(lambda x: x['query']))\n",
    "        self.last_viewed_vectorizor.adapt(parsed_dataset.map(lambda x: x['last_viewed'])) \n",
    "        \n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential()\n",
    "\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, kernel_regularizer='l2')) #l2 reg was in research - helps performance\n",
    "\n",
    "\n",
    "    def call(self, data):    \n",
    "        all_embs = tf.concat(\n",
    "                [\n",
    "                    self.month_embedding(data['month']),\n",
    "                    self.hour_embedding(data['hour']),\n",
    "                    self.query_embedding(data['query']),\n",
    "                    self.last_viewed_embedding(data['last_viewed'])\n",
    "                ], axis=1)\n",
    "        return self.dense_layers(all_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea2030b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_qm = QueryModel([256, 128, 64, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddce90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #validate batch output\n",
    "\n",
    "# # validate output\n",
    "# #should roll out a EMB_DIM * 4 (for the four features in the query)\n",
    "# batches = last_viewed_adapt_data.batch(128)\n",
    "# qm_emb = batches.map(lambda x: test_qm(x))\n",
    "\n",
    "# for line in qm_emb.take(1):\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a79702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        \n",
    "        #continuous example - allowing batch norms via layer below for standardization of inputs\n",
    "        self.price_normalizer = tf.keras.layers.experimental.preprocessing.Normalization(name=\"price_norm\")\n",
    "\n",
    "        #categorical with vocabs\n",
    "        pr_vocab = ['002_$100 - $299', '000_$0 - $49', '001_$50 - $99', \n",
    "                    '003_$300 - $599', '005_$1000 - $3999', '004_$600 - $999', \n",
    "                    '006_$4000+', '']\n",
    "\n",
    "        self.price_range_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                  vocabulary=pr_vocab, mask_token=None, name=\"price_range_lu\"),\n",
    "            tf.keras.layers.Embedding(\n",
    "                len(pr_vocab)+1, EMBEDDING_DIM, mask_zero=True, name=\"price_range_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(name=\"price_range_flatten\")\n",
    "        ])\n",
    "        \n",
    "        # categorical: description - below are all embeddings with unk vocabs - will be adapted \n",
    "        self.description_vectorizor = tf.keras.layers.experimental.preprocessing.TextVectorization(ngrams=2, name = \"description_vectorizor\")\n",
    "\n",
    "        self.description_embedding = tf.keras.Sequential([\n",
    "            self.description_vectorizor,\n",
    "            tf.keras.layers.Embedding(MAX_TOKENS, EMBEDDING_DIM, mask_zero=True, name = \"desc_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(name=\"desc_flatten\")\n",
    "        ])\n",
    "        \n",
    "        #categorical: sku\n",
    "        self.sku_vectorizor = tf.keras.layers.experimental.preprocessing.TextVectorization(name = \"sku_vectorizor\")\n",
    "\n",
    "        self.sku_embedding = tf.keras.Sequential([\n",
    "            self.sku_vectorizor,\n",
    "            tf.keras.layers.Embedding(MAX_TOKENS, EMBEDDING_DIM, mask_zero=True, name = \"sku_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(name=\"sku_flat\")\n",
    "        ])\n",
    "        \n",
    "        ## product ragged stuff - \n",
    "\n",
    "        ## prod heirarcy\n",
    "        def split_fn(string):\n",
    "            return tf.strings.split(string, sep=\"|\")\n",
    "\n",
    "        self.prod_heir_vectorizor = tf.keras.layers.experimental.preprocessing.TextVectorization(ngrams=2, split=split_fn, name = \"heir_vectorizor\")\n",
    "\n",
    "        self.heir_embedding = tf.keras.Sequential([\n",
    "            self.prod_heir_vectorizor,\n",
    "            tf.keras.layers.Embedding(MAX_TOKENS, EMBEDDING_DIM, mask_zero=True, name = \"heir_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(name=\"heir_flat\")\n",
    "        ])\n",
    "        \n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential()\n",
    "\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "            # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, kernel_regularizer='l2')) #l2 reg was in research - helps performance\n",
    "        \n",
    "        #adapt stuff\n",
    "        self.description_vectorizor.adapt(parsed_dataset.map(lambda x: x['description']))\n",
    "        self.sku_vectorizor.adapt(parsed_dataset.map(lambda x: x['IVM_s']))\n",
    "        self.prod_heir_vectorizor.adapt(parsed_dataset.map(lambda x: x['productTypeCombo_ss']))\n",
    "        \n",
    "        #continous adapts - look ahead batching - grab up to batch_size\n",
    "#         batch_size = 1024 #set to whatever you want to look ahead to get a good sample\n",
    "        self.price_normalizer.adapt(parsed_dataset.map(lambda x: x['price_td']))        \n",
    "\n",
    "\n",
    "    def call(self, data):\n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.price_normalizer(data['price_td']),\n",
    "                self.price_range_embedding(data['PriceRange_s']),\n",
    "                self.description_embedding(data['description']),\n",
    "                self.sku_embedding(data['IVM_s']),\n",
    "                self.heir_embedding(data['productTypeCombo_ss']),\n",
    "                data['visual']\n",
    "            ], axis=1)\n",
    "#         return self.dense_layers(all_embs)  #last plus for number continuous + 1 if you add other(s)\n",
    "        return self.dense_layers(all_embs)  #last plus for number continuous + 1 if you add other(s) 2048 for visual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "353129ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pm = ProductModel([256, 128, 64, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "378bd5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # validate output\n",
    "\n",
    "# batches = catalog_adapt_data.batch(128)\n",
    "# pm_emb = batches.map(lambda x: test_pm(x))\n",
    "\n",
    "# for line in pm_emb.take(1):\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b3a976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now combine them and assign retreival tasks, etc..\n",
    "\n",
    "class TheTwoTowers(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.query_model = QueryModel(layer_sizes)\n",
    "        self.candidate_model = ProductModel(layer_sizes)\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=catalog_adapt_data.batch(128).map(self.candidate_model),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, data, training=False):\n",
    "        query_embeddings = self.query_model(data)\n",
    "        product_embeddings = self.candidate_model(data)\n",
    "\n",
    "        return self.task(\n",
    "            query_embeddings, product_embeddings) #TOGGLE,  compute_metrics=not training)#### turn off metrics to save time on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b844dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tt = TheTwoTowers([256, 128, 64, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f47adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "num_records = 4293302 #sum(1 for _ in file_io.FileIO(SMALL_DATASET, 'rb')) #CHANGE THIS TO LARGE DATASET WHEN READY\n",
    "\n",
    "shuffled = parsed_dataset.shuffle(num_records, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train_records = int(round(num_records * 0.95,0))\n",
    "test_records = num_records - train_records\n",
    "\n",
    "# train_records\n",
    "train = shuffled.take(train_records)\n",
    "test = shuffled.skip(train_records).take(test_records)\n",
    "\n",
    "cached_train = train.shuffle(train_records).batch(2048)\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TheTwoTowers([256, 128, 64, 32])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ee276",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca886c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=5,\n",
    "    epochs=num_epochs,\n",
    "    verbose=0)\n",
    "\n",
    "accuracy = layer_history.history[\"factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Top-100 accuracy: {accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde9312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplt as plt\n",
    "\n",
    "plt.plot(epochs, layer_history.history[\"factorized_top_k/top_100_categorical_accuracy\"], label=\"{} - Deep Architecture\".format([256, 128, 64, 32]))\n",
    "plt.title(\"Accuracy vs epoch\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Top-100 accuracy\");\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3286c5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
