{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96f8d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92c22d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'two-tower'\n",
    "#PREFIX = 'css_retail'\n",
    "DISPLAY_NAME = f'{PREFIX}-tensorboard'\n",
    "PROJECT= 'babrams-recai-demo-final'\n",
    "REGION='us-central1'\n",
    "\n",
    "STAGING_BUCKET = \"\"\"gs://{}_vertex_training\"\"\".format(PROJECT) #lowes-reccomendation-tensorboard-logs-us-central1 - this \n",
    "#TENSORBOARD = 'projects/258043323883/locations/us-central1/tensorboards/4236655796332527616' #note really can only get this after gcloud beta ai tensorboards create...\n",
    "#VERTEX_SA = 'vertex-tb@lowes-reccomendation.iam.gserviceaccount.com'\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float(\"LR\", -1.01, \"Learning Rate\")\n",
    "flags.DEFINE_integer(\"EMBEDDING_DIM\", 15, \"Embedding dimension\")\n",
    "flags.DEFINE_integer(\"MAX_TOKENS\", 15, \"Max embeddings for query and last_n products\")\n",
    "flags.DEFINE_integer(\"NUM_EPOCHS\", 29, \"Number of epochs\")\n",
    "flags.DEFINE_string(\"MODEL_DIR\", 'model-dirs-lowes', \"GCS Bucket to store the model artifact\")\n",
    "flags.DEFINE_bool(\"DROPOUT\", False, \"Use Dropout - T/F bool type\")\n",
    "flags.DEFINE_float(\"DROPOUT_RATE\", -1.4, \"Dropout rate only works with DROPOUT=True\")\n",
    "#flags.DEFINE_integer(\"N_PRODUCTS\", 19999, \"number of products considered for embedding\")\n",
    "flags.DEFINE_integer(\"BATCH_SIZE\", 1023, \"batch size\")\n",
    "flags.DEFINE_string(\"ARCH\", '[127,64]', \"deep architecture, expressed as a list of ints in string format - will be parsed into list\")\n",
    "flags.DEFINE_integer(\"SEED\", 41781896, \"random seed\")\n",
    "#flags.DEFINE_string(\"TF_RECORDS_DIR\", \"gs://tfrs-central-a\", \"source data in tfrecord format gcs location\")\n",
    "\n",
    "# initialize vertex sdk\n",
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")\n",
    "\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d6486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r notebook_requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2eefcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_catalog_sql = \"\"\"\n",
    "WITH inner_q AS (\n",
    "    SELECT\n",
    "        SAFE_CAST(id AS INT) AS productId,\n",
    "        title,\n",
    "        description,\n",
    "        product_metadata.exact_price.original_price AS price,\n",
    "        ARRAY_TO_STRING(cats.categories, ' ') AS categories\n",
    "    FROM `babrams-recai-demo-final.css_retail.recommendation_ai_data` AS rad,\n",
    "    UNNEST(category_hierarchies) AS cats\n",
    ") SELECT\n",
    "    productId,\n",
    "    title,\n",
    "    description,\n",
    "    price,\n",
    "    ARRAY_TO_STRING(ARRAY_AGG(categories), \",\") AS categories\n",
    "FROM inner_q\n",
    "GROUP BY productId, title, description, price\n",
    "\"\"\"\n",
    "\n",
    "product_catalog = client.query(product_catalog_sql)\n",
    "product_catalog_df = product_catalog.to_dataframe()\n",
    "#product_catalog_df.describe()\n",
    "#product_categoricals = ['productId', 'title', 'description', 'categories']\n",
    "# product_catalog_df.dtypes\n",
    "\n",
    "class ProductModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, adapt_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        #preprocess stuff\n",
    "        self.sku_count = np.unique(\n",
    "            np.concatenate(\n",
    "                list(\n",
    "                    adapt_data.map(lambda x: x[\"productId\"]).batch(1000)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        #categorical: sku\n",
    "        self.sku_lookup = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "            name=\"sku_monotic\"\n",
    "        )\n",
    "        self.title_vectorizor = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=self.sku_count\n",
    "            , name=\"title_vectorizor\"\n",
    "        )\n",
    "        self.description_vectorizor = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=self.sku_count\n",
    "            , name=\"description_vectorizor\"\n",
    "        )\n",
    "        self.category_vectorizor = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=FLAGS.N_PRODUCTS\n",
    "            , name=\"category_vectorizor\"\n",
    "        )\n",
    "        self.price_normalization = tf.keras.layers.experimental.preprocessing.Normalization(axis=None)\n",
    "\n",
    "        \n",
    "        #adapt stuff\n",
    "        self.category_vectorizor.adapt(adapt_data.map(lambda x: x['categories']))\n",
    "        self.title_vectorizor.adapt(adapt_data.map(lambda x: x['title']))\n",
    "        self.description_vectorizor.adapt(adapt_data.map(lambda x: x['description']))\n",
    "        self.sku_lookup.adapt(adapt_data.map(lambda x: x['productId']))\n",
    "        self.price_normalization.adapt(adapt_data.map(lambda x: x['price']))\n",
    "        \n",
    "        #embed stuff\n",
    "        self.sku_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.sku_lookup,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    self.sku_count+1,\n",
    "                    FLAGS.EMBEDDING_DIM,\n",
    "                    mask_zero=True,\n",
    "                    name=\"sku_emb\"\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"sku_flat\"\n",
    "                )\n",
    "            ],\n",
    "            name=\"sku_embedding\"\n",
    "        )\n",
    "        self.title_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.title_vectorizor,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    self.sku_count+1, \n",
    "                    FLAGS.EMBEDDING_DIM, \n",
    "                    mask_zero=True, \n",
    "                    name=\"title_emb\"\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"title_flatten\"\n",
    "                )\n",
    "            ], \n",
    "            name=\"title_embedding\"\n",
    "        )\n",
    "        self.description_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.description_vectorizor,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    self.sku_count+1, \n",
    "                    FLAGS.EMBEDDING_DIM, \n",
    "                    mask_zero=True, \n",
    "                    name=\"desc_emb\"),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"desc_flatten\"\n",
    "                )\n",
    "            ], \n",
    "            name=\"description_embedding\"\n",
    "        )\n",
    "        self.category_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.category_vectorizor,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    self.category_vectorizer.vocab_size(), \n",
    "                    FLAGS.EMBEDDING_DIM, \n",
    "                    mask_zero=True, \n",
    "                    name=\"category_emb\"),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"category_flatten\"\n",
    "                )\n",
    "            ], \n",
    "            name=\"category_embedding\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"dense_layers_product\")\n",
    "        \n",
    "        # Adding weight initialzier\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=FLAGS.SEED)\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                activation=\"relu\",\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "            if FLAGS.DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(\n",
    "                    FLAGS.DROPOUT_RATE\n",
    "                ))\n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(tf.keras.layers.Lambda(\n",
    "            lambda x: tf.nn.l2_normalize(\n",
    "                x,\n",
    "                1,\n",
    "                epsilon=1e-12,\n",
    "                name=\"normalize_dense\"\n",
    "            )\n",
    "        ))\n",
    "\n",
    "    def call(self, data):\n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                tf.reshape(self.price_normalization(data[\"price\"]), (-1, 1)),\n",
    "                self.description_embedding(data['description']),\n",
    "                self.sku_embedding(data['productId']),\n",
    "                self.category_embedding(data['categories']),\n",
    "                self.title_embedding(data['title'])\n",
    "            ], axis=1)\n",
    "        return self.dense_layers(all_embs)  #last plus for number continuous + 1 if you add other(s) 2048 for visual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d36c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "customer_data_sql = \"\"\"\n",
    "SELECT\n",
    "    id AS userId,\n",
    "    age,\n",
    "    gender,\n",
    "    latitude,\n",
    "    longitude,\n",
    "    zip,\n",
    "    traffic_source,\n",
    "    TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), created_at, DAY) AS customer_lifetime_days\n",
    "FROM `babrams-recai-demo-final.css_retail.customers` AS customers\n",
    "\"\"\"\n",
    "customer_data = client.query(customer_data_sql)\n",
    "customer_data_df = customer_data.to_dataframe()\n",
    "#customer_data_df.describe()\n",
    "\n",
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, adapt_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        #preprocess stuff\n",
    "        self.user_lookup = tf.keras.layers.experimental.preprocessing.StringLookup()\n",
    "        self.max_age = adapt_data.map(\n",
    "            lambda x: x['age']\n",
    "        ).reduce(\n",
    "            tf.cast(0, tf.int64),\n",
    "            tf.maximum\n",
    "        ).numpy().max()\n",
    "        self.min_age = adapt_data.map(\n",
    "            lambda x: x['age']\n",
    "        ).reduce(\n",
    "            np.int64(1e9),\n",
    "            tf.minimum\n",
    "        ).numpy().min()\n",
    "        self.age_buckets = np.linspace(self.min_age, self.max_age, num=20)\n",
    "        self.max_lifetime = adapt_data.map(\n",
    "            lambda x: x['customer_lifetime_days']\n",
    "        ).reduce(\n",
    "            tf.cast(0, tf.int64),\n",
    "            tf.maximum\n",
    "        ).numpy().max()\n",
    "        self.min_lifetime = adapt_data.map(\n",
    "            lambda x: x['customer_lifetime_days']\n",
    "        ).reduce(\n",
    "            np.int64(1e9),\n",
    "            tf.minimum\n",
    "        ).numpy().min()\n",
    "        self.lifetime_buckets = np.linspace(self.min_lifetime, self.max_lifetime, num=100)\n",
    "        self.traffic_source_lookup = tf.keras.layers.experimental.preprocessing.StringLookup()\n",
    "        self.zip_lookup = tf.keras.layers.experimental.preprocessing.StringLookup()\n",
    "        self.gender_lookup = tf.keras.layers.experimental.preprocessing.StringLookup()\n",
    "        \n",
    "        #adapt stuff\n",
    "        self.user_lookup.adapt(adapt_data.map(lambda x: x['productId']))\n",
    "        self.zip_lookup.adapt(adapt_data.map(lambda x: x['zip']))\n",
    "        self.gender_lookup.adapt(adapt_data.map(lambda x: x['gender']))\n",
    "         \n",
    "        \n",
    "        #embed stuff\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            self.user_lookup,\n",
    "            tf.keras.layers.Embedding(\n",
    "                self.user_lookup.vocab_size() + 1,\n",
    "                FLAGS.EMBEDDING_DIM,\n",
    "                mask_zero=True,\n",
    "                name=\"user_emb\"\n",
    "            ),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(\n",
    "                name=\"user_flat\"\n",
    "            )\n",
    "        ], name=\"user_embedding\")\n",
    "        self.age_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.Discretization(\n",
    "                self.age_buckets.tolist()\n",
    "            ),\n",
    "            tf.keras.layers.Embedding(\n",
    "                len(self.age_buckets) + 1,\n",
    "                32\n",
    "            )\n",
    "        ], name=\"age_embedding\")\n",
    "        self.lifetime_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.Discretization(\n",
    "                self.lifetime_buckets.tolist(),\n",
    "                name=\"lifetime_disc\"\n",
    "            ),\n",
    "            tf.keras.layers.Embedding(\n",
    "                len(self.lifetime_buckets) + 1,\n",
    "                32\n",
    "            )\n",
    "        ], name=\"customer_lifetime_embedding\")\n",
    "        self.traffic_source_embedding = tf.keras.Sequential([\n",
    "            self.traffic_source_lookup,\n",
    "            tf.keras.layers.Embedding(\n",
    "                self.traffic_source_lookup.vocab_size() + 1,\n",
    "                FLAGS.EMBEDDING_DIM,\n",
    "                mask_zero=True,\n",
    "                name=\"traffic_source_emb\"\n",
    "            ),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(\n",
    "                name=\"traffic_source_flat\"\n",
    "            )\n",
    "        ], name=\"traffic_source_embedding\")\n",
    "        self.zip_embedding = tf.keras.Sequential([\n",
    "            self.zip_lookup,\n",
    "            tf.keras.layers.Embedding(\n",
    "                self.zip_lookup.vocab_size() + 1,\n",
    "                FLAGS.EMBEDDING_DIM,\n",
    "                mask_zero=True,\n",
    "                name=\"zip_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(name=\"zip_flat\")\n",
    "        ], name=\"zip_embedding\")\n",
    "        self.gender_embedding = tf.keras.Sequential([\n",
    "            self.genderlookup,\n",
    "            tf.keras.layers.Embedding(\n",
    "                self.gender_lookup.vocab_size() + 1,\n",
    "                FLAGS.EMBEDDING_DIM,\n",
    "                mask_zero=True,\n",
    "                name=\"gender_emb\"\n",
    "            ),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(\n",
    "                name=\"gender_flat\"\n",
    "            )\n",
    "        ], name=\"gender_embedding\")\n",
    "        \n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"dense_layers_product\")\n",
    "        \n",
    "        # Adding weight initialzier\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=FLAGS.SEED)\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                activation=\"relu\",\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "            if FLAGS.DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(\n",
    "                    FLAGS.DROPOUT_RATE\n",
    "                ))\n",
    "            # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(tf.keras.layers.Lambda(\n",
    "            lambda x: tf.nn.l2_normalize(\n",
    "                x,\n",
    "                1,\n",
    "                epsilon=1e-12,\n",
    "                name=\"normalize_dense\"\n",
    "            )\n",
    "        ))\n",
    "        \n",
    "\n",
    "    def call(self, data):\n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.user_embedding(data['userId']),\n",
    "                self.age_embedding(data['age']),\n",
    "                self.lifetime_embedding(data['customer_lifetime_days']),\n",
    "                self.traffic_source_embedding(data['traffic_source']),\n",
    "                self.zip_embedding(data['zip']),\n",
    "                self.gender_embedding(data['gender'])\n",
    "            ], axis=1)\n",
    "        return self.dense_layers(all_embs)  #last plus for number continuous + 1 if you add other(s) 2048 for visual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9632f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_data_sql = \"\"\"\n",
    "WITH inner_q AS (\n",
    "    SELECT\n",
    "        SAFE_CAST(userInfo.userId AS INT) AS userId,\n",
    "        SAFE_CAST(eventTime AS TIMESTAMP) AS eventTime,\n",
    "        productEventDetail.cartId,\n",
    "        productEventDetail.purchaseTransaction.revenue,\n",
    "        products.id AS productId,\n",
    "        products.quantity,\n",
    "        products.displayPrice AS price\n",
    "    FROM `babrams-recai-demo-final.css_retail.purchase_complete` AS purchase,\n",
    "    UNNEST(productEventDetail.productDetails) AS products\n",
    ") SELECT\n",
    "    inner_q.* EXCEPT (eventTime),\n",
    "    UNIX_MILLIS(eventTime) AS eventTime,\n",
    "    EXTRACT(HOUR FROM eventTime) AS hour,\n",
    "    EXTRACT(DAY FROM eventTime) AS day,\n",
    "    EXTRACT(MONTH FROM eventTime) AS month,\n",
    "    EXTRACT(DAYOFWEEK FROM eventTime) AS dow\n",
    "FROM inner_q;\n",
    "\"\"\"\n",
    "\n",
    "purchase_data = client.query(purchase_data_sql)\n",
    "purchase_data_df = purchase_data.to_dataframe()\n",
    "#purchase_categoricals = ['userId', 'cartId', 'productId']\n",
    "# purchase_data_df.dtypes\n",
    "\n",
    "class EventModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, adapt_data):\n",
    "        super().__init__()\n",
    "\n",
    "        ### preprocess stuff\n",
    "        self.month_vocab = tf.constant(\n",
    "            [\"%02d\" % i for i in range(1,13)],\n",
    "            name=\"month_vocab\"\n",
    "        )\n",
    "        self.day_vocab = tf.constant(\n",
    "            [\"%02d\" % i for i in range(1,32)],\n",
    "            name='day_vocab'\n",
    "        )\n",
    "        self.dow_vocab = tf.constant(\n",
    "            [\"%02d\" % i for i in range(1,8)],\n",
    "            name=\"dow_vocab\"\n",
    "        )\n",
    "        self.hour_vocab = tf.constant(\n",
    "            [\"%02d\" % i for i in range(0,24)],\n",
    "            name=\"hour_vocab\"\n",
    "        )\n",
    "        self.eventtime_normalization = tf.keras.layers.experimental.preprocessing.Normalization(axis=None)\n",
    "        self.price_normalization = tf.keras.layers.experimental.preprocessing.Normalization(axis=None)\n",
    "\n",
    "\n",
    "        ### adapt stuff\n",
    "        self.eventtime_normalization.adapt(adapt_data.map(lambda x: x['eventTime']).batch(1024))\n",
    "        self.price_normalization.adapt(adapt_data.map(lambda x: x['price']))\n",
    "\n",
    "        ### embed stuff\n",
    "        self.month_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.month_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"month_lookup\",\n",
    "                output_mode='count')\n",
    "        ], name=\"month_emb\")\n",
    "        self.hour_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.hour_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"hour_lookup\",\n",
    "                output_mode='count')\n",
    "        ], name=\"hour_emb\")\n",
    "        self.day_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.day_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"day_lookup\",\n",
    "                output_mode='count')\n",
    "        ], name=\"day_emb\")\n",
    "        self.dow_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.dow_vocab,\n",
    "                mask_token=None,\n",
    "                name=\"dow_lookup\",\n",
    "                output_mode='count')\n",
    "        ], name=\"dow_emb\")\n",
    "\n",
    "\n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"dense_layers_query\")\n",
    "\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=FLAGS.SEED)\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                activation=\"relu\",\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "            if FLAGS.DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(\n",
    "                    FLAGS.DROPOUT_RATE\n",
    "                ))\n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(\n",
    "                layer_size,\n",
    "                kernel_initializer=initializer\n",
    "            ))\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(tf.keras.layers.Lambda(\n",
    "            lambda x: tf.nn.l2_normalize(\n",
    "                x,\n",
    "                1,\n",
    "                epsilon=1e-12,\n",
    "                name=\"normalize_dense\"\n",
    "            )\n",
    "        ))\n",
    "\n",
    "\n",
    "    def call(self, data):\n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.month_embedding(data['month']),\n",
    "                self.dow_embedding(data['dow']),\n",
    "                self.day_embedding(data['day']),\n",
    "                self.hour_embedding(data['hour']),\n",
    "                tf.reshape(self.eventtime_normalization(data['eventTime']), (-1,1)),\n",
    "                tf.reshape(self.price_normalization(data['price']), (-1,1)),\n",
    "                #self.query_embedding(data['query']),\n",
    "                #self.last_viewed_embedding(data['last_viewed'])\n",
    "            ], axis=1)\n",
    "        return self.dense_layers(all_embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a27e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheTwoTowers(tfrs.models.Model):\n",
    "    def __init__(self, layer_sizes, query_adapt_data, cat_adapt_data):\n",
    "        super().__init__()\n",
    "        self.query_model = UserModel(layer_sizes, query_adapt_data)\n",
    "        self.candidate_model = ProductModel(layer_sizes, cat_adapt_data)\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=cat_adapt_data.batch(128).cache().map(self.candidate_model),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, data, training=False):\n",
    "        query_embeddings = self.query_model(data)\n",
    "        product_embeddings = self.candidate_model(data)\n",
    "\n",
    "        return self.task(\n",
    "            query_embeddings,\n",
    "            product_embeddings,\n",
    "            compute_metrics=not training\n",
    "        )#### turn off metrics to save time on training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _is_chief(task_type, task_id):\n",
    "    '''Helper function. Determines if machine is chief.'''\n",
    "\n",
    "    return task_type == 'chief'\n",
    "\n",
    "\n",
    "def _get_temp_dir(dirpath, task_id):\n",
    "    '''Helper function. Gets temporary directory for saving model.'''\n",
    "\n",
    "    base_dirpath = 'workertemp_' + str(task_id)\n",
    "    temp_dir = os.path.join(dirpath, base_dirpath)\n",
    "    tf.io.gfile.makedirs(temp_dir)\n",
    "    return temp_dir\n",
    "\n",
    "\n",
    "def write_filepath(filepath, task_type, task_id):\n",
    "    '''Helper function. Gets filepath to save model.'''\n",
    "\n",
    "    dirpath = os.path.dirname(filepath)\n",
    "    base = os.path.basename(filepath)\n",
    "    if not _is_chief(task_type, task_id):\n",
    "        dirpath = _get_temp_dir(dirpath, task_id)\n",
    "    return os.path.join(dirpath, base)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def runner(argv):\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "def main(argv):\n",
    "\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "\n",
    "    ### SET STRATEGY\n",
    "    #     gpus = tf.config.list_logical_devices('GPU')\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "    ## get product, query data files\n",
    "    client = storage.Client()\n",
    "    files = []\n",
    "    for blob in client.list_blobs('tfrs-tf-records'):\n",
    "        files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "\n",
    "    files_cat = []\n",
    "    for blob in client.list_blobs('prod-catalog-central'):\n",
    "        files_cat.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "    files = []\n",
    "    for blob in client.list_blobs('tfrs-tf-records'):\n",
    "        files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "\n",
    "    files_cat = []\n",
    "    for blob in client.list_blobs('prod-catalog-central'):\n",
    "        files_cat.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "    ## establish the pipelines\n",
    "    # Set dev dataset CHANGE THIS LATER TO THE WHOLE DIR\n",
    "    raw_dataset = tf.data.TFRecordDataset(files) #local machine training wheels - using smaller data set for starters\n",
    "    cat_dataset = tf.data.TFRecordDataset(files_cat)\n",
    "\n",
    "\n",
    "    #See `pipeline-opts.ipynb` for more info on tuning options\n",
    "    parsed_dataset = raw_dataset.map(\n",
    "        parse_tfrecord_fn,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    # Set AutoShardPolicy\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    parsed_dataset = parsed_dataset.with_options(options)\n",
    "\n",
    "\n",
    "    # Doing another pipeline for the adapts to get startup to run much faster\n",
    "\n",
    "    parsed_dataset_adapt = raw_dataset.map(\n",
    "        parse_tfrecord_fn,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    parsed_dataset_adapt = parsed_dataset_adapt.batch(FLAGS.BATCH_SIZE)\n",
    "\n",
    "    # parsed_dataset_adapt = parsed_dataset_adapt.batch(BATCH_SIZE)\n",
    "    # loading de-duplicated product catalog\n",
    "\n",
    "    parsed_dataset_candidates = cat_dataset.map(\n",
    "        parse_tfrecord_catalog,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    parsed_dataset_candidates = parsed_dataset_candidates\n",
    "\n",
    "    logging.info('Setting model adapts and compiling the model')\n",
    "    # Wrap variable creation within strategy scope\n",
    "    with strategy.scope():\n",
    "        model = TheTwoTowers( get_arch_from_string(FLAGS.ARCH), query_adapt_data=parsed_dataset_adapt, cat_adapt_data=parsed_dataset_candidates)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adagrad(FLAGS.LR))\n",
    "    logging.info('Adapts finish - training next')\n",
    "    tf.random.set_seed(FLAGS.SEED)\n",
    "\n",
    "    #tensorboard_cb = tf.keras.callbacks.TensorBoard(JOB_DIR, histogram_freq=0)\n",
    "\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'], #this sends to the log dir for vertex tensorboard\n",
    "        histogram_freq=0)\n",
    "\n",
    "    shuffled = parsed_dataset.shuffle(200_000, seed=FLAGS.SEED, reshuffle_each_iteration=False)\n",
    "    # shuffled = shuffled.cache()\n",
    "    parsed_dataset_candidates = parsed_dataset_candidates.shuffle(200_000, seed=FLAGS.SEED, reshuffle_each_iteration=False).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # set up the global batch size\n",
    "    global_batch_size = FLAGS.BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "    # train_records\n",
    "    train = shuffled.take(3_000_000)\n",
    "    test = shuffled.skip(3_000_000).take(200_000)\n",
    "\n",
    "    cached_train = train.batch(global_batch_size)\n",
    "    cached_test = test.batch(global_batch_size * 2).cache()\n",
    "    logging.info('Training starting')\n",
    "    layer_history = model.fit(\n",
    "        cached_train,\n",
    "        validation_data=cached_test,\n",
    "        validation_freq=5,\n",
    "        callbacks=[tensorboard_cb],\n",
    "        epochs=FLAGS.NUM_EPOCHS,\n",
    "        verbose=0)\n",
    "\n",
    "    # Determine type and task of the machine from the strategy cluster resolver\n",
    "    task_type, task_id = (strategy.cluster_resolver.task_type,\n",
    "                          strategy.cluster_resolver.task_id)\n",
    "    write_model_path = write_filepath(FLAGS.MODEL_DIR, task_type, task_id)# NEW\n",
    "    logging.info('Getting evaluation metrics')\n",
    "    index = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
    "    val_metrics = model.evaluate(cached_test, return_dict=True) #check performance\n",
    "    logging.info('Validation metrics below:')\n",
    "    logging.info(print(val_metrics))\n",
    "    # Create the index for the scann lookup - this is the query model lookup to the closest n products\n",
    "    logging.info('Creating trained index')\n",
    "    index = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
    "    logging.info(f'Saving model to {FLAGS.MODEL_DIR}')\n",
    "    tf.saved_model.save(\n",
    "        index,\n",
    "        write_model_path\n",
    "    )\n",
    "    print('model saved!')\n",
    "    logging.info('All done - model saved')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(main)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}