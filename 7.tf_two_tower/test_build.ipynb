{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96f8d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92c22d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'two-tower' \n",
    "PREFIX = 'css_retail'\n",
    "DISPLAY_NAME = f'{PREFIX}-tensorboard'\n",
    "PROJECT= 'babrams-recai-demo-final'\n",
    "REGION='us-central1'\n",
    "\n",
    "STAGING_BUCKET = \"\"\"gs://{}_vertex_training\"\"\".format(PROJECT) #lowes-reccomendation-tensorboard-logs-us-central1 - this \n",
    "#TENSORBOARD = 'projects/258043323883/locations/us-central1/tensorboards/4236655796332527616' #note really can only get this after gcloud beta ai tensorboards create...\n",
    "#VERTEX_SA = 'vertex-tb@lowes-reccomendation.iam.gserviceaccount.com'\n",
    "\n",
    "# initialize vertex sdk\n",
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")\n",
    "\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d6486",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r notebook_requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b2da03",
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'LR' is defined twice. First from /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py, Second from /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py.  Description from first occurrence: Learning Rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11228/3715321153.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LR\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Learning Rate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EMBEDDING_DIM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Embedding dimension\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MAX_TOKENS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Max embeddings for query and last_n products\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_float\u001b[0;34m(name, default, help, lower_bound, upper_bound, flag_values, required, **args)\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m       \u001b[0mrequired\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequired\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m       **args)\n\u001b[0m\u001b[1;32m    375\u001b[0m   \u001b[0m_register_bounds_validator_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflag_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, required, **args)\u001b[0m\n\u001b[1;32m    104\u001b[0m   return DEFINE_flag(\n\u001b[1;32m    105\u001b[0m       \u001b[0m_flag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       module_name, required)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name, required)\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'LR' is defined twice. First from /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py, Second from /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py.  Description from first occurrence: Learning Rate"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import os\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "from google.cloud import storage\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float(\"LR\", 0.01, \"Learning Rate\")\n",
    "flags.DEFINE_integer(\"EMBEDDING_DIM\", 16, \"Embedding dimension\")\n",
    "flags.DEFINE_integer(\"MAX_TOKENS\", 16, \"Max embeddings for query and last_n products\")\n",
    "flags.DEFINE_integer(\"NUM_EPOCHS\", 30, \"Number of epochs\")\n",
    "flags.DEFINE_string(\"MODEL_DIR\", 'model-dirs-lowes', \"GCS Bucket to store the model artifact\")\n",
    "flags.DEFINE_bool(\"DROPOUT\", False, \"Use Dropout - T/F bool type\")\n",
    "flags.DEFINE_float(\"DROPOUT_RATE\", 0.4, \"Dropout rate only works with DROPOUT=True\")\n",
    "#flags.DEFINE_integer(\"N_PRODUCTS\", 20000, \"number of products considered for embedding\")\n",
    "flags.DEFINE_integer(\"BATCH_SIZE\", 1024, \"batch size\")\n",
    "flags.DEFINE_string(\"ARCH\", '[128,64]', \"deep architecture, expressed as a list of ints in string format - will be parsed into list\")\n",
    "flags.DEFINE_integer(\"SEED\", 41781897, \"random seed\")\n",
    "#flags.DEFINE_string(\"TF_RECORDS_DIR\", \"gs://tfrs-central-a\", \"source data in tfrecord format gcs location\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2eefcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_catalog_sql = \"\"\"\n",
    "with inner_q as (\n",
    "    select \n",
    "        cast(id as int) as productId,\n",
    "        title,\n",
    "        description,\n",
    "        product_metadata.exact_price.original_price as price,\n",
    "        array_to_string(cats.categories, ' ') as categories\n",
    "    from `babrams-recai-demo-final.css_retail.recommendation_ai_data` as rad\n",
    "    , unnest(category_hierarchies) as cats\n",
    ") select \n",
    "    productId,\n",
    "    title,\n",
    "    description,\n",
    "    price,\n",
    "    array_to_string(array_agg(categories), \",\") as categories\n",
    "from inner_q \n",
    "group by productId, title, description, price\n",
    "\"\"\"\n",
    "\n",
    "product_catalog_df = client.query(product_catalog_sql).to_dataframe()\n",
    "product_catalog_df.describe()\n",
    "product_categoricals = ['productId', 'title', 'description', 'categories']\n",
    "# product_catalog_df.dtypes\n",
    "\n",
    "class ProductModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, adapt_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        #preprocess stuff\n",
    "        self.sku_count = np.unique(np.concatenate(list(adapt_data.map(lambda x: x[\"productId\"]).batch(1000))))\n",
    "        #categorical: sku\n",
    "        self.sku_lookup = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "            name=\"sku_monotic\"\n",
    "        )\n",
    "        self.title_vectorizor = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=self.sku_count\n",
    "            , name=\"title_vectorizor\"\n",
    "        )\n",
    "        self.description_vectorizor = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=self.sku_count\n",
    "            , name=\"description_vectorizor\"\n",
    "        )\n",
    "        self.category_vectorizor = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=FLAGS.N_PRODUCTS\n",
    "            , name=\"category_vectorizor\"\n",
    "        )\n",
    "        \n",
    "        #adapt stuff\n",
    "        self.category_vectorizor.adapt(adapt_data.map(lambda x: x['categories']))\n",
    "        self.title_vectorizor.adapt(adapt_data.map(lambda x: x['title']))\n",
    "        self.description_vectorizor.adapt(adapt_data.map(lambda x: x['description']))\n",
    "        self.sku_lookup.adapt(adapt_data.map(lambda x: x['productId']))\n",
    "        \n",
    "        #embed stuff\n",
    "        self.sku_embedding = tf.keras.Sequential([\n",
    "            self.sku_lookup,\n",
    "            tf.keras.layers.Embedding(self.sku_count+1, FLAGS.EMBEDDING_DIM, mask_zero=True, name = \"sku_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(name=\"sku_flat\")\n",
    "        ], name=\"sku_embedding\")    \n",
    "        self.title_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.title_vectorizor,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    self.sku_count+1, \n",
    "                    FLAGS.EMBEDDING_DIM, \n",
    "                    mask_zero=True, \n",
    "                    name=\"title_emb\"),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"title_flatten\"\n",
    "                )\n",
    "            ], \n",
    "            name=\"title_embedding\"\n",
    "        )\n",
    "        self.description_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.description_vectorizor,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    self.sku_count+1, \n",
    "                    FLAGS.EMBEDDING_DIM, \n",
    "                    mask_zero=True, \n",
    "                    name = \"desc_emb\"),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"desc_flatten\"\n",
    "                )\n",
    "            ], \n",
    "            name=\"description_embedding\"\n",
    "        )\n",
    "        self.category_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.category_vectorizor,\n",
    "                tf.keras.layers.Embedding(\n",
    "                    self.category_vectorizer.vocab_size(), \n",
    "                    FLAGS.EMBEDDING_DIM, \n",
    "                    mask_zero=True, \n",
    "                    name = \"category_emb\"),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(\n",
    "                    name=\"category_flatten\"\n",
    "                )\n",
    "            ], \n",
    "            name=\"category_embedding\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"dense_layers_product\")\n",
    "        \n",
    "        # Adding weight initialzier\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=FLAGS.SEED)\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\", kernel_initializer=initializer))\n",
    "            if FLAGS.DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(FLAGS.DROPOUT_RATE))\n",
    "            # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, kernel_initializer=initializer))\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(tf.keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, 1, epsilon=1e-12, name=\"normalize_dense\")))\n",
    "        \n",
    "        \n",
    "    def call(self, data):\n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                tf.reshape(data[\"price\"], (-1, 1)),\n",
    "                self.description_embedding(data['description']),\n",
    "                self.sku_embedding(data['productId']),\n",
    "                self.category_embedding(data['categories']),\n",
    "                self.title_embedding(data['title'])\n",
    "            ], axis=1)\n",
    "        return self.dense_layers(all_embs)  #last plus for number continuous + 1 if you add other(s) 2048 for visual\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d36c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "customer_data_sql = \"\"\"\n",
    "select\n",
    "    id as userId,\n",
    "    age,\n",
    "    gender,\n",
    "    latitude,\n",
    "    longitude,\n",
    "    zip,\n",
    "    traffic_source,\n",
    "    TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), created_at, DAY) as customer_lifetime_days\n",
    "from `babrams-recai-demo-final.css_retail.customers` as customers\n",
    "\"\"\"\n",
    "customer_data = client.query(customer_data_sql)\n",
    "customer_data_df = customer_data.to_dataframe()\n",
    "customer_data_df.describe()\n",
    "\n",
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, adapt_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        #preprocess stuff\n",
    "        self.user_lookup = tf.keras.layers.experimental.preprocessing.StringLookup()\n",
    "        self.max_age = adapt_data.map(lambda x: x['age']).reduce(tensorflow.cast(0, tensorflow.int64), tensorflow.maximum).numpy().max()\n",
    "        self.min_age = adapt_data.map(lambda x: x['age']).reduce(np.int64(1e9), tf.minimum).numpy().min()\n",
    "        self.age_buckets = np.linspace(min_age, max_age, num=20)\n",
    "        self.max_lifetime = adapt_data.map(lambda x: x['customer_lifetime_days']).reduce(tensorflow.cast(0, tensorflow.int64), tensorflow.maximum).numpy().max()\n",
    "        self.min_lifetime = adapt_data.map(lambda x: x['customer_lifetime_days']).reduce(np.int64(1e9), tf.minimum).numpy().min()\n",
    "        self.lifetime_buckets = np.linspace(min_lifetime, max_lifetime, num=100)\n",
    "        self.traffic_source_lookup = tf.keras.layers.experimental.preprocessing.StringLookup()\n",
    "        self.zip_lookup = tf.keras.layers.experimental.preprocessing.StringLookup()\n",
    "        self.gender_lookup = tf.keras.layers.experimental.preprocessing.StringLookup()\n",
    "        \n",
    "        #adapt stuff\n",
    "        self.user_lookup.adapt(adapt_data.map(lambda x: x['productId']))\n",
    "        self.zip_lookup.adapt(adapt_data.map(lambda x: x['zip']))\n",
    "        self.gender_lookup.adapt(adapt_data.map(lambda x: x['gender']))\n",
    "         \n",
    "        \n",
    "        #embed stuff\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            self.user_lookup,\n",
    "            tf.keras.layers.Embedding(FLAGS.N_PRODUCTS+1, FLAGS.EMBEDDING_DIM, mask_zero=True, name = \"user_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(name=\"user_flat\")\n",
    "        ], name=\"user_embedding\")\n",
    "        self.age_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.Discretization(age_buckets.tolist()),  \n",
    "            tf.keras.layers.Embedding(len(age_buckets) + 1, 32)\n",
    "        ], name=\"age_embedding\")\n",
    "        self.lifetime_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.Discretization(lifetime_buckets.tolist(), name = \"lifetime_disc\"),  \n",
    "            tf.keras.layers.Embedding(len(lifetime_buckets) + 1, 32)\n",
    "        ], name=\"customer_lifetime_embedding\")\n",
    "        self.traffic_source_embedding = tf.keras.Sequential([\n",
    "            self.traffic_source_lookup,\n",
    "            tf.keras.layers.Embedding(self.traffic_source_lookup.vocab_size()+1, FLAGS.EMBEDDING_DIM, mask_zero=True, name = \"traffic_source_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(name=\"traffic_source_flat\")\n",
    "        ], name=\"traffic_source_embedding\")\n",
    "        self.zip_embedding = tf.keras.Sequential([\n",
    "            self.zip_lookup,\n",
    "            tf.keras.layers.Embedding(self.zip_lookup.vocab_size()+1, FLAGS.EMBEDDING_DIM, mask_zero=True, name = \"zip_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(name=\"zip_flat\")\n",
    "        ], name=\"zip_embedding\")\n",
    "        self.gender_embedding = tf.keras.Sequential([\n",
    "            self.genderlookup,\n",
    "            tf.keras.layers.Embedding(self.gender_lookup.vocab_size()+1, FLAGS.EMBEDDING_DIM, mask_zero=True, name = \"gender_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(name=\"gender_flat\")\n",
    "        ], name=\"gender_embedding\")\n",
    "        \n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"dense_layers_product\")\n",
    "        \n",
    "        # Adding weight initialzier\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=FLAGS.SEED)\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\", kernel_initializer=initializer))\n",
    "            if FLAGS.DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(FLAGS.DROPOUT_RATE))\n",
    "            # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, kernel_initializer=initializer))\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(tf.keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, 1, epsilon=1e-12, name=\"normalize_dense\")))\n",
    "        \n",
    "\n",
    "    def call(self, data):\n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.user_embedding(data['userId']),\n",
    "                self.age_embedding(data['age']),\n",
    "                self.lifetime_embedding(data['customer_lifetime_days']),\n",
    "                self.traffic_source_embedding(data['traffic_source']),\n",
    "                self.zip_embedding(data['zip']),\n",
    "                self.gender_embedding(data['gender'])\n",
    "            ], axis=1)\n",
    "        return self.dense_layers(all_embs)  #last plus for number continuous + 1 if you add other(s) 2048 for visual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9632f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_data_sql = \"\"\"\n",
    "select\n",
    "    cast(userInfo.userId as int) as userId, \n",
    "    unix_millis(safe_cast(eventTime as timestamp)) as eventTime,\n",
    "    productEventDetail.cartId,\n",
    "    productEventDetail.purchaseTransaction.revenue,\n",
    "    products.id as productId,\n",
    "    products.quantity,\n",
    "    products.displayPrice as price\n",
    "from `babrams-recai-demo-final.css_retail.purchase_complete` as purchase\n",
    ", UNNEST(productEventDetail.productDetails) products\n",
    "\"\"\"\n",
    "\n",
    "purchase_data_df = client.query(purchase_data_sql).to_dataframe()\n",
    "purchase_data_df.describe()\n",
    "purchase_categoricals = ['userId', 'cartId', 'productId']\n",
    "# purchase_data_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77084a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, adapt_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        #month_vocab = tf.constant([\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"], name=\"month_vocab\")\n",
    "        #hour_vocab = tf.constant([\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n",
    "        #    \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"00\"], name=\"hour_vocab\")\n",
    "\n",
    "        #self.month_embedding = tf.keras.Sequential([\n",
    "        #    tf.keras.layers.StringLookup(\n",
    "        #        vocabulary=month_vocab, mask_token=None, name=\"month_lookup\", output_mode='count')\n",
    "        #], name=\"month\")\n",
    "        \n",
    "        #self.hour_embedding = tf.keras.Sequential([\n",
    "        #    tf.keras.layers.StringLookup(\n",
    "        #        vocabulary=hour_vocab, mask_token=None, name=\"hour_lookup\", output_mode='count')\n",
    "        #], name=\"hour\")\n",
    "    \n",
    "        #self.query_vectorizor = tf.keras.layers.TextVectorization(\n",
    "        #    max_tokens=FLAGS.MAX_TOKENS, name=\"query_tv\", ngrams=2)\n",
    "        \n",
    "        #self.last_viewed_vectorizor = tf.keras.layers.TextVectorization(\n",
    "        #    max_tokens=FLAGS.MAX_TOKENS, name=\"last_viewed_tv\", ngrams=2)\n",
    "        \n",
    "        #self.query_embedding = tf.keras.Sequential([\n",
    "        #    self.query_vectorizor,\n",
    "        #    tf.keras.layers.Embedding(FLAGS.MAX_TOKENS+1, FLAGS.EMBEDDING_DIM , mask_zero=True, name=\"query_emb\"),\n",
    "        #    tf.keras.layers.GlobalAveragePooling1D()\n",
    "        #], name=\"query_embedding_model\")\n",
    "        \n",
    "        #self.last_viewed_embedding = tf.keras.Sequential([\n",
    "        #    self.last_viewed_vectorizor,\n",
    "        #    tf.keras.layers.Embedding(FLAGS.MAX_TOKENS+1, FLAGS.EMBEDDING_DIM , mask_zero=True, name=\"last_v_emb\"),\n",
    "        #    tf.keras.layers.GlobalAveragePooling1D()\n",
    "        #], name=\"last_viewed_embedding\")\n",
    "        \n",
    "        ### adapt stuff\n",
    "        #self.query_vectorizor.adapt(adapt_data.map(lambda x: x['query']))\n",
    "        #self.last_viewed_vectorizor.adapt(adapt_data.map(lambda x: x['last_viewed'])) \n",
    "        \n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"dense_layers_query\")\n",
    "        \n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=FLAGS.SEED)\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\", kernel_initializer=initializer))\n",
    "            if FLAGS.DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(FLAGS.DROPOUT_RATE))\n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, kernel_initializer=initializer))\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(tf.keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, 1, epsilon=1e-12, name=\"normalize_dense\")))\n",
    "\n",
    "\n",
    "    def call(self, data):    \n",
    "        all_embs = tf.concat(\n",
    "                [\n",
    "                    #self.month_embedding(data['month']),\n",
    "                    #self.hour_embedding(data['hour']),\n",
    "                    #self.query_embedding(data['query']),\n",
    "                    #self.last_viewed_embedding(data['last_viewed'])\n",
    "                ], axis=1)\n",
    "        return self.dense_layers(all_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdcd06a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers_tf = tensorflow.convert_to_tensor(customer_data_df)\n",
    "#purchase_tf = tensorflow.convert_to_tensor(purchase_data_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a27e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheTwoTowers(tfrs.models.Model):\n",
    "    def __init__(self, layer_sizes, query_adapt_data, cat_adapt_data):\n",
    "        super().__init__()\n",
    "        self.query_model = QueryModel(layer_sizes, query_adapt_data)\n",
    "        self.candidate_model = ProductModel(layer_sizes, cat_adapt_data)\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=cat_adapt_data.batch(128).cache().map(self.candidate_model),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, data, training=False):\n",
    "        query_embeddings = self.query_model(data)\n",
    "        product_embeddings = self.candidate_model(data)\n",
    "\n",
    "        return self.task(\n",
    "            query_embeddings, product_embeddings, compute_metrics=not training)#### turn off metrics to save time on training\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
