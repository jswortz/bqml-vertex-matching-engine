{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e5923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install google-cloud-aiplatform --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93525754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9672208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'twotower' \n",
    "PREFIX = 'lowes-rec'\n",
    "DISPLAY_NAME = f'{PREFIX}-tensorboard'\n",
    "PROJECT= 'lowes-reccomendation'\n",
    "REGION='us-central1'\n",
    "STAGING_BUCKET = 'gs://tfrs-central-a/models'\n",
    "TENSORBOARD = 'projects/258043323883/locations/us-central1/tensorboards/1819067221364703232' # TODO args\n",
    "VERTEX_SA = '258043323883-compute@developer.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea72147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud beta ai tensorboards create --display-name $DISPLAY_NAME --project $PROJECT --region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc83a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize vertex sdk\n",
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7427f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flags.DEFINE_string('bucket_name', None, 'GCS bucket name without gs://')\n",
    "# flags.DEFINE_string('filename_pattern', None, 'e.g., gs://{BUCKET_NAME}/*.tfrec')\n",
    "# flags.DEFINE_string('tb_instance', None, 'tensorboard instance')\n",
    "# flags.DEFINE_integer('embed_dim', 32, 'embedding dimension')\n",
    "# flags.DEFINE_integer('max_tokens', 1000000, 'max tokens for string vocab')\n",
    "# flags.DEFINE_integer('batch_size', 128, 'per replica batch size')\n",
    "# flags.DEFINE_integer('epochs', 5, 'training epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a4085ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘vertex-train’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir vertex-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f329455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vertex-train/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile vertex-train/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "RUN pip install -r trainer/requirements.txt\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ca722da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vertex-train/trainer/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile vertex-train/trainer/requirements.txt\n",
    "\n",
    "tensorflow-recommenders==0.5.2\n",
    "tensorflow-io-gcs-filesystem==0.18.0\n",
    "absl-py==0.12.0\n",
    "google-api-python-client==1.12.8\n",
    "tfx-bsl==1.1.1\n",
    "google-cloud-bigquery==2.20.0\n",
    "dill==0.3.1.1\n",
    "pyarrow==2.0.0\n",
    "tfx-bsl==1.1.1\n",
    "tensorflow-transform==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cae0a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vertex-train/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vertex-train/trainer/task.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "from classes_functions import QueryModel, TheTwoTowers, ProductModel, parse_tfrecord_fn, parse_tfrecord_catalog\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def _get_model_dir(model_dir):\n",
    "    \"\"\"Defines utility functions for model saving.\n",
    "\n",
    "    In a multi-worker scenario, the chief worker will save to the\n",
    "    desired model directory, while the other workers will save the model to\n",
    "    temporary directories. It’s important that these temporary directories\n",
    "    are unique in order to prevent multiple workers from writing to the same\n",
    "    location. Saving can contain collective ops, so all workers must save and\n",
    "    not just the chief.\n",
    "    \"\"\"\n",
    "\n",
    "    def _is_chief(task_type, task_id):\n",
    "        return ((task_type == 'chief' and task_id == 0) or task_type is None)\n",
    "\n",
    "    tf_config = os.getenv('TF_CONFIG')\n",
    "    if tf_config:\n",
    "        tf_config = json.loads(tf_config)\n",
    "\n",
    "    if not _is_chief(tf_config['task']['type'], tf_config['task']['index']):\n",
    "        model_dir = os.path.join(model_dir,\n",
    "                                 'worker-{}').format(tf_config['task']['index'])\n",
    "\n",
    "    logging.info('Setting model_dir to: %s', model_dir)\n",
    "\n",
    "    return model_dir\n",
    "\n",
    "def main(_):\n",
    "    \n",
    "    ## set params TODO - make params work with `absl`\n",
    "    \n",
    "    EMBEDDING_DIM = 32\n",
    "    MAX_TOKENS = 50_000 #1265634\n",
    "    N_PRODUCTS = 10_000 #212_862 212861\n",
    "    N_HEIR = 10_000 #54724 \n",
    "    BATCH_SIZE = 2048\n",
    "    ARCH = [256, 128, 64]\n",
    "    NUM_EPOCHS = 30\n",
    "    TF_RECORDS_DIR = 'gs://tfrs-central-a'\n",
    "    TF_RECORDS_CAT = 'gs://prod-catalog-central'\n",
    "    NUM_RECORDS = 4_293_302\n",
    "    \n",
    "    # Sets mixed_precision policy. Using 'mixed_float16' or 'mixed_bfloat16'\n",
    "  # can have significant impact on model speeds by utilizing float16 in case of\n",
    "  # GPUs, and bfloat16 in the case of TPUs. loss_scale takes effect only when\n",
    "  # dtype is float16\n",
    "    if params.runtime.mixed_precision_dtype:\n",
    "        performance.set_mixed_precision_policy(params.runtime.mixed_precision_dtype)\n",
    "    distribution_strategy = distribute_utils.get_distribution_strategy(\n",
    "        distribution_strategy=params.runtime.distribution_strategy,\n",
    "        all_reduce_alg=params.runtime.all_reduce_alg,\n",
    "        num_gpus=params.runtime.num_gpus,\n",
    "        tpu_address=params.runtime.tpu,\n",
    "        **params.runtime.model_parallelism())\n",
    "\n",
    "    \n",
    "    ## get product, query data files\n",
    "    client = storage.Client()\n",
    "    files = []\n",
    "    for blob in client.list_blobs('tfrs-tf-records'):\n",
    "        files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "    \n",
    "    files_cat = []\n",
    "    for blob in client.list_blobs('prod-catalog-central'):\n",
    "        files_cat.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "    print(files_cat[:2])client = storage.Client()\n",
    "    files = []\n",
    "    for blob in client.list_blobs('tfrs-tf-records'):\n",
    "        files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "\n",
    "    files_cat = []\n",
    "    for blob in client.list_blobs('prod-catalog-central'):\n",
    "        files_cat.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "    ## establish the pipelines\n",
    "    # Set dev dataset CHANGE THIS LATER TO THE WHOLE DIR\n",
    "    raw_dataset = tf.data.TFRecordDataset(files) #local machine training wheels - using smaller data set for starters\n",
    "    cat_dataset = tf.data.TFRecordDataset(files_cat)\n",
    "\n",
    "\n",
    "    #See `pipeline-opts.ipynb` for more info on tuning options\n",
    "    parsed_dataset = raw_dataset.map(\n",
    "            parse_tfrecord_fn,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).prefetch(  # Overlap producer and consumer works\n",
    "            tf.data.AUTOTUNE\n",
    "        )\n",
    "\n",
    "\n",
    "    # Doing another pipeline for the adapts to get startup to run much faster\n",
    "\n",
    "    parsed_dataset_adapt = raw_dataset.map(\n",
    "        parse_tfrecord_fn,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).prefetch(  # Overlap producer and consumer works\n",
    "            tf.data.AUTOTUNE\n",
    "        )\n",
    "\n",
    "    parsed_dataset_adapt = parsed_dataset_adapt.batch(BATCH_SIZE)\n",
    "\n",
    "    # parsed_dataset_adapt = parsed_dataset_adapt.batch(BATCH_SIZE)\n",
    "    # loading de-duplicated product catalog\n",
    "\n",
    "    parsed_dataset_candidates = cat_dataset.map(\n",
    "            parse_tfrecord_catalog,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).prefetch(  # Overlap producer and consumer works\n",
    "            tf.data.AUTOTUNE\n",
    "        )\n",
    "\n",
    "    parsed_dataset_candidates = parsed_dataset_candidates.cache()\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    shuffled = parsed_dataset.shuffle(200_000, seed=42, reshuffle_each_iteration=False)\n",
    "    # shuffled = shuffled.cache()\n",
    "\n",
    "    test_pct = 0.05\n",
    "    n_test = int(NUM_RECORDS * test_pct)\n",
    "\n",
    "    # train_records\n",
    "    test = shuffled.take(n_test)\n",
    "    train = shuffled.skip(n_test)\n",
    "\n",
    "    cached_train = train.batch(BATCH_SIZE)\n",
    "    cached_test = test.batch(BATCH_SIZE * 2).cache()\n",
    "    \n",
    "    model = TheTwoTowers(ARCH)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "    \n",
    "    layer_history = model.fit(\n",
    "        cached_train,\n",
    "        validation_data=cached_test,\n",
    "        validation_freq=5,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        verbose=0)\n",
    "\n",
    "    accuracy = layer_history.history[\"factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "    print(f\"Top-100 accuracy: {accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63692f2",
   "metadata": {},
   "source": [
    "## Note we manually copy/pasted the model classes and data parsing functions to `./vertex-train/trainer/classes_functions.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3eec5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  87.04kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
      " ---> 307b41b1aec7\n",
      "Step 2/5 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> 8b66c51b024f\n",
      "Step 3/5 : COPY trainer /trainer\n",
      " ---> 5e713d678733\n",
      "Step 4/5 : RUN pip install -r trainer/requirements.txt\n",
      " ---> Running in b5ef73c46a9f\n",
      "Collecting tensorflow-recommenders==0.5.2\n",
      "  Downloading tensorflow_recommenders-0.5.2-py3-none-any.whl (85 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem==0.18.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.18.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.5 MB)\n",
      "Collecting absl-py==0.12.0\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting google-api-python-client==1.12.8\n",
      "  Downloading google_api_python_client-1.12.8-py2.py3-none-any.whl (61 kB)\n",
      "Collecting tfx-bsl==1.1.1\n",
      "  Downloading tfx_bsl-1.1.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (19.0 MB)\n",
      "Collecting google-cloud-bigquery==2.20.0\n",
      "  Downloading google_cloud_bigquery-2.20.0-py2.py3-none-any.whl (189 kB)\n",
      "Collecting dill==0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "Collecting pyarrow==2.0.0\n",
      "  Downloading pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7 MB)\n",
      "Collecting tensorflow-transform==1.1.0\n",
      "  Downloading tensorflow_transform-1.1.0-py3-none-any.whl (401 kB)\n",
      "Requirement already satisfied: tensorflow>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py==0.12.0->-r trainer/requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.12.8->-r trainer/requirements.txt (line 5)) (3.0.1)\n",
      "Requirement already satisfied: google-auth>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.12.8->-r trainer/requirements.txt (line 5)) (1.34.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.12.8->-r trainer/requirements.txt (line 5)) (0.1.0)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.12.8->-r trainer/requirements.txt (line 5)) (1.31.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.12.8->-r trainer/requirements.txt (line 5)) (0.19.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.13 in /opt/conda/lib/python3.7/site-packages (from tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (3.16.0)\n",
      "Requirement already satisfied: numpy<1.20,>=1.16 in /opt/conda/lib/python3.7/site-packages (from tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (1.19.5)\n",
      "Collecting tensorflow-metadata<1.2,>=1.1\n",
      "  Downloading tensorflow_metadata-1.1.0-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: pandas<2,>=1.0 in /opt/conda/lib/python3.7/site-packages (from tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: apache-beam[gcp]<3,>=2.29 in /opt/conda/lib/python3.7/site-packages (from tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (2.31.0)\n",
      "Requirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15 in /opt/conda/lib/python3.7/site-packages (from tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (2.5.1)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.20.0->-r trainer/requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.20.0->-r trainer/requirements.txt (line 7)) (21.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.20.0->-r trainer/requirements.txt (line 7)) (1.19.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.20.0->-r trainer/requirements.txt (line 7)) (1.7.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.20.0->-r trainer/requirements.txt (line 7)) (2.25.1)\n",
      "Requirement already satisfied: pydot<2,>=1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==1.1.0->-r trainer/requirements.txt (line 11)) (1.4.2)\n",
      "Requirement already satisfied: future<1.0.0,>=0.18.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (0.18.2)\n",
      "Requirement already satisfied: grpcio<2,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (1.38.1)\n",
      "Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (1.9.2.1)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (2.6.0)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (3.12.0)\n",
      "Requirement already satisfied: oauth2client<5,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (4.1.3)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (2.8.2)\n",
      "Collecting typing-extensions<3.8.0,>=3.7.0\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (1.7)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (2021.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.21.4 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (1.4.4)\n",
      "Requirement already satisfied: cachetools<5,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (4.2.2)\n",
      "Collecting google-cloud-language<2,>=1.3.0\n",
      "  Downloading google_cloud_language-1.3.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting google-cloud-spanner<2,>=1.13.0\n",
      "  Downloading google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255 kB)\n",
      "Collecting google-cloud-vision<2,>=0.38.0\n",
      "  Downloading google_cloud_vision-1.0.0-py2.py3-none-any.whl (435 kB)\n",
      "Requirement already satisfied: google-cloud-pubsub<2,>=0.39.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (1.7.0)\n",
      "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (0.5.31)\n",
      "Collecting google-cloud-videointelligence<2,>=1.8.0\n",
      "  Downloading google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183 kB)\n",
      "Requirement already satisfied: google-cloud-dlp<2,>=0.12.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (1.0.0)\n",
      "Requirement already satisfied: google-cloud-profiler<4,>=3.0.4 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (3.0.5)\n",
      "Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (0.2.2)\n",
      "Collecting google-cloud-bigtable<2,>=0.31.1\n",
      "  Downloading google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267 kB)\n",
      "Collecting google-cloud-datastore<2,>=1.7.1\n",
      "  Downloading google_cloud_datastore-1.15.3-py2.py3-none-any.whl (134 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.8->-r trainer/requirements.txt (line 5)) (1.53.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client==1.12.8->-r trainer/requirements.txt (line 5)) (49.6.0.post20210108)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (0.16.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.16.0->google-api-python-client==1.12.8->-r trainer/requirements.txt (line 5)) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.16.0->google-api-python-client==1.12.8->-r trainer/requirements.txt (line 5)) (0.2.7)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (0.12.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery==2.20.0->-r trainer/requirements.txt (line 7)) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery==2.20.0->-r trainer/requirements.txt (line 7)) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery==2.20.0->-r trainer/requirements.txt (line 7)) (2.20)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (0.6.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client==1.12.8->-r trainer/requirements.txt (line 5)) (2.4.7)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.29->tfx-bsl==1.1.1->-r trainer/requirements.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.20.0->-r trainer/requirements.txt (line 7)) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.20.0->-r trainer/requirements.txt (line 7)) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.20.0->-r trainer/requirements.txt (line 7)) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.20.0->-r trainer/requirements.txt (line 7)) (2021.5.30)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (0.36.2)\n",
      "Collecting grpcio<2,>=1.29.0\n",
      "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (3.3.0)\n",
      "Collecting six\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (1.5.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (0.4.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (2.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (4.6.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.3.0->tensorflow-recommenders==0.5.2->-r trainer/requirements.txt (line 2)) (3.5.0)\n",
      "Building wheels for collected packages: dill\n",
      "  Building wheel for dill (setup.py): started\n",
      "  Building wheel for dill (setup.py): finished with status 'done'\n",
      "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78532 sha256=3050a829011e7444878a2ed8675ac0ec885c6e97a380e554c41c743907cba9ec\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
      "Successfully built dill\n",
      "Installing collected packages: six, typing-extensions, grpcio, absl-py, pyarrow, google-api-python-client, dill, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-language, google-cloud-datastore, google-cloud-bigtable, google-cloud-bigquery, tensorflow-metadata, tfx-bsl, tensorflow-transform, tensorflow-recommenders, tensorflow-io-gcs-filesystem\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.0\n",
      "    Uninstalling typing-extensions-3.10.0.0:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.38.1\n",
      "    Uninstalling grpcio-1.38.1:\n",
      "      Successfully uninstalled grpcio-1.38.1\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.13.0\n",
      "    Uninstalling absl-py-0.13.0:\n",
      "      Successfully uninstalled absl-py-0.13.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 5.0.0\n",
      "    Uninstalling pyarrow-5.0.0:\n",
      "      Successfully uninstalled pyarrow-5.0.0\n",
      "  Attempting uninstall: google-api-python-client\n",
      "    Found existing installation: google-api-python-client 2.15.0\n",
      "    Uninstalling google-api-python-client-2.15.0:\n",
      "      Successfully uninstalled google-api-python-client-2.15.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "  Attempting uninstall: google-cloud-vision\n",
      "    Found existing installation: google-cloud-vision 2.4.2\n",
      "    Uninstalling google-cloud-vision-2.4.2:\n",
      "      Successfully uninstalled google-cloud-vision-2.4.2\n",
      "  Attempting uninstall: google-cloud-videointelligence\n",
      "    Found existing installation: google-cloud-videointelligence 2.3.2\n",
      "    Uninstalling google-cloud-videointelligence-2.3.2:\n",
      "      Successfully uninstalled google-cloud-videointelligence-2.3.2\n",
      "  Attempting uninstall: google-cloud-spanner\n",
      "    Found existing installation: google-cloud-spanner 3.6.0\n",
      "    Uninstalling google-cloud-spanner-3.6.0:\n",
      "      Successfully uninstalled google-cloud-spanner-3.6.0\n",
      "  Attempting uninstall: google-cloud-language\n",
      "    Found existing installation: google-cloud-language 2.2.2\n",
      "    Uninstalling google-cloud-language-2.2.2:\n",
      "      Successfully uninstalled google-cloud-language-2.2.2\n",
      "  Attempting uninstall: google-cloud-datastore\n",
      "    Found existing installation: google-cloud-datastore 2.1.6\n",
      "    Uninstalling google-cloud-datastore-2.1.6:\n",
      "      Successfully uninstalled google-cloud-datastore-2.1.6\n",
      "  Attempting uninstall: google-cloud-bigtable\n",
      "    Found existing installation: google-cloud-bigtable 2.3.3\n",
      "    Uninstalling google-cloud-bigtable-2.3.3:\n",
      "      Successfully uninstalled google-cloud-bigtable-2.3.3\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 2.23.2\n",
      "    Uninstalling google-cloud-bigquery-2.23.2:\n",
      "      Successfully uninstalled google-cloud-bigquery-2.23.2\n",
      "  Attempting uninstall: tensorflow-metadata\n",
      "    Found existing installation: tensorflow-metadata 1.2.0\n",
      "    Uninstalling tensorflow-metadata-1.2.0:\n",
      "      Successfully uninstalled tensorflow-metadata-1.2.0\n",
      "  Attempting uninstall: tfx-bsl\n",
      "    Found existing installation: tfx-bsl 1.2.0\n",
      "    Uninstalling tfx-bsl-1.2.0:\n",
      "      Successfully uninstalled tfx-bsl-1.2.0\n",
      "  Attempting uninstall: tensorflow-transform\n",
      "    Found existing installation: tensorflow-transform 1.2.0\n",
      "    Uninstalling tensorflow-transform-1.2.0:\n",
      "      Successfully uninstalled tensorflow-transform-1.2.0\n",
      "Successfully installed absl-py-0.12.0 dill-0.3.1.1 google-api-python-client-1.12.8 google-cloud-bigquery-2.20.0 google-cloud-bigtable-1.7.0 google-cloud-datastore-1.15.3 google-cloud-language-1.3.0 google-cloud-spanner-1.19.1 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 grpcio-1.34.1 pyarrow-2.0.0 six-1.15.0 tensorflow-io-gcs-filesystem-0.18.0 tensorflow-metadata-1.1.0 tensorflow-recommenders-0.5.2 tensorflow-transform-1.1.0 tfx-bsl-1.1.1 typing-extensions-3.7.4.3\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container b5ef73c46a9f\n",
      " ---> ea4ba70e61bd\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      " ---> Running in d22293fda2db\n",
      "Removing intermediate container d22293fda2db\n",
      " ---> 34832cec77b8\n",
      "Successfully built 34832cec77b8\n",
      "Successfully tagged gcr.io/lowes-reccomendation/multiworker:two-tower-jw\n",
      "The push refers to repository [gcr.io/lowes-reccomendation/multiworker]\n",
      "5de5eb4a2a32: Preparing\n",
      "542c74cd4d1c: Preparing\n",
      "5bb1aa5df10d: Preparing\n",
      "f028010939aa: Preparing\n",
      "dc99c4ea3a81: Preparing\n",
      "37b508c5711b: Preparing\n",
      "756ab564e194: Preparing\n",
      "2ae86808a3d1: Preparing\n",
      "1dccbdf9b557: Preparing\n",
      "cfcbdbc2b748: Preparing\n",
      "937ab8f29c2e: Preparing\n",
      "5d417b2f7486: Preparing\n",
      "d6a297a3e6e4: Preparing\n",
      "6474a5e8117f: Preparing\n",
      "fe498124ed57: Preparing\n",
      "d5454704bb3d: Preparing\n",
      "fb896ef24b4b: Preparing\n",
      "5087113f67c8: Preparing\n",
      "2a92857a1d48: Preparing\n",
      "0ded97864c52: Preparing\n",
      "b50bbaac3e32: Preparing\n",
      "262ea1af4c10: Preparing\n",
      "b420a468ca49: Preparing\n",
      "608c205798d1: Preparing\n",
      "0760cd6d4269: Preparing\n",
      "fb4755c89c2a: Preparing\n",
      "22cfb9034da6: Preparing\n",
      "8bec4fbfce85: Preparing\n",
      "3b129ca3db46: Preparing\n",
      "64cb1a1930ab: Preparing\n",
      "600ef5a43f1f: Preparing\n",
      "8f8f0266f834: Preparing\n",
      "756ab564e194: Waiting\n",
      "2ae86808a3d1: Waiting\n",
      "1dccbdf9b557: Waiting\n",
      "cfcbdbc2b748: Waiting\n",
      "b50bbaac3e32: Waiting\n",
      "262ea1af4c10: Waiting\n",
      "b420a468ca49: Waiting\n",
      "937ab8f29c2e: Waiting\n",
      "608c205798d1: Waiting\n",
      "0760cd6d4269: Waiting\n",
      "5d417b2f7486: Waiting\n",
      "fb4755c89c2a: Waiting\n",
      "22cfb9034da6: Waiting\n",
      "8bec4fbfce85: Waiting\n",
      "d6a297a3e6e4: Waiting\n",
      "6474a5e8117f: Waiting\n",
      "3b129ca3db46: Waiting\n",
      "64cb1a1930ab: Waiting\n",
      "fe498124ed57: Waiting\n",
      "600ef5a43f1f: Waiting\n",
      "d5454704bb3d: Waiting\n",
      "8f8f0266f834: Waiting\n",
      "fb896ef24b4b: Waiting\n",
      "5087113f67c8: Waiting\n",
      "2a92857a1d48: Waiting\n",
      "0ded97864c52: Waiting\n",
      "37b508c5711b: Waiting\n",
      "dc99c4ea3a81: Layer already exists\n",
      "f028010939aa: Layer already exists\n",
      "5bb1aa5df10d: Layer already exists\n",
      "756ab564e194: Layer already exists\n",
      "37b508c5711b: Layer already exists\n",
      "2ae86808a3d1: Layer already exists\n",
      "cfcbdbc2b748: Layer already exists\n",
      "937ab8f29c2e: Layer already exists\n",
      "1dccbdf9b557: Layer already exists\n",
      "5d417b2f7486: Layer already exists\n",
      "d6a297a3e6e4: Layer already exists\n",
      "6474a5e8117f: Layer already exists\n",
      "fe498124ed57: Layer already exists\n",
      "d5454704bb3d: Layer already exists\n",
      "fb896ef24b4b: Layer already exists\n",
      "5087113f67c8: Layer already exists\n",
      "2a92857a1d48: Layer already exists\n",
      "0ded97864c52: Layer already exists\n",
      "b50bbaac3e32: Layer already exists\n",
      "b420a468ca49: Layer already exists\n",
      "262ea1af4c10: Layer already exists\n",
      "608c205798d1: Layer already exists\n",
      "0760cd6d4269: Layer already exists\n",
      "fb4755c89c2a: Layer already exists\n",
      "22cfb9034da6: Layer already exists\n",
      "8bec4fbfce85: Layer already exists\n",
      "3b129ca3db46: Layer already exists\n",
      "64cb1a1930ab: Layer already exists\n",
      "600ef5a43f1f: Layer already exists\n",
      "8f8f0266f834: Layer already exists\n",
      "542c74cd4d1c: Pushed\n",
      "5de5eb4a2a32: Pushed\n",
      "two-tower-jw: digest: sha256:ad3eebebdcae9131d21131a0d2a2a0e12f9330e7ff9c1b1e0ce2d82fd61e41f7 size: 7048\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "PROJECT_ID='lowes-reccomendation'\n",
    "IMAGE_URI=\"gcr.io/$PROJECT_ID/multiworker:two-tower-jw\"\n",
    "docker build ./vertex-train/ -t $IMAGE_URI\n",
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8e301",
   "metadata": {},
   "source": [
    "## Submit Job from custom container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90f97a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_IMAGE='gcr.io/lowes-reccomendation/multiworker:two-tower'\n",
    "TRAIN_IMAGE='gcr.io/lowes-reccomendation/multiworker:two-tower-jw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f15a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMINDER ON PARAMS\n",
    "#     EMBEDDING_DIM = 32\n",
    "#     MAX_TOKENS = 50_000 #1265634\n",
    "#     N_PRODUCTS = 10_000 #212_862 212861\n",
    "#     N_HEIR = 10_000 #54724 \n",
    "#     BATCH_SIZE = 2048\n",
    "#     ARCH = [256, 128, 64]\n",
    "#     NUM_EPOCHS = 30\n",
    "#     TF_RECORDS_DIR = 'gs://tfrs-central-a'\n",
    "#     TF_RECORDS_CAT = 'gs://prod-catalog-central'\n",
    "#     NUM_RECORDS = 4_293_302\n",
    "\n",
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"c2-standard-30\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "#            \"command\": [\"python\", \"train.py\"],\n",
    "            \"args\": [\n",
    "                '--bucket_name=tfrs-central-a', \n",
    "                '--filename_pattern=gs://{BUCKET_NAME}/*.tfrec',\n",
    "                '--tb_instance=projects/258043323883/locations/us-central1/tensorboards/1819067221364703232',\n",
    "                '--epochs=5'\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a692063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n"
     ]
    }
   ],
   "source": [
    "job_name = f'{PREFIX}_CUSTOM_CONTAINER'\n",
    "base_output_dir = '{}/jobs/{}/{}'.format(STAGING_BUCKET, job_name, time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=base_output_dir\n",
    ")\n",
    "\n",
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51598f60",
   "metadata": {},
   "source": [
    "## Submit jobs from custom container - v2\n",
    "[distributed GPU training example](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/reduction_server/distributed-training-reduction-server.ipynb)\n",
    "\n",
    "Try:\n",
    "* Worker Pool 0: (1) `n1-highcpu-16`, (2) `v100`\n",
    "* Worker pool 1: (1) `n1-highcpu-16`, (2) `v100`\n",
    "* Worker Pool 2: (2) `n1-highcpu-16`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55f947c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_worker_pool_specs(\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_count=0,\n",
    "    accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type=\"n1-highcpu-16\",\n",
    "    reduction_server_image_uri=\"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\"machine_type\": machine_type}\n",
    "\n",
    "    container_spec = {\n",
    "        \"image_uri\": image_uri,\n",
    "        \"args\": args,\n",
    "        \"command\": cmd,\n",
    "    }\n",
    "\n",
    "    chief_spec = {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"container_spec\": container_spec,\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": replica_count - 1,\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"container_spec\": container_spec,\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "\n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": reduction_server_count,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": reduction_server_machine_type,\n",
    "            },\n",
    "            \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "\n",
    "    return worker_pool_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f26c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'{PREFIX}_CUSTOM_SCRIPT'\n",
    "base_output_dir = '{}/jobs/{}/{}'.format(STAGING_BUCKET, job_name, time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "#container_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-4:latest'\n",
    "container_uri = 'gcr.io/deeplearning-platform-release/tf2-gpu.2-6'\n",
    "requirements = ['tensorflow-recommenders','tensorboard_plugin_profile']\n",
    "\n",
    "WORKER_CMD = [\"python\", \"-m\", \"trainer.task\"]\n",
    "WORKER_ARGS = [\n",
    "    '--bucket_name=tfrs-central-a', \n",
    "    '--filename_pattern=gs://{BUCKET_NAME}/*.tfrec',\n",
    "    '--tb_instance=projects/258043323883/locations/us-central1/tensorboards/1819067221364703232',\n",
    "    '--epochs=5',]\n",
    "\n",
    "REPLICA_COUNT = 1\n",
    "WORKER_MACHINE_TYPE = \"a2-highgpu-4g\"\n",
    "ACCELERATOR_TYPE = \"NVIDIA_TESLA_A100\"\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 4\n",
    "PER_REPLICA_BATCH_SIZE = 32\n",
    "\n",
    "REDUCTION_SERVER_COUNT = 4\n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "\n",
    "\n",
    "machine_type = 'c2-standard-30' # 'n1-standard-4'\n",
    "accelerator_type = 'NVIDIA_TESLA_V100' # NVIDIA_TESLA_T4'\n",
    "accelerator_count = 1\n",
    "replicas=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54049054",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'{PREFIX}_CUSTOM_SCRIPT'\n",
    "base_output_dir = '{}/jobs/{}/{}'.format(STAGING_BUCKET, job_name, time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "#container_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-4:latest'\n",
    "container_uri = 'gcr.io/deeplearning-platform-release/tf2-gpu.2-5'\n",
    "requirements = ['tensorflow-recommenders==0.5.2','tensorflow-io-gcs-filesystem==0.18.0', 'absl-py==0.12.0', 'google-api-python-client==1.12.8', 'tfx-bsl==1.1.1', 'google-cloud-bigquery==2.20.0', 'dill==0.3.1.1', 'pyarrow==2.0.0', 'tfx-bsl==1.1.1', 'tensorflow-transform==1.1.0']\n",
    "args = [\n",
    "    '--bucket_name=tfrs-central-a', \n",
    "    '--filename_pattern=gs://{BUCKET_NAME}/*.tfrec',\n",
    "    '--tb_instance=projects/258043323883/locations/us-central1/tensorboards/1819067221364703232',\n",
    "    '--epochs=5',\n",
    "]\n",
    "\n",
    "machine_type = 'c2-standard-30' # 'n1-standard-4'\n",
    "accelerator_type = 'NVIDIA_TESLA_V100' # NVIDIA_TESLA_T4'\n",
    "accelerator_count = 1\n",
    "replicas=2\n",
    "\n",
    "job = vertex_ai.CustomJob.from_local_script(\n",
    "    display_name=job_name,\n",
    "    machine_type=machine_type,\n",
    "    replica_count=replicas,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    script_path='vertex-train/trainer/task.py', # 'trainer/train.py', \n",
    "    container_uri=container_uri,\n",
    "    requirements=requirements,\n",
    "    args=args,\n",
    "    staging_bucket=base_output_dir\n",
    ")\n",
    "\n",
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
