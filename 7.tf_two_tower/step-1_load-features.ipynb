{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd5fbee",
   "metadata": {},
   "source": [
    "# This notebook is to load a json file data into the `tfRecords` format\n",
    "\n",
    "##### Description of the `.jsonl` file in `./training-data-schema.json`\n",
    "\n",
    "#### This does\n",
    "\n",
    "1. Reads parameters for existing data\n",
    "2. Create a small development data set\n",
    "3. Expand to the full data and stream data into a formatted tfrecord in a new bucket `TF_RECORDS_DIR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9670ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil -m rm gs://tfrs-tf-records/* #use this to clear the target directory - if you are adding additional fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f3f0b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters\n",
    "BUCKET = 'gs://mcskinner-sample-data/2tower/last-view'\n",
    "SCHEMA_JSON = 'gs://mcskinner-sample-data/2tower/last-view/training-data-schema.json'\n",
    "TRAIN_JSON = 'gs://tfrs-sample-data/training-data.jsonl'\n",
    "MY_BUCKET = 'gs://tfrs-sample-data'\n",
    "TF_RECORDS_DIR = 'gs://tfrs-central-a'\n",
    "SMALL_DATASET = 'gs://tfrs-sample-data/training-data_dev.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b83010f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a smaller dev dataset first\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "\n",
    "ROW_LIMIT = 10000\n",
    "jsonl_path = os.path.join(MY_BUCKET, 'training-data.jsonl')\n",
    "SMALL_DATASET = 'gs://tfrs-sample-data/training-data_dev.jsonl'\n",
    "\n",
    "input_file_columns = subprocess.getoutput(f'gsutil cp {jsonl_path} - | head -{ROW_LIMIT} | gsutil cp - {SMALL_DATASET}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada647c2",
   "metadata": {},
   "source": [
    "### Read in the schema for later parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65693cbc",
   "metadata": {},
   "source": [
    "#### FYI:Pre-calculated counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60a2a155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records: 4293302\n"
     ]
    }
   ],
   "source": [
    "num_records = 4293302 #sum(1 for _ in file_io.FileIO(SMALL_DATASET, 'rb')) #CHANGE THIS TO LARGE DATASET WHEN READY\n",
    "print(\"Total number of records: {}\".format(num_records))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a83202a",
   "metadata": {},
   "source": [
    "### Establish the parameters\n",
    "\n",
    "`num_samples` is the number of data samples for each TFRECORD file\n",
    "\n",
    "`num_tfrecods` is total number of TFRecords that we will create.\n",
    "\n",
    "Generally - aim for around 100 MB size tfrecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "233085d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Expected TFRecords: 352\n"
     ]
    }
   ],
   "source": [
    "#To be tuned later\n",
    "\n",
    "num_samples = 12228\n",
    "num_tfrecords = num_records // num_samples \n",
    "if num_records % num_samples:\n",
    "    num_tfrecords += 1\n",
    "\n",
    "print(\"Number of Expected TFRecords: {}\".format(num_tfrecords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b20cde",
   "metadata": {},
   "source": [
    "### These helper functions declare different feature types\n",
    "This is used to parase the jsonl file\n",
    "\n",
    "Note [this](https://keras.io/examples/keras_recipes/creating_tfrecords/#define-dataset-helper-functions) is a good resource\n",
    "\n",
    "#### Notes on data transforms:\n",
    "* Grabbing all fields avaialble for query\n",
    "* Transforming and flattening of array / ragged data inputs (`['last_viewed', 'ss_prodTypeCombo_ss',` etc..`]`)\n",
    "* Using special delimiter (`|` in this case) to later unpack values as a string-split for text vectorizor layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e54027e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def string_array(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[v.encode('utf-8') for v in value]))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[float(v) for v in value]))\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[int(v) for v in value]))\n",
    "\n",
    "\n",
    "def float_feature_list(value):\n",
    "    \"\"\"Returns a list of float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "    \n",
    "def parse_line(ln):\n",
    "    q_ln = ln['query']\n",
    "    c_ln = ln['candidate']\n",
    "    \n",
    "    timestamp = string_array(q_ln['search_date_time']),\n",
    "    \n",
    "    month = str(q_ln['search_date_time']).split(\"-\")[1]\n",
    "    \n",
    "    hour = str(q_ln['search_date_time']).split(\" \")[1]\n",
    "    hour = str(hour).split(\":\")[0]\n",
    "    \n",
    "    lv = \"\"\n",
    "    for item in q_ln['last_viewed']:\n",
    "        lv = lv + \" \" + item\n",
    "        \n",
    "    lv = lv + \" END\" #so we get string len > 0 on non-last viewed results\n",
    "    \n",
    "    pt = \"\"\n",
    "    for item in c_ln['productTypeCombo_ss']:\n",
    "        pt = pt + \"|\" + item\n",
    "        \n",
    "    pt_feature = tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(pt).encode('utf-8')]))\n",
    "    lv_feature = tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(lv).encode('utf-8')]))\n",
    "\n",
    "            \n",
    "    hour = tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(hour).encode('utf-8')]))\n",
    "    \n",
    "    month = tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(month).encode('utf-8')]))\n",
    "\n",
    "    \n",
    "    feature = {\n",
    "        #query features\n",
    "        \"query\": string_array(q_ln['query']), #this is actually a list - consider tokenizing\n",
    "#         \"search_date_time\": timestamp,\n",
    "        \"last_viewed\": lv_feature,\n",
    "\n",
    "        #candidate features\n",
    "        \"IVM_s\": string_array(c_ln['IVM_s']), #target??\n",
    "        \"description\": string_array(c_ln['description']),\n",
    "#         \"total_ratings_i\": float_feature(c_ln['total_ratings_i']),\n",
    "#         \"overall_ratings\": float_feature(c_ln['overall_ratings']),\n",
    "#         \"avg_rating_td\": float_feature(c_ln['avg_rating_td']),\n",
    "#         \"parent_description\": string_array(c_ln['parent_description']),\n",
    "#         \"Brand_s\": string_array(c_ln['Brand_s']),\n",
    "#         \"item_type\": string_array(c_ln['item_type']), #just fixed this to be a string\n",
    "#         \"prc_rdc_amt\": float_feature(c_ln['prc_rdc_amt']),\n",
    "#         \"quantity_sold\": float_feature(c_ln['quantity_sold']),\n",
    "#         \"sales_dollar_f\": float_feature(c_ln['sales_dollar_f']),\n",
    "#         \"freight_term\": string_array(c_ln['freight_term']),\n",
    "#         \"is_energy_star_s\": string_array(c_ln['is_energy_star_s']),\n",
    "        \"price_td\": float_feature(c_ln['price_td']),\n",
    "        \"PriceRange_s\": string_array(c_ln['PriceRange_s']),\n",
    "#         \"prc_rdc_pct\": float_feature(c_ln['prc_rdc_pct']),\n",
    "#         \"spellcheck\": string_array(c_ln['spellcheck']),\n",
    "        \"productTypeCombo_ss\": pt_feature, #this is actually a list - consider tokenizing\n",
    "#         \"Searchable_t\": string_array(c_ln['Searchable_t']), #this is actually a list - consider tokenizing\n",
    "#         \"clean_Brand_s\": string_array(c_ln['clean_Brand_s']),\n",
    "        \"visual\": float_feature(c_ln['visual']),\n",
    "        \"month\": month,\n",
    "        \"hour\": hour\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad9668",
   "metadata": {},
   "source": [
    "### Create a target TF Records dataset for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8475b81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (352 of 352) |######################| Elapsed Time: 1:37:58 Time:  1:37:58\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import progressbar\n",
    "\n",
    "# this will generate data into the correct format for tf_records\n",
    "record_counter = 0\n",
    "lns = [] #empty holder for the lines\n",
    "tfrec_counter = 0\n",
    "\n",
    "\n",
    "# quick function to write the data as we read through it\n",
    "def write_a_tfrec(lns):\n",
    "    #next write to a tfrecord\n",
    "    with tf.io.TFRecordWriter(\n",
    "        TF_RECORDS_DIR + \"/file_%.2i-%i.tfrec\" % (tfrec_counter, len(lns))\n",
    "    ) as writer:\n",
    "        for ln in lns:\n",
    "            example = parse_line(ln)\n",
    "            writer.write(example.SerializeToString())\n",
    "            \n",
    "            \n",
    "with progressbar.ProgressBar(max_value=num_tfrecords) as bar:\n",
    "        \n",
    "    with file_io.FileIO(TRAIN_JSON, 'r') as reader:\n",
    "        for line in reader:\n",
    "            record_counter += 1\n",
    "            if record_counter % num_samples == 0 or record_counter == num_records: \n",
    "                write_a_tfrec(lns) #write out a batch\n",
    "                lns = [] #reset to a new batch\n",
    "                tfrec_counter += 1\n",
    "                bar.update(tfrec_counter)\n",
    "            else:\n",
    "                pass\n",
    "#             lns.append(json.loads(line)) #toggle if you want to save lines locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb4c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify records\n",
    "!gsutil ls $TF_RECORDS_DIR"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
